--- git status ---
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
	modified:   .gitignore
	modified:   source/unitreelab/setup.py
	new file:   source/unitreelab/unitreelab/MotionLib/isaac_utils/__init__.py
	new file:   source/unitreelab/unitreelab/MotionLib/isaac_utils/device_dtype_mixin.py
	new file:   source/unitreelab/unitreelab/MotionLib/isaac_utils/maths.py
	new file:   source/unitreelab/unitreelab/MotionLib/isaac_utils/rotations.py
	new file:   source/unitreelab/unitreelab/MotionLib/isaac_utils/torch_utils.py
	new file:   source/unitreelab/unitreelab/MotionLib/motion_lib.py
	new file:   source/unitreelab/unitreelab/MotionLib/motion_lib_h1.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/__init__.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/core/__init__.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/core/backend/__init__.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/core/backend/abstract.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/core/backend/logger.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/core/rotation3d.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/core/tensor_utils.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/core/tests/__init__.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/core/tests/test_rotation.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/skeleton/__init__.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/skeleton/backend/__init__.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/skeleton/backend/fbx/__init__.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/skeleton/backend/fbx/fbx_py27_backend.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/skeleton/backend/fbx/fbx_read_wrapper.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/skeleton/skeleton3d.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/skeleton/tests/__init__.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/skeleton/tests/ant.xml
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/skeleton/tests/test_skeleton.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/skeleton/tests/transfer_npy.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/visualization/__init__.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/visualization/common.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/visualization/core.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/visualization/plt_plotter.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/visualization/simple_plotter_tasks.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/visualization/skeleton_plotter_tasks.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/visualization/tests/__init__.py
	new file:   source/unitreelab/unitreelab/MotionLib/poselib/visualization/tests/test_plotter.py
	modified:   source/unitreelab/unitreelab/__init__.py
	modified:   source/unitreelab/unitreelab/assets/g1/.asset_hash
	deleted:    source/unitreelab/unitreelab/assets/g1/Props/instanceable_meshes.usd
	modified:   source/unitreelab/unitreelab/assets/g1/config.yaml
	new file:   source/unitreelab/unitreelab/assets/g1/configuration/g1_base.usd
	new file:   source/unitreelab/unitreelab/assets/g1/configuration/g1_physics.usd
	new file:   source/unitreelab/unitreelab/assets/g1/configuration/g1_sensor.usd
	modified:   source/unitreelab/unitreelab/assets/g1/g1.usd
	new file:   source/unitreelab/unitreelab/assets/h1/.asset_hash
	new file:   source/unitreelab/unitreelab/assets/h1/config.yaml
	new file:   source/unitreelab/unitreelab/assets/h1/configuration/h1_base.usd
	new file:   source/unitreelab/unitreelab/assets/h1/configuration/h1_physics.usd
	new file:   source/unitreelab/unitreelab/assets/h1/configuration/h1_sensor.usd
	modified:   source/unitreelab/unitreelab/assets/h1/h1.usd
	modified:   source/unitreelab/unitreelab/assets/unitree.py
	new file:   source/unitreelab/unitreelab/envs/manager_based_rl_wbc_env.py
	new file:   source/unitreelab/unitreelab/envs/manager_based_rl_wbc_env_cfg.py
	new file:   source/unitreelab/unitreelab/managers/__init__.py
	new file:   source/unitreelab/unitreelab/managers/manager_term_cfg.py
	new file:   source/unitreelab/unitreelab/managers/motion_manager.py
	modified:   source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1/agents/rsl_rl_ppo_cfg.py
	modified:   source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1/flat_env_cfg.py
	modified:   source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1/rough_env_cfg.py
	modified:   source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1_with_dagger/rough_env_cfg.py
	new file:   source/unitreelab/unitreelab/tasks/locomotion/velocity/config/h1_wbc/__init__.py
	new file:   source/unitreelab/unitreelab/tasks/locomotion/velocity/config/h1_wbc/agents/__init__.py
	new file:   source/unitreelab/unitreelab/tasks/locomotion/velocity/config/h1_wbc/agents/rsl_rl_ppo_cfg.py
	new file:   source/unitreelab/unitreelab/tasks/locomotion/velocity/config/h1_wbc/wbc_env_cfg.py
	modified:   source/unitreelab/unitreelab/tasks/locomotion/velocity/mdp/observations.py
	modified:   source/unitreelab/unitreelab/terrains/terrain_generator_cfg.py
 


--- git diff ---
diff --git a/.gitignore b/.gitignore
index e4c34fb..9ca5cd2 100644
--- a/.gitignore
+++ b/.gitignore
@@ -37,4 +37,7 @@ _build
 
 
 # Run bash
-run.sh
\ No newline at end of file
+run.sh
+
+# data
+data
\ No newline at end of file
diff --git a/source/unitreelab/setup.py b/source/unitreelab/setup.py
index da054d1..3952e11 100644
--- a/source/unitreelab/setup.py
+++ b/source/unitreelab/setup.py
@@ -14,6 +14,8 @@ EXTENSION_TOML_DATA = toml.load(os.path.join(EXTENSION_PATH, "config", "extensio
 INSTALL_REQUIRES = [
     # NOTE: Add dependencies
     "psutil",
+    "easydict",
+    "lightning"
 ]
 
 # Installation operation
diff --git a/source/unitreelab/unitreelab/MotionLib/isaac_utils/__init__.py b/source/unitreelab/unitreelab/MotionLib/isaac_utils/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/source/unitreelab/unitreelab/MotionLib/isaac_utils/device_dtype_mixin.py b/source/unitreelab/unitreelab/MotionLib/isaac_utils/device_dtype_mixin.py
new file mode 100644
index 0000000..d8a52df
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/isaac_utils/device_dtype_mixin.py
@@ -0,0 +1,103 @@
+# Copied from PyTorch Lightning at their suggestion,
+# since they are deprecating this class in a future
+# version of the package.
+
+from typing import Any, List, Optional, Union
+
+import torch
+from torch.nn import Module
+from typing_extensions import Self
+
+
+class DeviceDtypeModuleMixin(Module):
+    __jit_unused_properties__: List[str] = ["device", "dtype"]
+
+    def __init__(self) -> None:
+        super().__init__()
+        self._dtype: Union[str, torch.dtype] = torch.get_default_dtype()
+        self._device = torch.device("cpu")
+
+    @property
+    def dtype(self) -> Union[str, torch.dtype]:
+        return self._dtype
+
+    @dtype.setter
+    def dtype(self, new_dtype: Union[str, torch.dtype]) -> None:
+        # necessary to avoid infinite recursion
+        raise RuntimeError(
+            "Cannot set the dtype explicitly. Please use module.to(new_dtype)."
+        )
+
+    @property
+    def device(self) -> torch.device:
+        device = self._device
+
+        # make this more explicit to always include the index
+        if device.type == "cuda" and device.index is None:
+            return torch.device(f"cuda:{torch.cuda.current_device()}")
+
+        return device
+
+    def to(self, *args: Any, **kwargs: Any) -> Self:  # type: ignore[valid-type]
+        """See :meth:`torch.nn.Module.to`."""
+        # this converts `str` device to `torch.device`
+        device, dtype = torch._C._nn._parse_to(*args, **kwargs)[:2]
+        self.__update_properties(device=device, dtype=dtype)
+        return super().to(*args, **kwargs)
+
+    def cuda(self, device: Optional[Union[torch.device, int]] = None) -> Self:  # type: ignore[valid-type]
+        """Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers
+        different objects. So it should be called before constructing optimizer if the module will live on GPU
+        while being optimized.
+        Arguments:
+            device: If specified, all parameters will be copied to that device. If `None`, the current CUDA device
+                index will be used.
+        Returns:
+            Module: self
+        """
+        if device is None:
+            device = torch.device("cuda", torch.cuda.current_device())
+        elif isinstance(device, int):
+            device = torch.device("cuda", index=device)
+        self.__update_properties(device=device)
+        return super().cuda(device=device)
+
+    def cpu(self) -> Self:  # type: ignore[valid-type]
+        """See :meth:`torch.nn.Module.cpu`."""
+        self.__update_properties(device=torch.device("cpu"))
+        return super().cpu()
+
+    def type(self, dst_type: Union[str, torch.dtype]) -> Self:  # type: ignore[valid-type]
+        """See :meth:`torch.nn.Module.type`."""
+        self.__update_properties(dtype=dst_type)
+        return super().type(dst_type=dst_type)
+
+    def float(self) -> Self:  # type: ignore[valid-type]
+        """See :meth:`torch.nn.Module.float`."""
+        self.__update_properties(dtype=torch.float)
+        return super().float()
+
+    def double(self) -> Self:  # type: ignore[valid-type]
+        """See :meth:`torch.nn.Module.double`."""
+        self.__update_properties(dtype=torch.double)
+        return super().double()
+
+    def half(self) -> Self:  # type: ignore[valid-type]
+        """See :meth:`torch.nn.Module.half`."""
+        self.__update_properties(dtype=torch.half)
+        return super().half()
+
+    def __update_properties(
+        self,
+        device: Optional[torch.device] = None,
+        dtype: Optional[Union[str, torch.dtype]] = None,
+    ) -> None:
+        def apply_fn(module: Union[DeviceDtypeModuleMixin, Module]) -> None:
+            if not isinstance(module, DeviceDtypeModuleMixin):
+                return
+            if device is not None:
+                module._device = device
+            if dtype is not None:
+                module._dtype = dtype
+
+        self.apply(apply_fn)
diff --git a/source/unitreelab/unitreelab/MotionLib/isaac_utils/maths.py b/source/unitreelab/unitreelab/MotionLib/isaac_utils/maths.py
new file mode 100644
index 0000000..bb911a6
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/isaac_utils/maths.py
@@ -0,0 +1,146 @@
+# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
+#
+# NVIDIA CORPORATION and its licensors retain all intellectual property
+# and proprietary rights in and to this software, related documentation
+# and any modifications thereto.  Any use, reproduction, disclosure or
+# distribution of this software and related documentation without an express
+# license agreement from NVIDIA CORPORATION is strictly prohibited.
+
+import torch
+import numpy as np
+import random
+import os
+
+
+@torch.jit.script
+def normalize(x, eps: float = 1e-9):
+    return x / x.norm(p=2, dim=-1).clamp(min=eps, max=None).unsqueeze(-1)
+
+
+@torch.jit.script
+def scale_transform(x: torch.Tensor, lower: torch.Tensor, upper: torch.Tensor) -> torch.Tensor:
+    """
+    Normalizes a given input tensor to a range of [-1, 1].
+
+    @note It uses pytorch broadcasting functionality to deal with batched input.
+
+    Args:
+        x: Input tensor of shape (N, dims).
+        lower: The minimum value of the tensor. Shape (dims,)
+        upper: The maximum value of the tensor. Shape (dims,)
+
+    Returns:
+        Normalized transform of the tensor. Shape (N, dims)
+    """
+    # default value of center
+    offset = (lower + upper) * 0.5
+    # return normalized tensor
+    return 2 * (x - offset) / (upper - lower)
+
+
+@torch.jit.script
+def unscale_transform(x: torch.Tensor, lower: torch.Tensor, upper: torch.Tensor) -> torch.Tensor:
+    """
+    Denormalizes a given input tensor from range of [-1, 1] to (lower, upper).
+
+    @note It uses pytorch broadcasting functionality to deal with batched input.
+
+    Args:
+        x: Input tensor of shape (N, dims).
+        lower: The minimum value of the tensor. Shape (dims,)
+        upper: The maximum value of the tensor. Shape (dims,)
+
+    Returns:
+        Denormalized transform of the tensor. Shape (N, dims)
+    """
+    # default value of center
+    offset = (lower + upper) * 0.5
+    # return normalized tensor
+    return x * (upper - lower) * 0.5 + offset
+
+
+@torch.jit.script
+def copysign(a, b):
+    # type: (float, Tensor) -> Tensor
+    a = torch.tensor(a, device=b.device, dtype=torch.float).repeat(b.shape[0])
+    return torch.abs(a) * torch.sign(b)
+
+
+@torch.jit.script
+def torch_rand_float(lower, upper, shape, device):
+    # type: (float, float, Tuple[int, int], str) -> Tensor
+    return (upper - lower) * torch.rand(*shape, device=device) + lower
+
+
+@torch.jit.script
+def torch_random_dir_2(shape, device):
+    # type: (Tuple[int, int], str) -> Tensor
+    angle = torch_rand_float(-np.pi, np.pi, shape, device).squeeze(-1)
+    return torch.stack([torch.cos(angle), torch.sin(angle)], dim=-1)
+
+
+@torch.jit.script
+def tensor_clamp(t, min_t, max_t):
+    return torch.max(torch.min(t, max_t), min_t)
+
+
+@torch.jit.script
+def scale(x, lower, upper):
+    return 0.5 * (x + 1.0) * (upper - lower) + lower
+
+
+@torch.jit.script
+def unscale(x, lower, upper):
+    return (2.0 * x - upper - lower) / (upper - lower)
+
+
+def unscale_np(x, lower, upper):
+    return (2.0 * x - upper - lower) / (upper - lower)
+
+
+def set_seed(seed, torch_deterministic=False):
+    """ set seed across modules """
+    if seed == -1 and torch_deterministic:
+        seed = 42
+    elif seed == -1:
+        seed = np.random.randint(0, 10000)
+    print("Setting seed: {}".format(seed))
+
+    random.seed(seed)
+    np.random.seed(seed)
+    torch.manual_seed(seed)
+    os.environ["PYTHONHASHSEED"] = str(seed)
+    torch.cuda.manual_seed(seed)
+    torch.cuda.manual_seed_all(seed)
+
+    if torch_deterministic:
+        # refer to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility
+        os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
+        torch.backends.cudnn.benchmark = False
+        torch.backends.cudnn.deterministic = True
+        torch.use_deterministic_algorithms(True)
+    else:
+        torch.backends.cudnn.benchmark = True
+        torch.backends.cudnn.deterministic = False
+
+    return seed
+
+
+def matmul(matrix_a, matrix_b):
+    return torch.matmul(matrix_a, matrix_b)
+
+
+def sin(data):
+    return torch.sin(data)
+
+
+def cos(data):
+    return torch.cos(data)
+
+
+def transpose_2d(data):
+    return torch.transpose(data, 1, 0)
+
+
+def inverse(data):
+    return torch.linalg.inv(data)
diff --git a/source/unitreelab/unitreelab/MotionLib/isaac_utils/rotations.py b/source/unitreelab/unitreelab/MotionLib/isaac_utils/rotations.py
new file mode 100644
index 0000000..7ad4116
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/isaac_utils/rotations.py
@@ -0,0 +1,492 @@
+# Copyright (c) 2018-2022, NVIDIA Corporation
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions are met:
+#
+# 1. Redistributions of source code must retain the above copyright notice, this
+#    list of conditions and the following disclaimer.
+#
+# 2. Redistributions in binary form must reproduce the above copyright notice,
+#    this list of conditions and the following disclaimer in the documentation
+#    and/or other materials provided with the distribution.
+#
+# 3. Neither the name of the copyright holder nor the names of its
+#    contributors may be used to endorse or promote products derived from
+#    this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+import torch
+from torch import Tensor
+import torch.nn.functional as F
+from .maths import *
+from typing import Tuple
+
+
+def wxyz_to_xyzw(quat: Tensor):
+    shape = quat.shape
+    flat_quat = quat.view(-1, 4)
+    flat_quat = flat_quat[:, [1, 2, 3, 0]]
+    return flat_quat.view(shape)
+
+
+def xyzw_to_wxyz(quat: Tensor):
+    shape = quat.shape
+    flat_quat = quat.view(-1, 4)
+    flat_quat = flat_quat[:, [3, 0, 1, 2]]
+    return flat_quat.view(shape)
+
+
+@torch.jit.script
+def _sqrt_positive_part(x: Tensor) -> Tensor:
+    """
+    Returns torch.sqrt(torch.max(0, x))
+    but with a zero subgradient where x is 0.
+    """
+    ret = torch.zeros_like(x)
+    positive_mask = x > 0
+    ret[positive_mask] = torch.sqrt(x[positive_mask])
+    return ret
+
+
+def rad2deg(radian_value: Tensor, device=None) -> Tensor:
+    """_summary_
+
+    Args:
+        radian_value (torch.Tensor): _description_
+        device (_type_, optional): _description_. Defaults to None.
+
+    Returns:
+        torch.Tensor: _description_
+    """
+    return torch.rad2deg(radian_value).float().to(device)
+
+
+def deg2rad(degree_value: float, device=None) -> Tensor:
+    """_summary_
+
+    Args:
+        degree_value (torch.Tensor): _description_
+        device (_type_, optional): _description_. Defaults to None.
+
+    Returns:
+        torch.Tensor: _description_
+    """
+    return torch.deg2rad(degree_value).float().to(device)
+
+
+@torch.jit.script
+def quat_mul(a, b, w_last: bool):
+    assert a.shape == b.shape
+    shape = a.shape
+    a = a.reshape(-1, 4)
+    b = b.reshape(-1, 4)
+
+    if w_last:
+        x1, y1, z1, w1 = a[..., 0], a[..., 1], a[..., 2], a[..., 3]
+        x2, y2, z2, w2 = b[..., 0], b[..., 1], b[..., 2], b[..., 3]
+    else:
+        w1, x1, y1, z1 = a[..., 0], a[..., 1], a[..., 2], a[..., 3]
+        w2, x2, y2, z2 = b[..., 0], b[..., 1], b[..., 2], b[..., 3]
+    ww = (z1 + x1) * (x2 + y2)
+    yy = (w1 - y1) * (w2 + z2)
+    zz = (w1 + y1) * (w2 - z2)
+    xx = ww + yy + zz
+    qq = 0.5 * (xx + (z1 - x1) * (x2 - y2))
+    w = qq - ww + (z1 - y1) * (y2 - z2)
+    x = qq - xx + (x1 + w1) * (x2 + w2)
+    y = qq - yy + (w1 - x1) * (y2 + z2)
+    z = qq - zz + (z1 + y1) * (w2 - x2)
+
+    if w_last:
+        quat = torch.stack([x, y, z, w], dim=-1).view(shape)
+    else:
+        quat = torch.stack([w, x, y, z], dim=-1).view(shape)
+
+    return quat
+
+
+@torch.jit.script
+def quat_conjugate(a: Tensor, w_last: bool) -> Tensor:
+    shape = a.shape
+    a = a.reshape(-1, 4)
+    if w_last:
+        return torch.cat((-a[:, :3], a[:, -1:]), dim=-1).view(shape)
+    else:
+        return torch.cat((a[:, 0:1], -a[:, 1:]), dim=-1).view(shape)
+
+
+@torch.jit.script
+def quat_apply(a: Tensor, b: Tensor, w_last: bool) -> Tensor:
+    shape = b.shape
+    a = a.reshape(-1, 4)
+    b = b.reshape(-1, 3)
+    if w_last:
+        xyz = a[:, :3]
+        w = a[:, 3:]
+    else:
+        xyz = a[:, 1:]
+        w = a[:, :1]
+    t = xyz.cross(b, dim=-1) * 2
+    return (b + w * t + xyz.cross(t, dim=-1)).view(shape)
+
+
+@torch.jit.script
+def quat_rotate(q: Tensor, v: Tensor, w_last: bool) -> Tensor:
+    shape = q.shape
+    flat_q = q.reshape(-1, shape[-1])
+    flat_v = v.reshape(-1, v.shape[-1])
+    if w_last:
+        q_w = flat_q[:, -1]
+        q_vec = flat_q[:, :3]
+    else:
+        q_w = flat_q[:, 0]
+        q_vec = flat_q[:, 1:]
+    a = flat_v * (2.0 * q_w**2 - 1.0).unsqueeze(-1)
+    b = torch.cross(q_vec, flat_v, dim=-1) * q_w.unsqueeze(-1) * 2.0
+    c = (
+        q_vec
+        * torch.bmm(
+            q_vec.reshape(flat_q.shape[0], 1, 3), flat_v.reshape(flat_q.shape[0], 3, 1)
+        ).squeeze(-1)
+        * 2.0
+    )
+    return (a + b + c).reshape(v.shape)
+
+
+@torch.jit.script
+def quat_rotate_inverse(q: Tensor, v: Tensor, w_last: bool) -> Tensor:
+    shape = q.shape
+    if w_last:
+        q_w = q[:, -1]
+        q_vec = q[:, :3]
+    else:
+        q_w = q[:, 0]
+        q_vec = q[:, 1:]
+    a = v * (2.0 * q_w**2 - 1.0).unsqueeze(-1)
+    b = torch.cross(q_vec, v, dim=-1) * q_w.unsqueeze(-1) * 2.0
+    c = (
+        q_vec
+        * torch.bmm(q_vec.view(shape[0], 1, 3), v.view(shape[0], 3, 1)).squeeze(-1)
+        * 2.0
+    )
+    return a - b + c
+
+
+@torch.jit.script
+def quat_unit(a):
+    return normalize(a)
+
+
+@torch.jit.script
+def quat_mul_norm(x: Tensor, y: Tensor, w_last: bool) -> Tensor:
+    """
+    Combine two set of 3D rotations together using \**\* operator. The shape needs to be
+    broadcastable
+    """
+    return quat_unit(quat_mul(x, y, w_last))
+
+
+@torch.jit.script
+def quat_angle_axis(x: Tensor, w_last: bool) -> Tuple[Tensor, Tensor]:
+    """
+    The (angle, axis) representation of the rotation. The axis is normalized to unit length.
+    The angle is guaranteed to be between [0, pi].
+    """
+    if w_last:
+        w = x[..., -1]
+        axis = x[..., :3]
+    else:
+        w = x[..., 0]
+        axis = x[..., 1:]
+    s = 2 * (w**2) - 1
+    angle = s.clamp(-1, 1).arccos()  # just to be safe
+    axis /= axis.norm(p=2, dim=-1, keepdim=True).clamp(min=1e-9)
+    return angle, axis
+
+
+@torch.jit.script
+def quat_from_angle_axis(angle: Tensor, axis: Tensor, w_last: bool) -> Tensor:
+    theta = (angle / 2).unsqueeze(-1)
+    xyz = normalize(axis) * theta.sin()
+    w = theta.cos()
+    if w_last:
+        return quat_unit(torch.cat([xyz, w], dim=-1))
+    else:
+        return quat_unit(torch.cat([w, xyz], dim=-1))
+
+
+@torch.jit.script
+def vec_to_heading(h_vec):
+    h_theta = torch.atan2(h_vec[..., 1], h_vec[..., 0])
+    return h_theta
+
+
+@torch.jit.script
+def heading_to_quat(h_theta, w_last: bool):
+    axis = torch.zeros(
+        h_theta.shape
+        + [
+            3,
+        ],
+        device=h_theta.device,
+    )
+    axis[..., 2] = 1
+    heading_q = quat_from_angle_axis(h_theta, axis, w_last=w_last)
+    return heading_q
+
+
+@torch.jit.script
+def quat_axis(q: Tensor, axis: int, w_last: bool) -> Tensor:
+    basis_vec = torch.zeros(q.shape[0], 3, device=q.device)
+    basis_vec[:, axis] = 1
+    return quat_rotate(q, basis_vec, w_last)
+
+
+@torch.jit.script
+def normalize_angle(x):
+    return torch.atan2(torch.sin(x), torch.cos(x))
+
+
+@torch.jit.script
+def get_basis_vector(q: Tensor, v: Tensor, w_last: bool) -> Tensor:
+    return quat_rotate(q, v, w_last)
+
+
+@torch.jit.script
+def get_euler_xyz(q: Tensor, w_last: bool) -> Tuple[Tensor, Tensor, Tensor]:
+    if w_last:
+        qx, qy, qz, qw = 0, 1, 2, 3
+    else:
+        qw, qx, qy, qz = 0, 1, 2, 3
+    # roll (x-axis rotation)
+    sinr_cosp = 2.0 * (q[:, qw] * q[:, qx] + q[:, qy] * q[:, qz])
+    cosr_cosp = (
+        q[:, qw] * q[:, qw]
+        - q[:, qx] * q[:, qx]
+        - q[:, qy] * q[:, qy]
+        + q[:, qz] * q[:, qz]
+    )
+    roll = torch.atan2(sinr_cosp, cosr_cosp)
+
+    # pitch (y-axis rotation)
+    sinp = 2.0 * (q[:, qw] * q[:, qy] - q[:, qz] * q[:, qx])
+    pitch = torch.where(
+        torch.abs(sinp) >= 1, copysign(np.pi / 2.0, sinp), torch.asin(sinp)
+    )
+
+    # yaw (z-axis rotation)
+    siny_cosp = 2.0 * (q[:, qw] * q[:, qz] + q[:, qx] * q[:, qy])
+    cosy_cosp = (
+        q[:, qw] * q[:, qw]
+        + q[:, qx] * q[:, qx]
+        - q[:, qy] * q[:, qy]
+        - q[:, qz] * q[:, qz]
+    )
+    yaw = torch.atan2(siny_cosp, cosy_cosp)
+
+    return roll % (2 * np.pi), pitch % (2 * np.pi), yaw % (2 * np.pi)
+
+
+@torch.jit.script
+def quat_from_euler_xyz(
+    roll: Tensor, pitch: Tensor, yaw: Tensor, w_last: bool
+) -> Tensor:
+    cy = torch.cos(yaw * 0.5)
+    sy = torch.sin(yaw * 0.5)
+    cr = torch.cos(roll * 0.5)
+    sr = torch.sin(roll * 0.5)
+    cp = torch.cos(pitch * 0.5)
+    sp = torch.sin(pitch * 0.5)
+
+    qw = cy * cr * cp + sy * sr * sp
+    qx = cy * sr * cp - sy * cr * sp
+    qy = cy * cr * sp + sy * sr * cp
+    qz = sy * cr * cp - cy * sr * sp
+
+    if w_last:
+        return torch.stack([qx, qy, qz, qw], dim=-1)
+    else:
+        return torch.stack([qw, qx, qy, qz], dim=-1)
+
+
+@torch.jit.script
+def quat_diff_rad(a: Tensor, b: Tensor, w_last: bool) -> Tensor:
+    """
+    Get the difference in radians between two quaternions.
+
+    Args:
+        a: first quaternion, shape (N, 4)
+        b: second quaternion, shape (N, 4)
+    Returns:
+        Difference in radians, shape (N,)
+    """
+    b_conj = quat_conjugate(b, w_last)
+    mul = quat_mul(a, b_conj, w_last)
+    # 2 * torch.acos(torch.abs(mul[:, -1]))
+    return 2.0 * torch.asin(torch.clamp(torch.norm(mul[:, 1:], p=2, dim=-1), max=1.0))
+
+
+# NB: do not make this function jit, since it is passed around as an argument.
+def normalise_quat_in_pose(pose):
+    """Takes a pose and normalises the quaternion portion of it.
+
+    Args:
+        pose: shape N, 7
+    Returns:
+        Pose with normalised quat. Shape N, 7
+    """
+    pos = pose[:, 0:3]
+    quat = pose[:, 3:7]
+    quat /= torch.norm(quat, dim=-1, p=2).reshape(-1, 1)
+    return torch.cat([pos, quat], dim=-1)
+
+
+@torch.jit.script
+def quat_apply_yaw(quat: Tensor, vec: Tensor, w_last: bool) -> Tensor:
+    quat_yaw = quat.clone().view(-1, 4)
+    quat_yaw[:, :2] = 0.0
+    quat_yaw = normalize(quat_yaw)
+    return quat_apply(quat_yaw, vec, w_last)
+
+
+@torch.jit.script
+def quaternion_to_matrix(quaternions: torch.Tensor, w_last: bool) -> torch.Tensor:
+    """
+    Convert rotations given as quaternions to rotation matrices.
+
+    Args:
+        quaternions: quaternions of shape (..., 4).
+        w_last: If True, the real part of the quaternion is last.
+
+    Returns:
+        Rotation matrices as tensor of shape (..., 3, 3).
+    """
+    if w_last:
+        i, j, k, r = torch.unbind(quaternions, -1)
+    else:
+        r, i, j, k = torch.unbind(quaternions, -1)
+    two_s = 2.0 / (quaternions * quaternions).sum(-1)
+
+    o = torch.stack(
+        (
+            1 - two_s * (j * j + k * k),
+            two_s * (i * j - k * r),
+            two_s * (i * k + j * r),
+            two_s * (i * j + k * r),
+            1 - two_s * (i * i + k * k),
+            two_s * (j * k - i * r),
+            two_s * (i * k - j * r),
+            two_s * (j * k + i * r),
+            1 - two_s * (i * i + j * j),
+        ),
+        -1,
+    )
+    return o.reshape(quaternions.shape[:-1] + (3, 3))
+
+
+@torch.jit.script
+def axis_angle_to_quaternion(axis_angle: torch.Tensor, w_last: bool) -> torch.Tensor:
+    """
+    Convert rotations given as axis/angle to quaternions.
+
+    Args:
+        axis_angle: Rotations given as a vector in axis angle form,
+            as a tensor of shape (..., 3), where the magnitude is
+            the angle turned anticlockwise in radians around the
+            vector's direction.
+        w_last: If True, the real part of the quaternion is last.
+
+    Returns:
+        quaternions as tensor of shape (..., 4).
+    """
+    angles = torch.norm(axis_angle, p=2, dim=-1, keepdim=True)
+    half_angles = angles * 0.5
+    eps = 1e-6
+    small_angles = angles.abs() < eps
+    sin_half_angles_over_angles = torch.empty_like(angles)
+    sin_half_angles_over_angles[~small_angles] = (
+        torch.sin(half_angles[~small_angles]) / angles[~small_angles]
+    )
+    # for x small, sin(x/2) is about x/2 - (x/2)^3/6
+    # so sin(x/2)/x is about 1/2 - (x*x)/48
+    sin_half_angles_over_angles[small_angles] = (
+        0.5 - (angles[small_angles] * angles[small_angles]) / 48
+    )
+    quaternions = torch.cat(
+        [torch.cos(half_angles), axis_angle * sin_half_angles_over_angles], dim=-1
+    )
+    if w_last:
+        quaternions = wxyz_to_xyzw(quaternions)
+    return quaternions
+
+
+@torch.jit.script
+def matrix_to_quaternion(matrix: torch.Tensor, w_last: bool) -> torch.Tensor:
+    """
+    Convert rotations given as rotation matrices to quaternions.
+
+    Args:
+        matrix: Rotation matrices as tensor of shape (..., 3, 3).
+        w_last: If True, the real part of the quaternion is last.
+
+    Returns:
+        quaternions as tensor of shape (..., 4).
+    """
+    if matrix.size(-1) != 3 or matrix.size(-2) != 3:
+        raise ValueError(f"Invalid rotation matrix shape {matrix.shape}.")
+
+    batch_dim = matrix.shape[:-2]
+    m00, m01, m02, m10, m11, m12, m20, m21, m22 = torch.unbind(
+        matrix.reshape(batch_dim + (9,)), dim=-1
+    )
+
+    q_abs = _sqrt_positive_part(
+        torch.stack(
+            [
+                1.0 + m00 + m11 + m22,
+                1.0 + m00 - m11 - m22,
+                1.0 - m00 + m11 - m22,
+                1.0 - m00 - m11 + m22,
+            ],
+            dim=-1,
+        )
+    )
+
+    # we produce the desired quaternion multiplied by each of r, i, j, k
+    quat_by_rijk = torch.stack(
+        [
+            torch.stack([q_abs[..., 0] ** 2, m21 - m12, m02 - m20, m10 - m01], dim=-1),
+            torch.stack([m21 - m12, q_abs[..., 1] ** 2, m10 + m01, m02 + m20], dim=-1),
+            torch.stack([m02 - m20, m10 + m01, q_abs[..., 2] ** 2, m12 + m21], dim=-1),
+            torch.stack([m10 - m01, m20 + m02, m21 + m12, q_abs[..., 3] ** 2], dim=-1),
+        ],
+        dim=-2,
+    )
+
+    # We floor here at 0.1 but the exact level is not important; if q_abs is small,
+    # the candidate won't be picked.
+    flr = torch.tensor(0.1).to(dtype=q_abs.dtype, device=q_abs.device)
+    quat_candidates = quat_by_rijk / (2.0 * q_abs[..., None].max(flr))
+
+    # if not for numerical problems, quat_candidates[i] should be same (up to a sign),
+    # forall i; we pick the best-conditioned one (with the largest denominator)
+
+    quat_candidates = quat_candidates[
+        F.one_hot(q_abs.argmax(dim=-1), num_classes=4) > 0.5, :  # pyre-ignore[16]
+    ].reshape(batch_dim + (4,))
+
+    if w_last:
+        quat_candidates = wxyz_to_xyzw(quat_candidates)
+
+    return quat_candidates
diff --git a/source/unitreelab/unitreelab/MotionLib/isaac_utils/torch_utils.py b/source/unitreelab/unitreelab/MotionLib/isaac_utils/torch_utils.py
new file mode 100644
index 0000000..c879e24
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/isaac_utils/torch_utils.py
@@ -0,0 +1,210 @@
+# Copyright (c) 2018-2022, NVIDIA Corporation
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions are met:
+#
+# 1. Redistributions of source code must retain the above copyright notice, this
+#    list of conditions and the following disclaimer.
+#
+# 2. Redistributions in binary form must reproduce the above copyright notice,
+#    this list of conditions and the following disclaimer in the documentation
+#    and/or other materials provided with the distribution.
+#
+# 3. Neither the name of the copyright holder nor the names of its
+#    contributors may be used to endorse or promote products derived from
+#    this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+import torch
+from torch import Tensor
+import numpy as np
+from .rotations import quat_rotate, quat_from_angle_axis, normalize_angle
+from typing import Tuple
+
+
+@torch.jit.script
+def quat_to_angle_axis(q: Tensor, w_last: bool = False) -> Tuple[Tensor, Tensor]:
+    # computes axis-angle representation from quaternion q
+    # q must be normalized
+    min_theta = 1e-5
+    if not w_last:
+        qx, qy, qz, qw = 1, 2, 3, 0
+    else:
+        qx, qy, qz, qw = 0, 1, 2, 3
+
+    sin_theta = torch.sqrt(1 - q[..., qw] * q[..., qw])
+    angle = 2 * torch.acos(q[..., qw])
+    angle = normalize_angle(angle)
+    sin_theta_expand = sin_theta.unsqueeze(-1)
+    axis = q[..., qx:qz + 1] / sin_theta_expand
+
+    mask = torch.abs(sin_theta) > min_theta
+    default_axis = torch.zeros_like(axis)
+    default_axis[..., -1] = 1
+
+    angle = torch.where(mask, angle, torch.zeros_like(angle))
+    mask_expand = mask.unsqueeze(-1)
+    axis = torch.where(mask_expand, axis, default_axis)
+    return angle, axis
+
+
+@torch.jit.script
+def angle_axis_to_exp_map(angle: Tensor, axis: Tensor) -> Tensor:
+    # compute exponential map from axis-angle
+    angle_expand = angle.unsqueeze(-1)
+    exp_map = angle_expand * axis
+    return exp_map
+
+
+@torch.jit.script
+def quat_to_exp_map(q: Tensor, w_last: bool = False) -> Tensor:
+    # compute exponential map from quaternion
+    # q must be normalized
+    angle, axis = quat_to_angle_axis(q, w_last)
+    exp_map = angle_axis_to_exp_map(angle, axis)
+    return exp_map
+
+
+@torch.jit.script
+def quat_to_tan_norm(q: Tensor, w_last: bool) -> Tensor:
+    # represents a rotation using the tangent and normal vectors
+    ref_tan = torch.zeros_like(q[..., 0:3])
+    ref_tan[..., 0] = 1
+    tan = quat_rotate(q, ref_tan, w_last)
+
+    ref_norm = torch.zeros_like(q[..., 0:3])
+    ref_norm[..., -1] = 1
+    norm = quat_rotate(q, ref_norm, w_last)
+
+    norm_tan = torch.cat([tan, norm], dim=len(tan.shape) - 1)
+    return norm_tan
+
+
+@torch.jit.script
+def exp_map_to_angle_axis(exp_map: Tensor) -> Tuple[Tensor, Tensor]:
+    min_theta = 1e-5
+
+    angle = torch.norm(exp_map, dim=-1)
+    angle_exp = torch.unsqueeze(angle, dim=-1)
+    axis = exp_map / angle_exp
+    angle = normalize_angle(angle)
+
+    default_axis = torch.zeros_like(exp_map)
+    default_axis[..., -1] = 1
+
+    mask = torch.abs(angle) > min_theta
+    angle = torch.where(mask, angle, torch.zeros_like(angle))
+    mask_expand = mask.unsqueeze(-1)
+    axis = torch.where(mask_expand, axis, default_axis)
+
+    return angle, axis
+
+
+@torch.jit.script
+def exp_map_to_quat(exp_map: Tensor, w_last: bool) -> Tensor:
+    angle, axis = exp_map_to_angle_axis(exp_map)
+    q = quat_from_angle_axis(angle, axis, w_last)
+    return q
+
+
+@torch.jit.script
+def calc_heading(q: Tensor, w_last: bool) -> Tensor:
+    # calculate heading direction from quaternion
+    # the heading is the direction on the xy plane
+    # q must be normalized
+    ref_dir = torch.zeros_like(q[..., 0:3])
+    ref_dir[..., 0] = 1
+    rot_dir = quat_rotate(q, ref_dir, w_last)
+
+    heading = torch.atan2(rot_dir[..., 1], rot_dir[..., 0])
+    return heading
+
+
+@torch.jit.script
+def calc_heading_quat(q: Tensor, w_last: bool) -> Tensor:
+    # calculate heading rotation from quaternion
+    # the heading is the direction on the xy plane
+    # q must be normalized
+    heading = calc_heading(q, w_last)
+    axis = torch.zeros_like(q[..., 0:3])
+    axis[..., 2] = 1
+
+    heading_q = quat_from_angle_axis(heading, axis, w_last)
+    return heading_q
+
+
+@torch.jit.script
+def calc_heading_quat_inv(q: Tensor, w_last: bool = False) -> Tensor:
+    # calculate heading rotation from quaternion
+    # the heading is the direction on the xy plane
+    # q must be normalized
+    heading = calc_heading(q, w_last)
+    axis = torch.zeros_like(q[..., 0:3])
+    axis[..., 2] = 1
+
+    heading_q = quat_from_angle_axis(-heading, axis, w_last)
+    return heading_q
+
+
+@torch.jit.script
+def slerp(q0: Tensor, q1: Tensor, t: Tensor) -> Tensor:
+    cos_half_theta = torch.sum(q0 * q1, dim=-1)
+
+    neg_mask = cos_half_theta < 0
+    q1 = q1.clone()
+    q1[neg_mask] = -q1[neg_mask]
+    cos_half_theta = torch.abs(cos_half_theta)
+    cos_half_theta = torch.unsqueeze(cos_half_theta, dim=-1)
+
+    half_theta = torch.acos(cos_half_theta)
+    sin_half_theta = torch.sqrt(1.0 - cos_half_theta * cos_half_theta)
+
+    ratioA = torch.sin((1 - t) * half_theta) / sin_half_theta
+    ratioB = torch.sin(t * half_theta) / sin_half_theta
+
+    new_q = ratioA * q0 + ratioB * q1
+
+    new_q = torch.where(torch.abs(sin_half_theta) < 0.001, 0.5 * q0 + 0.5 * q1, new_q)
+    new_q = torch.where(torch.abs(cos_half_theta) >= 1, q0, new_q)
+
+    return new_q
+
+
+def get_axis_params(value, axis_idx, x_value=0., dtype=float, n_dims=3):
+    """construct arguments to `Vec` according to axis index.
+    """
+    zs = np.zeros((n_dims,))
+    assert axis_idx < n_dims, "the axis dim should be within the vector dimensions"
+    zs[axis_idx] = 1.
+    params = np.where(zs == 1., value, zs)
+    params[0] = x_value
+    return list(params.astype(dtype))
+
+
+def grad_norm(params):
+    grad_norm = 0.0
+    for p in params:
+        if p.grad is not None:
+            grad_norm += torch.sum(p.grad**2)
+    return torch.sqrt(grad_norm)
+
+
+def to_torch(x, dtype=torch.float, device='cuda:0', requires_grad=False) -> torch.Tensor:
+    return torch.tensor(x, dtype=dtype, device=device, requires_grad=requires_grad)
+
+
+@torch.jit.script
+def heading_to_vec(h_theta):
+    v = torch.stack([torch.cos(h_theta), torch.sin(h_theta)], dim=-1)
+    return v
diff --git a/source/unitreelab/unitreelab/MotionLib/motion_lib.py b/source/unitreelab/unitreelab/MotionLib/motion_lib.py
new file mode 100644
index 0000000..92b92ba
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/motion_lib.py
@@ -0,0 +1,1019 @@
+import os
+from copy import deepcopy
+from dataclasses import dataclass
+from typing import Any, List, Tuple
+from easydict import EasyDict
+
+import numpy as np
+import torch
+import yaml
+from lightning_fabric.utilities.rank_zero import _get_rank
+from torch import Tensor, nn
+
+from .isaac_utils import rotations, torch_utils
+from .isaac_utils.device_dtype_mixin import DeviceDtypeModuleMixin
+from .poselib.core.rotation3d import quat_angle_axis, quat_inverse, quat_mul_norm
+from .poselib.skeleton.skeleton3d import SkeletonMotion, SkeletonState
+
+
+@dataclass
+class MotionState:
+    root_pos: Tensor
+    root_rot: Tensor
+    dof_pos: Tensor
+    root_vel: Tensor
+    root_ang_vel: Tensor
+    dof_vel: Tensor
+    key_body_pos: Tensor
+    rb_pos: Tensor
+    rb_rot: Tensor
+    local_rot: Tensor
+    rb_vel: Tensor
+    rb_ang_vel: Tensor
+
+
+class LoadedMotions(nn.Module):
+    def __init__(
+        self,
+        motions: Tuple[SkeletonMotion],
+        motion_lengths: Tensor,
+        motion_weights: Tensor,
+        motion_timings: Tensor,
+        motion_fps: Tensor,
+        motion_dt: Tensor,
+        motion_num_frames: Tensor,
+        motion_files: Tuple[str],
+        sub_motion_to_motion: Tensor,
+        ref_respawn_offsets: Tensor,
+        text_embeddings: Tensor = None,
+        has_text_embeddings: Tensor = None,
+        supported_scene_ids: List[List[str]] = None,
+        **kwargs,  # Catch some nn.Module arguments that aren't needed
+    ):
+        super().__init__()
+        self.motions = motions
+        self.motion_files = motion_files
+        self.register_buffer("motion_lengths", motion_lengths, persistent=False)
+        self.register_buffer("motion_weights", motion_weights, persistent=False)
+        self.register_buffer("motion_timings", motion_timings, persistent=False)
+        self.register_buffer("motion_fps", motion_fps, persistent=False)
+        self.register_buffer("motion_dt", motion_dt, persistent=False)
+        self.register_buffer("motion_num_frames", motion_num_frames, persistent=False)
+        self.register_buffer(
+            "sub_motion_to_motion", sub_motion_to_motion, persistent=False
+        )
+        self.register_buffer(
+            "ref_respawn_offsets", ref_respawn_offsets, persistent=False
+        )
+        if text_embeddings is None:
+            text_embeddings = torch.zeros(len(motions), 3, 512, dtype=torch.float32)
+            has_text_embeddings = torch.zeros(len(motions), dtype=torch.bool)
+        self.register_buffer("text_embeddings", text_embeddings, persistent=False)
+        self.register_buffer(
+            "has_text_embeddings", has_text_embeddings, persistent=False
+        )
+        if supported_scene_ids is None:
+            supported_scene_ids = [None for _ in range(len(motions))]
+        self.supported_scene_ids = supported_scene_ids
+
+
+class MotionLib(DeviceDtypeModuleMixin):
+    gts: Tensor
+    grs: Tensor
+    lrs: Tensor
+    gvs: Tensor
+    gavs: Tensor
+    grvs: Tensor
+    gravs: Tensor
+    dvs: Tensor
+    length_starts: Tensor
+    motion_ids: Tensor
+    key_body_ids: Tensor
+
+    def __init__(
+        self,
+        motion_file,
+        dof_body_ids,
+        dof_offsets,
+        key_body_ids,
+        device="cpu",
+        ref_height_adjust: float = 0,
+        target_frame_rate: int = 30,
+        create_text_embeddings: bool = False,
+        spawned_scene_ids: List[str] = None,
+        fix_motion_heights: bool = True,
+        skeleton_tree: Any = None,
+        local_rot_conversion: Tensor = None,
+        w_last: bool = True,
+    ):
+        super().__init__()
+        self.w_last = w_last
+        self.fix_heights = fix_motion_heights
+        self.skeleton_tree = skeleton_tree
+        self.create_text_embeddings = create_text_embeddings
+        self.dof_body_ids = dof_body_ids
+        self.dof_offsets = dof_offsets
+        self.num_dof = dof_offsets[-1]
+        self.ref_height_adjust = ref_height_adjust
+        self.local_rot_conversion = local_rot_conversion
+
+        self.register_buffer(
+            "key_body_ids",
+            key_body_ids.clone().detach(),
+            persistent=False,
+        )
+
+        if str(motion_file).split(".")[-1] in ["yaml", "npy", "npz", "np"]:
+            print("Loading motions from yaml/npy file")
+            self._load_motions(motion_file, target_frame_rate)
+        else:
+            rank = _get_rank()
+            if rank is None:
+                rank = 0
+            # This is used for large motion files that are split across multiple GPUs
+            motion_file = motion_file.replace("_slurmrank", f"_{rank}")
+            print(f"Loading motions from state file: {motion_file}")
+
+            with open(motion_file, "rb") as file:
+                state: LoadedMotions = torch.load(file, map_location="cpu")
+
+            # Create LoadedMotions instance with loaded state dict
+            # We re-create to enable backwards compatibility. This allows LoadedMotions class to accept "None" values and set defaults if needed.
+            state_dict = {
+                **vars(state),
+                **{k: v for k, v in state._buffers.items() if v is not None},
+            }
+            self.state = LoadedMotions(**state_dict)
+
+        motions = self.state.motions
+        self.register_buffer(
+            "gts",
+            torch.cat([m.global_translation for m in motions], dim=0).to(
+                dtype=torch.float32
+            ),
+            persistent=False,
+        )
+        self.register_buffer(
+            "grs",
+            torch.cat([m.global_rotation for m in motions], dim=0).to(
+                dtype=torch.float32
+            ),
+            persistent=False,
+        )
+        self.register_buffer(
+            "lrs",
+            torch.cat([m.local_rotation for m in motions], dim=0).to(
+                dtype=torch.float32
+            ),
+            persistent=False,
+        )
+        self.register_buffer(
+            "grvs",
+            torch.cat([m.global_root_velocity for m in motions], dim=0).to(
+                dtype=torch.float32
+            ),
+            persistent=False,
+        )
+        self.register_buffer(
+            "gravs",
+            torch.cat([m.global_root_angular_velocity for m in motions], dim=0).to(
+                dtype=torch.float32
+            ),
+            persistent=False,
+        )
+        self.register_buffer(
+            "gavs",
+            torch.cat([m.global_angular_velocity for m in motions], dim=0).to(
+                dtype=torch.float32
+            ),
+            persistent=False,
+        )
+        self.register_buffer(
+            "gvs",
+            torch.cat([m.global_velocity for m in motions], dim=0).to(
+                dtype=torch.float32
+            ),
+            persistent=False,
+        )
+        self.register_buffer(
+            "dvs",
+            torch.cat([m.dof_vels for m in motions], dim=0).to(
+                device=device, dtype=torch.float32
+            ),
+            persistent=False,
+        )
+
+        lengths = self.state.motion_num_frames
+        lengths_shifted = lengths.roll(1)
+        lengths_shifted[0] = 0
+        self.register_buffer(
+            "length_starts", lengths_shifted.cumsum(0), persistent=False
+        )
+
+        self.register_buffer(
+            "motion_ids",
+            torch.arange(
+                len(self.state.motions), dtype=torch.long, device=self._device
+            ),
+            persistent=False,
+        )
+
+        scenes_per_motion, motion_to_scene_ids = self.parse_scenes(spawned_scene_ids)
+
+        self.register_buffer(
+            "scenes_per_motion",
+            torch.tensor(scenes_per_motion, device=self._device, dtype=torch.long),
+            persistent=False,
+        )
+
+        self.register_buffer(
+            "motion_to_scene_ids",
+            torch.tensor(motion_to_scene_ids, device=self._device, dtype=torch.long),
+            persistent=False,
+        )
+
+        self.to(device)
+
+    def num_motions(self):
+        """Returns the number of motions in the state.
+
+        Returns:
+            int: The number of motions.
+        """
+        return len(self.state.motions)
+
+    def num_sub_motions(self):
+        """Returns the number of sub-motions in the state.
+
+        A sub-motion is a segment or a part of a larger motion sequence.
+        In the context of this code, a motion can be divided into multiple sub-motions,
+        each representing a smaller portion of the overall motion.
+        These sub-motions are used to manage and manipulate parts of the motion sequence
+        independently, allowing for more granular control and analysis of the motion data.
+
+        Returns:
+            int: The number of sub-motions.
+        """
+        return self.state.motion_weights.shape[0]
+
+    def get_total_length(self):
+        """Returns the total length of all motions.
+
+        Returns:
+            int: The total length of all motions.
+        """
+        return sum(self.state.motion_lengths)
+
+    def get_total_trainable_length(self):
+        """Returns the total trainable length of all motions.
+
+        The total trainable length is calculated by summing the differences
+        between the end and start times of each motion timing.
+
+        Returns:
+            int: The total trainable length of all motions.
+        """
+        return sum(self.state.motion_timings[:, 1] - self.state.motion_timings[:, 0])
+
+    def get_motion(self, motion_id):
+        return self.state.motions[motion_id]
+
+    def sample_motions(self, n, valid_mask=None):
+        if valid_mask is not None:
+            weights = self.state.motion_weights.clone()
+            weights[~valid_mask] = 0
+        else:
+            weights = self.state.motion_weights
+
+        sub_motion_ids = torch.multinomial(weights, num_samples=n, replacement=True)
+
+        return sub_motion_ids
+
+    def sample_other_motions(self, already_chosen_ids: Tensor) -> Tensor:
+        """Samples other motions that are not in the already chosen IDs.
+
+        Args:
+            already_chosen_ids (Tensor): A tensor containing the IDs of motions that have already been chosen.
+
+        Returns:
+            Tensor: A tensor containing the IDs of the sampled motions that are not in the already chosen IDs.
+        """
+        n = already_chosen_ids.shape[0]
+        motion_weights = self.state.motion_weights.unsqueeze(0).tile([n, 1])
+        motion_weights = motion_weights.scatter(
+            1, already_chosen_ids.unsqueeze(-1), torch.zeros_like(motion_weights)
+        )
+        sub_motion_ids = torch.multinomial(motion_weights, num_samples=1).squeeze(-1)
+        return sub_motion_ids
+
+    def sample_text_embeddings(self, sub_motion_ids: Tensor) -> Tensor:
+        """Samples text embeddings for the given sub-motion IDs.
+
+        Args:
+            sub_motion_ids (Tensor): A tensor containing the IDs of the sub-motions.
+
+        Returns:
+            Tensor: A tensor containing the sampled text embeddings for the given sub-motion IDs.
+        """
+        if hasattr(self.state, "text_embeddings"):
+            indices = torch.randint(
+                0, 3, (sub_motion_ids.shape[0],), device=self.device
+            )
+            return self.state.text_embeddings[sub_motion_ids, indices]
+        return 0
+
+    def sample_time(self, sub_motion_ids, truncate_time=None):
+        phase = torch.rand(sub_motion_ids.shape, device=self.device)
+
+        motion_len = (
+            self.state.motion_timings[sub_motion_ids, 1]
+            - self.state.motion_timings[sub_motion_ids, 0]
+        )
+
+        if truncate_time is not None:
+            assert truncate_time >= 0.0
+            motion_len -= truncate_time
+            assert torch.all(motion_len >= 0)
+
+        motion_time = phase * motion_len
+        motion_time = motion_time + self.state.motion_timings[sub_motion_ids, 0]
+
+        return motion_time
+
+    def get_sub_motion_length(self, sub_motion_ids):
+        return (
+            self.state.motion_timings[sub_motion_ids, 1]
+            - self.state.motion_timings[sub_motion_ids, 0]
+        )
+
+    def get_motion_length(self, sub_motion_ids):
+        motion_ids = self.state.sub_motion_to_motion[sub_motion_ids]
+        return self.state.motion_lengths[motion_ids]
+
+    def get_motion_state(
+        self, sub_motion_ids, motion_times, joint_3d_format="exp_map"
+    ) -> MotionState:
+        motion_ids = self.state.sub_motion_to_motion[sub_motion_ids]
+
+        motion_len = self.state.motion_lengths[motion_ids]
+        motion_times = motion_times.clip(min=0).clip(
+            max=motion_len
+        )  # Making sure time is in bounds
+
+        num_frames = self.state.motion_num_frames[motion_ids]
+        dt = self.state.motion_dt[motion_ids]
+
+        frame_idx0, frame_idx1, blend = self._calc_frame_blend(
+            motion_times, motion_len, num_frames, dt
+        )
+
+        f0l = frame_idx0 + self.length_starts[motion_ids]
+        f1l = frame_idx1 + self.length_starts[motion_ids]
+
+        root_pos0 = self.gts[f0l, 0]
+        root_pos1 = self.gts[f1l, 0]
+
+        root_rot0 = self.grs[f0l, 0]
+        root_rot1 = self.grs[f1l, 0]
+
+        local_rot0 = self.lrs[f0l]
+        local_rot1 = self.lrs[f1l]
+
+        root_vel0 = self.grvs[f0l]
+        root_vel1 = self.grvs[f1l]
+
+        root_ang_vel0 = self.gravs[f0l]
+        root_ang_vel1 = self.gravs[f1l]
+
+        global_vel0 = self.gvs[f0l]
+        global_vel1 = self.gvs[f1l]
+
+        global_ang_vel0 = self.gavs[f0l]
+        global_ang_vel1 = self.gavs[f1l]
+
+        key_body_pos0 = self.gts[f0l.unsqueeze(-1), self.key_body_ids.unsqueeze(0)]
+        key_body_pos1 = self.gts[f1l.unsqueeze(-1), self.key_body_ids.unsqueeze(0)]
+
+        dof_vel0 = self.dvs[f0l]
+        dof_vel1 = self.dvs[f1l]
+
+        rb_pos0 = self.gts[f0l]
+        rb_pos1 = self.gts[f1l]
+
+        rb_rot0 = self.grs[f0l]
+        rb_rot1 = self.grs[f1l]
+
+        vals = [
+            root_pos0,
+            root_pos1,
+            local_rot0,
+            local_rot1,
+            root_vel0,
+            root_vel1,
+            root_ang_vel0,
+            root_ang_vel1,
+            global_vel0,
+            global_vel1,
+            global_ang_vel0,
+            global_ang_vel1,
+            dof_vel0,
+            dof_vel1,
+            key_body_pos0,
+            key_body_pos1,
+            rb_pos0,
+            rb_pos1,
+            rb_rot0,
+            rb_rot1,
+        ]
+        for v in vals:
+            assert v.dtype != torch.float64
+
+        blend = blend.unsqueeze(-1)
+
+        root_pos: Tensor = (1.0 - blend) * root_pos0 + blend * root_pos1
+        root_pos[:, 2] += self.ref_height_adjust
+
+        root_rot: Tensor = torch_utils.slerp(root_rot0, root_rot1, blend)
+
+        blend_exp = blend.unsqueeze(-1)
+        key_body_pos = (1.0 - blend_exp) * key_body_pos0 + blend_exp * key_body_pos1
+        key_body_pos[:, :, 2] += self.ref_height_adjust
+
+        local_rot = torch_utils.slerp(
+            local_rot0, local_rot1, torch.unsqueeze(blend, axis=-1)
+        )
+
+        if hasattr(self, "dof_pos"):  # H1 joints
+            dof_pos = (1.0 - blend) * self.dof_pos[f0l] + blend * self.dof_pos[f1l]
+        else:
+            dof_pos: Tensor = self._local_rotation_to_dof(local_rot, joint_3d_format)
+
+        root_vel = (1.0 - blend) * root_vel0 + blend * root_vel1
+        root_ang_vel = (1.0 - blend) * root_ang_vel0 + blend * root_ang_vel1
+        dof_vel = (1.0 - blend) * dof_vel0 + blend * dof_vel1
+        rb_pos = (1.0 - blend_exp) * rb_pos0 + blend_exp * rb_pos1
+        rb_pos[:, :, 2] += self.ref_height_adjust
+        rb_rot = torch_utils.slerp(rb_rot0, rb_rot1, blend_exp)
+        global_vel = (1.0 - blend_exp) * global_vel0 + blend_exp * global_vel1
+        global_ang_vel = (
+            1.0 - blend_exp
+        ) * global_ang_vel0 + blend_exp * global_ang_vel1
+
+        motion_state = MotionState(
+            root_pos=root_pos,
+            root_rot=root_rot,
+            root_vel=root_vel,
+            root_ang_vel=root_ang_vel,
+            key_body_pos=key_body_pos,
+            dof_pos=dof_pos,
+            dof_vel=dof_vel,
+            local_rot=local_rot,
+            rb_pos=rb_pos,
+            rb_rot=rb_rot,
+            rb_vel=global_vel,
+            rb_ang_vel=global_ang_vel,
+        )
+
+        return motion_state
+
+    @staticmethod
+    def _load_motion_file(motion_file):
+        return SkeletonMotion.from_file(motion_file)
+
+    def _load_motions(self, motion_file, target_frame_rate):
+        if self.create_text_embeddings:
+            from transformers import AutoTokenizer, XCLIPTextModel
+
+            model = XCLIPTextModel.from_pretrained("microsoft/xclip-base-patch32")
+            tokenizer = AutoTokenizer.from_pretrained("microsoft/xclip-base-patch32")
+
+        motions = []
+        motion_lengths = []
+        motion_dt = []
+        motion_num_frames = []
+        text_embeddings = []
+        has_text_embeddings = []
+        (
+            motion_files,
+            motion_weights,
+            motion_timings,
+            motion_fpses,
+            sub_motion_to_motion,
+            ref_respawn_offsets,
+            motion_labels,
+            supported_scene_ids,
+        ) = self._fetch_motion_files(motion_file)
+
+        num_motion_files = len(motion_files)
+
+        for f in range(num_motion_files):
+            curr_file = motion_files[f]
+
+            print(
+                "Loading {:d}/{:d} motion files: {:s}".format(
+                    f + 1, num_motion_files, curr_file
+                )
+            )
+            curr_motion = self._load_motion_file(curr_file)
+
+            curr_motion = fix_motion_fps(
+                curr_motion,
+                motion_fpses[f],
+                target_frame_rate,
+                self.skeleton_tree,
+            )
+            motion_fpses[f] = float(curr_motion.fps)
+
+            if self.fix_heights:
+                curr_motion = self.fix_motion_heights(curr_motion, self.skeleton_tree)
+
+            curr_dt = 1.0 / motion_fpses[f]
+
+            num_frames = curr_motion.global_translation.shape[0]
+            curr_len = 1.0 / motion_fpses[f] * (num_frames - 1)
+
+            motion_dt.append(curr_dt)
+            motion_num_frames.append(num_frames)
+
+            curr_dof_vels = self._compute_motion_dof_vels(curr_motion)
+            curr_motion.dof_vels = curr_dof_vels
+
+            motions.append(curr_motion)
+            motion_lengths.append(curr_len)
+
+        num_sub_motions = len(sub_motion_to_motion)
+
+        for f in range(num_sub_motions):
+            # Incase start/end weren't provided, set to (0, motion_length)
+            motion_f = sub_motion_to_motion[f]
+            if motion_timings[f][1] == -1:
+                motion_timings[f][1] = motion_lengths[motion_f]
+
+            motion_timings[f][1] = min(
+                motion_timings[f][1], motion_lengths[motion_f]
+            )  # CT hack: fix small timing differences
+
+            assert (
+                motion_timings[f][0] < motion_timings[f][1]
+            ), f"Motion start {motion_timings[f][0]} >= motion end {motion_timings[f][1]} in motion {motion_f}"
+
+            if self.create_text_embeddings and motion_labels[f][0] != "":
+                with torch.inference_mode():
+                    inputs = tokenizer(
+                        motion_labels[f],
+                        padding=True,
+                        truncation=True,
+                        return_tensors="pt",
+                    )
+                    outputs = model(**inputs)
+                    pooled_output = outputs.pooler_output  # pooled (EOS token) states
+                    text_embeddings.append(pooled_output)  # should be [3, 512]
+                    has_text_embeddings.append(True)
+            else:
+                text_embeddings.append(
+                    torch.zeros((3, 512), dtype=torch.float32)
+                )  # just hold something temporary
+                has_text_embeddings.append(False)
+
+        motion_lengths = torch.tensor(
+            motion_lengths, device=self._device, dtype=torch.float32
+        )
+
+        motion_weights = torch.tensor(
+            motion_weights, dtype=torch.float32, device=self._device
+        )
+        motion_weights /= motion_weights.sum()
+
+        motion_timings = torch.tensor(
+            motion_timings, dtype=torch.float32, device=self._device
+        )
+
+        sub_motion_to_motion = torch.tensor(
+            sub_motion_to_motion, dtype=torch.long, device=self._device
+        )
+
+        ref_respawn_offsets = torch.tensor(
+            ref_respawn_offsets, dtype=torch.float32, device=self._device
+        )
+
+        motion_fpses = torch.tensor(
+            motion_fpses, device=self._device, dtype=torch.float32
+        )
+        motion_dt = torch.tensor(motion_dt, device=self._device, dtype=torch.float32)
+        motion_num_frames = torch.tensor(motion_num_frames, device=self._device)
+
+        text_embeddings = torch.stack(text_embeddings).detach().to(device=self._device)
+        has_text_embeddings = torch.tensor(
+            has_text_embeddings, dtype=torch.bool, device=self._device
+        )
+
+        self.state = LoadedMotions(
+            motions=tuple(motions),
+            motion_lengths=motion_lengths,
+            motion_weights=motion_weights,
+            motion_timings=motion_timings,
+            motion_fps=motion_fpses,
+            motion_dt=motion_dt,
+            motion_num_frames=motion_num_frames,
+            motion_files=tuple(motion_files),
+            sub_motion_to_motion=sub_motion_to_motion,
+            ref_respawn_offsets=ref_respawn_offsets,
+            text_embeddings=text_embeddings,
+            has_text_embeddings=has_text_embeddings,
+            supported_scene_ids=supported_scene_ids,
+        )
+
+        num_motions = self.num_motions()
+        total_len = self.get_total_length()
+
+        print(
+            "Loaded {:d} motions with a total length of {:.3f}s.".format(
+                num_motions, total_len
+            )
+        )
+
+        num_sub_motions = self.num_sub_motions()
+        total_trainable_len = self.get_total_trainable_length()
+
+        print(
+            "Loaded {:d} sub motions with a total trainable length of {:.3f}s.".format(
+                num_sub_motions, total_trainable_len
+            )
+        )
+
+    def _fetch_motion_files(self, motion_file):
+        ext = os.path.splitext(motion_file)[1]
+        if ext == ".yaml":
+            dir_name = os.path.dirname(motion_file)
+            motion_files = []
+            sub_motion_to_motion = []
+            ref_respawn_offsets = []
+            motion_weights = []
+            motion_timings = []
+            motion_fpses = []
+            motion_labels = []
+            supported_scene_ids = []
+            with open(os.path.join(os.getcwd(), motion_file), "r") as f:
+                motion_config = EasyDict(yaml.load(f, Loader=yaml.SafeLoader))
+
+            motion_list = sorted(
+                motion_config.motions,
+                key=lambda x: 1e6 if "idx" not in x else int(x.idx),
+            )
+
+            motion_index = 0
+
+            for motion_id, motion_entry in enumerate(motion_list):
+                curr_file = motion_entry.file
+                curr_file = os.path.join(dir_name, curr_file)
+                motion_files.append(curr_file)
+                motion_fpses.append(motion_entry.get("fps", None))
+
+                if "sub_motions" not in motion_entry:
+                    motion_entry.sub_motions = [deepcopy(motion_entry)]
+                    motion_entry.sub_motions[0].idx = motion_index
+
+                for sub_motion in sorted(
+                    motion_entry.sub_motions, key=lambda x: int(x.idx)
+                ):
+                    curr_weight = sub_motion.weight
+                    assert curr_weight >= 0
+
+                    assert motion_index == sub_motion.idx
+
+                    motion_weights.append(curr_weight)
+
+                    sub_motion_to_motion.append(motion_id)
+
+                    ref_respawn_offset = sub_motion.get("ref_respawn_offset", 0)
+                    ref_respawn_offsets.append(ref_respawn_offset)
+
+                    if "timings" in sub_motion:
+                        curr_timing = sub_motion.timings
+                        start = curr_timing.start
+                        end = curr_timing.end
+                    else:
+                        start = 0
+                        end = -1
+
+                    motion_timings.append([start, end])
+
+                    sub_motion_labels = []
+                    if "labels" in sub_motion:
+                        # We assume 3 labels for each motion.
+                        # If there are fewer than 3 labels, the last label is repeated to fill the list.
+                        # If there are no labels, an empty string is used as the label.
+                        for label in sub_motion.labels:
+                            sub_motion_labels.append(label)
+                            if len(sub_motion_labels) == 3:
+                                break
+                        if len(sub_motion_labels) == 0:
+                            sub_motion_labels.append("")
+                        while len(sub_motion_labels) < 3:
+                            sub_motion_labels.append(sub_motion_labels[-1])
+                    else:
+                        sub_motion_labels.append("")
+                        sub_motion_labels.append("")
+                        sub_motion_labels.append("")
+
+                    motion_labels.append(sub_motion_labels)
+
+                    if "supported_scenes" in sub_motion:
+                        supported_scene_ids.append(sub_motion.supported_scenes)
+                    else:
+                        supported_scene_ids.append(None)
+
+                    motion_index += 1
+        else:
+            motion_files = [motion_file]
+            motion_weights = [1.0]
+            motion_timings = [[0, -1]]
+            motion_fpses = [None]
+            sub_motion_to_motion = [0]
+            ref_respawn_offsets = [0]
+            motion_labels = [["", "", ""]]
+            supported_scene_ids = [None]
+        return (
+            motion_files,
+            motion_weights,
+            motion_timings,
+            motion_fpses,
+            sub_motion_to_motion,
+            ref_respawn_offsets,
+            motion_labels,
+            supported_scene_ids,
+        )
+
+    def _calc_frame_blend(self, time, len, num_frames, dt):
+        phase = time / len
+        phase = torch.clip(phase, 0.0, 1.0)
+
+        frame_idx0 = (phase * (num_frames - 1)).long()
+        frame_idx1 = torch.min(frame_idx0 + 1, num_frames - 1)
+        blend = (time - frame_idx0 * dt) / dt
+
+        return frame_idx0, frame_idx1, blend
+
+    def _get_num_bodies(self):
+        motion = self.get_motion(0)
+        num_bodies = motion.num_joints
+        return num_bodies
+
+    def _compute_motion_dof_vels(self, motion: SkeletonMotion):
+        num_frames = motion.global_translation.shape[0]
+        dt = 1.0 / motion.fps
+        dof_vels = []
+
+        for f in range(num_frames - 1):
+            local_rot0 = motion.local_rotation[f]
+            local_rot1 = motion.local_rotation[f + 1]
+            frame_dof_vel = self._local_rotation_to_dof_vel(local_rot0, local_rot1, dt)
+            dof_vels.append(frame_dof_vel)
+
+        dof_vels.append(dof_vels[-1])
+        dof_vels = torch.stack(dof_vels, dim=0)
+
+        return dof_vels
+
+    # jp hack
+    # get rid of this ASAP, need a proper way of projecting from max coords to reduced coords
+    def _local_rotation_to_dof(self, local_rot, joint_3d_format):
+        body_ids = self.dof_body_ids
+        dof_offsets = self.dof_offsets
+
+        n = local_rot.shape[0]
+        dof_pos = torch.zeros((n, self.num_dof), dtype=torch.float, device=self._device)
+
+        for j in range(len(body_ids)):
+            body_id = body_ids[j]
+            joint_offset = dof_offsets[j]
+            joint_size = dof_offsets[j + 1] - joint_offset
+
+            if joint_size == 3:
+                joint_q = local_rot[:, body_id]
+                if joint_3d_format == "exp_map":
+                    formatted_joint = torch_utils.quat_to_exp_map(joint_q, w_last=True)
+                elif joint_3d_format == "xyz":
+                    x, y, z = rotations.get_euler_xyz(joint_q, w_last=True)
+                    formatted_joint = torch.stack([x, y, z], dim=-1)
+                else:
+                    raise ValueError(f"Unknown 3d format '{joint_3d_format}'")
+
+                dof_pos[:, joint_offset: (joint_offset + joint_size)] = formatted_joint
+            elif joint_size == 1:
+                joint_q = local_rot[:, body_id]
+                joint_theta, joint_axis = torch_utils.quat_to_angle_axis(
+                    joint_q, w_last=True
+                )
+                joint_theta = (
+                    joint_theta * joint_axis[..., 1]
+                )  # assume joint is always along y axis
+
+                joint_theta = rotations.normalize_angle(joint_theta)
+                dof_pos[:, joint_offset] = joint_theta
+
+            else:
+                print("Unsupported joint type")
+                assert False
+
+        return dof_pos
+
+    def _local_rotation_to_dof_vel(self, local_rot0, local_rot1, dt):
+        body_ids = self.dof_body_ids
+        dof_offsets = self.dof_offsets
+
+        dof_vel = torch.zeros([self.num_dof], device=self._device)
+
+        diff_quat_data = quat_mul_norm(quat_inverse(local_rot0), local_rot1)
+        diff_angle, diff_axis = quat_angle_axis(diff_quat_data)
+        local_vel = diff_axis * diff_angle.unsqueeze(-1) / dt
+        local_vel = local_vel
+
+        for j in range(len(body_ids)):
+            body_id = body_ids[j]
+            joint_offset = dof_offsets[j]
+            joint_size = dof_offsets[j + 1] - joint_offset
+
+            if joint_size == 3:
+                joint_vel = local_vel[body_id]
+                dof_vel[joint_offset: (joint_offset + joint_size)] = joint_vel
+
+            elif joint_size == 1:
+                assert joint_size == 1
+                joint_vel = local_vel[body_id]
+                dof_vel[joint_offset] = joint_vel[
+                    1
+                ]  # assume joint is always along y axis
+
+            else:
+                print("Unsupported joint type")
+                assert False
+
+        return dof_vel
+
+    def parse_scenes(self, spawned_scene_ids):
+        # If motions may have supported scenes, create the mapping to allow sampling scenes for motions.
+        motion_to_scene_ids = []
+        scenes_per_motion = []
+        if hasattr(self.state, "supported_scene_ids") and spawned_scene_ids is not None:
+
+            def indices(lst, element):
+                result = []
+                offset = -1
+                while True:
+                    try:
+                        offset = lst.index(element, offset + 1)
+                    except ValueError:
+                        return result
+                    result.append(offset)
+
+            max_num_scenes = max(
+                max(
+                    [
+                        len(scene_ids) if scene_ids is not None else 0
+                        for scene_ids in self.state.supported_scene_ids
+                    ]
+                ),
+                len(spawned_scene_ids),
+            )
+
+            for i in range(len(self.state.supported_scene_ids)):
+                if self.state.supported_scene_ids[i] is None:
+                    motion_to_scene_ids.append([-1] * max_num_scenes)
+                    scenes_per_motion.append(-1)
+                else:
+                    all_scene_ids = []
+                    for scene_id in self.state.supported_scene_ids[i]:
+                        if scene_id in spawned_scene_ids:
+                            # store all indices that match, multiple options may exist
+                            scene_indices = indices(spawned_scene_ids, scene_id)
+                            for scene_index in scene_indices:
+                                all_scene_ids.append(scene_index)
+
+                    scenes_per_motion.append(len(all_scene_ids))
+
+                    if len(all_scene_ids) == 0:
+                        all_scene_ids = [-1]
+                    while len(all_scene_ids) < max_num_scenes:
+                        all_scene_ids.append(-1)
+                    motion_to_scene_ids.append(all_scene_ids)
+
+        return scenes_per_motion, motion_to_scene_ids
+
+    def sample_motions_scene_aware(
+        self,
+        num_motions,
+        available_scenes,
+        single_robot_in_scene,
+        with_replacement=True,
+        available_motion_mask=None,
+    ):
+        sampled_motions = []
+        occupied_scenes = []
+
+        if available_motion_mask is None:
+            available_motion_mask = torch.ones(
+                len(self.scenes_per_motion), dtype=torch.bool, device=self.device
+            )
+
+        motion_weights = self.state.motion_weights.clone()
+
+        while len(sampled_motions) < num_motions:
+            # Create a view of available motions
+            for i, num_scenes in enumerate(self.scenes_per_motion):
+                if num_scenes != -1 and not torch.any(
+                    available_scenes[self.motion_to_scene_ids[i, :num_scenes]]
+                ):
+                    available_motion_mask[i] = False
+
+            # Sample a motion based on weights
+            motion_weights[~available_motion_mask] = 0
+            if motion_weights.sum() == 0:
+                raise ValueError("No more valid motions available")
+            sampled_motion = torch.multinomial(motion_weights, num_samples=1).item()
+            sampled_motions.append(sampled_motion)
+
+            if not with_replacement:
+                available_motion_mask[sampled_motion] = False
+
+            # Sample a scene for the motion if needed
+            if self.scenes_per_motion[sampled_motion] != -1:
+                num_scenes = self.scenes_per_motion[sampled_motion]
+                available_scene_mask = available_scenes[
+                    self.motion_to_scene_ids[sampled_motion, :num_scenes]
+                ]
+                valid_scenes = self.motion_to_scene_ids[sampled_motion, :num_scenes][
+                    available_scene_mask
+                ]
+                if valid_scenes.numel() > 0:
+                    scene = valid_scenes[
+                        torch.randint(0, valid_scenes.numel(), (1,)).item()
+                    ]
+                    occupied_scenes.append(scene)
+                    if single_robot_in_scene[scene]:
+                        available_scenes[scene] = False
+                else:
+                    raise ValueError("No more valid scenes available")
+            else:
+                occupied_scenes.append(-1)
+
+        return torch.tensor(
+            sampled_motions, device=self.device, dtype=torch.long
+        ), torch.tensor(occupied_scenes, device=self.device, dtype=torch.long)
+
+    @staticmethod
+    def fix_motion_heights(motion, skeleton_tree):
+        if skeleton_tree is None:
+            if hasattr(motion, "skeleton_tree"):
+                skeleton_tree = motion.skeleton_tree
+        body_heights = motion.global_translation[..., 2]
+        min_height = body_heights.min()
+
+        if skeleton_tree is None:
+            motion.global_translation[..., 2] -= min_height
+            return motion
+
+        root_translation = motion.root_translation
+        root_translation[:, 2] -= min_height
+
+        new_sk_state = SkeletonState.from_rotation_and_root_translation(
+            skeleton_tree,
+            motion.global_rotation,
+            root_translation,
+            is_local=False,
+        )
+
+        new_motion = SkeletonMotion.from_skeleton_state(new_sk_state, fps=motion.fps)
+
+        return new_motion
+
+
+def fix_motion_fps(
+    motion,
+    orig_fps,
+    target_frame_rate,
+    skeleton_tree,
+):
+    if skeleton_tree is None:
+        if hasattr(motion, "skeleton_tree"):
+            skeleton_tree = motion.skeleton_tree
+        else:
+            return motion
+
+    if orig_fps is None:
+        orig_fps = motion.fps
+
+    skip = int(np.round(orig_fps / target_frame_rate))
+
+    lr = motion.local_rotation[::skip]
+    rt = motion.root_translation[::skip]
+
+    new_sk_state = SkeletonState.from_rotation_and_root_translation(
+        skeleton_tree,
+        lr,
+        rt,
+        is_local=True,
+    )
+    new_motion = SkeletonMotion.from_skeleton_state(new_sk_state, fps=target_frame_rate)
+
+    return new_motion
diff --git a/source/unitreelab/unitreelab/MotionLib/motion_lib_h1.py b/source/unitreelab/unitreelab/MotionLib/motion_lib_h1.py
new file mode 100644
index 0000000..59a2376
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/motion_lib_h1.py
@@ -0,0 +1,246 @@
+from typing import List, Any
+
+import torch
+from easydict import EasyDict
+
+from .motion_lib import MotionLib, fix_motion_fps, LoadedMotions
+
+
+class H1_MotionLib(MotionLib):
+    def __init__(
+        self,
+        motion_file,
+        dof_body_ids,
+        dof_offsets,
+        key_body_ids,
+        device="cpu",
+        ref_height_adjust: float = 0,
+        target_frame_rate: int = 30,
+        w_last: bool = True,
+        create_text_embeddings: bool = False,
+        spawned_scene_ids: List[str] = None,
+        fix_motion_heights: bool = True,
+        skeleton_tree: Any = None,
+    ):
+
+        super().__init__(
+            motion_file=motion_file,
+            dof_body_ids=dof_body_ids,
+            dof_offsets=dof_offsets,
+            key_body_ids=key_body_ids,
+            device=device,
+            ref_height_adjust=ref_height_adjust,
+            target_frame_rate=target_frame_rate,
+            w_last=w_last,
+            create_text_embeddings=create_text_embeddings,
+            spawned_scene_ids=spawned_scene_ids,
+            fix_motion_heights=fix_motion_heights,
+            skeleton_tree=skeleton_tree,
+        )
+
+        motions = self.state.motions
+        self.register_buffer(
+            "dof_pos",
+            torch.cat([m.dof_pos for m in motions], dim=0).to(
+                device=device, dtype=torch.float32
+            ),
+            persistent=False,
+        )
+
+    @staticmethod
+    def _load_motion_file(motion_file):
+        motion = EasyDict(torch.load(motion_file))
+        return motion
+
+    def _compute_motion_dof_vels(self, motion):
+        # We pre-compute the dof vels in h1_humanoid_batch fk.
+        return motion.dof_vels
+
+    def fix_motion_heights(self, motion, skeleton_tree):
+        body_heights = motion.global_translation[..., 2].clone()
+        # TODO: this is a bit hacky and hardcoded for the current defined key body ids
+        # left ankle: self.key_body_ids[0]
+        # right ankle: self.key_body_ids[1]
+        body_heights[:, self.key_body_ids[0]] -= 0.08
+        body_heights[:, self.key_body_ids[1]] -= 0.08
+        min_height = body_heights.min()
+
+        motion.global_translation[..., 2] -= min_height
+        motion.global_translation[..., 2] -= 0.02
+        return motion
+
+    def _load_motions(self, motion_file, target_frame_rate):
+        if self.create_text_embeddings:
+            from transformers import AutoTokenizer, XCLIPTextModel
+
+            model = XCLIPTextModel.from_pretrained("microsoft/xclip-base-patch32")
+            tokenizer = AutoTokenizer.from_pretrained("microsoft/xclip-base-patch32")
+
+        motions = []
+        motion_lengths = []
+        motion_dt = []
+        motion_num_frames = []
+        text_embeddings = []
+        has_text_embeddings = []
+        (
+            motion_files,
+            motion_weights,
+            motion_timings,
+            motion_fpses,
+            sub_motion_to_motion,
+            ref_respawn_offsets,
+            motion_labels,
+            supported_scene_ids,
+        ) = self._fetch_motion_files(motion_file)
+
+        num_motion_files = len(motion_files)
+
+        for f in range(num_motion_files):
+            curr_file = motion_files[f]
+
+            print(
+                "Loading {:d}/{:d} motion files: {:s}".format(
+                    f + 1, num_motion_files, curr_file
+                )
+            )
+            curr_motion = self._load_motion_file(curr_file)
+
+            curr_motion = fix_motion_fps(
+                curr_motion,
+                motion_fpses[f],
+                target_frame_rate,
+                self.skeleton_tree,
+            )
+            motion_fpses[f] = float(curr_motion.fps)
+
+            if self.fix_heights:
+                curr_motion = self.fix_motion_heights(curr_motion, self.skeleton_tree)
+
+            curr_motion.dof_pos[:, 4] = - curr_motion.dof_pos[:, 2] - curr_motion.dof_pos[:, 3]
+            curr_motion.dof_pos[:, 9] = - curr_motion.dof_pos[:, 8] - curr_motion.dof_pos[:, 7]
+
+            curr_dt = 1.0 / motion_fpses[f]
+
+            num_frames = curr_motion.global_translation.shape[0]
+            curr_len = 1.0 / motion_fpses[f] * (num_frames - 1)
+
+            motion_dt.append(curr_dt)
+            motion_num_frames.append(num_frames)
+
+            curr_dof_vels = self._compute_motion_dof_vels(curr_motion)
+            curr_motion.dof_vels = curr_dof_vels
+
+            joint_reorder_indices = [0, 5, 10, 1, 6, 11, 15, 2, 7, 12, 16, 3, 8, 13, 17, 4, 9, 14, 18]
+            curr_motion.dof_pos = curr_motion.dof_pos[:, joint_reorder_indices]
+            curr_motion.dof_vels = curr_motion.dof_vels[:, joint_reorder_indices]
+
+            body_reorder_indices = [0, 1, 2, 7, 12, 3, 8, 13, 18, 4, 9, 14, 19, 5, 10, 15, 20, 6, 11, 16, 21, 17, 22]
+            curr_motion.global_angular_velocity = curr_motion.global_angular_velocity[:, body_reorder_indices, :]
+            curr_motion.global_rotation = curr_motion.global_rotation[:, body_reorder_indices, :]
+            curr_motion.global_rotation_mat = curr_motion.global_rotation_mat[:, body_reorder_indices, :, :]
+            curr_motion.global_velocity = curr_motion.global_velocity[:, body_reorder_indices, :]
+            curr_motion.global_translation = curr_motion.global_translation[:, body_reorder_indices, :]
+            curr_motion.local_rotation = curr_motion.local_rotation[:, body_reorder_indices, :]
+
+            motions.append(curr_motion)
+            motion_lengths.append(curr_len)
+
+        num_sub_motions = len(sub_motion_to_motion)
+
+        for f in range(num_sub_motions):
+            # Incase start/end weren't provided, set to (0, motion_length)
+            motion_f = sub_motion_to_motion[f]
+            if motion_timings[f][1] == -1:
+                motion_timings[f][1] = motion_lengths[motion_f]
+
+            motion_timings[f][1] = min(
+                motion_timings[f][1], motion_lengths[motion_f]
+            )  # CT hack: fix small timing differences
+
+            assert (
+                motion_timings[f][0] < motion_timings[f][1]
+            ), f"Motion start {motion_timings[f][0]} >= motion end {motion_timings[f][1]} in motion {motion_f}"
+
+            if self.create_text_embeddings and motion_labels[f][0] != "":
+                with torch.inference_mode():
+                    inputs = tokenizer(
+                        motion_labels[f],
+                        padding=True,
+                        truncation=True,
+                        return_tensors="pt",
+                    )
+                    outputs = model(**inputs)
+                    pooled_output = outputs.pooler_output  # pooled (EOS token) states
+                    text_embeddings.append(pooled_output)  # should be [3, 512]
+                    has_text_embeddings.append(True)
+            else:
+                text_embeddings.append(
+                    torch.zeros((3, 512), dtype=torch.float32)
+                )  # just hold something temporary
+                has_text_embeddings.append(False)
+
+        motion_lengths = torch.tensor(
+            motion_lengths, device=self._device, dtype=torch.float32
+        )
+
+        motion_weights = torch.tensor(
+            motion_weights, dtype=torch.float32, device=self._device
+        )
+        motion_weights /= motion_weights.sum()
+
+        motion_timings = torch.tensor(
+            motion_timings, dtype=torch.float32, device=self._device
+        )
+
+        sub_motion_to_motion = torch.tensor(
+            sub_motion_to_motion, dtype=torch.long, device=self._device
+        )
+
+        ref_respawn_offsets = torch.tensor(
+            ref_respawn_offsets, dtype=torch.float32, device=self._device
+        )
+
+        motion_fpses = torch.tensor(
+            motion_fpses, device=self._device, dtype=torch.float32
+        )
+        motion_dt = torch.tensor(motion_dt, device=self._device, dtype=torch.float32)
+        motion_num_frames = torch.tensor(motion_num_frames, device=self._device)
+
+        text_embeddings = torch.stack(text_embeddings).detach().to(device=self._device)
+        has_text_embeddings = torch.tensor(
+            has_text_embeddings, dtype=torch.bool, device=self._device
+        )
+
+        self.state = LoadedMotions(
+            motions=tuple(motions),
+            motion_lengths=motion_lengths,
+            motion_weights=motion_weights,
+            motion_timings=motion_timings,
+            motion_fps=motion_fpses,
+            motion_dt=motion_dt,
+            motion_num_frames=motion_num_frames,
+            motion_files=tuple(motion_files),
+            sub_motion_to_motion=sub_motion_to_motion,
+            ref_respawn_offsets=ref_respawn_offsets,
+            text_embeddings=text_embeddings,
+            has_text_embeddings=has_text_embeddings,
+            supported_scene_ids=supported_scene_ids,
+        )
+
+        num_motions = self.num_motions()
+        total_len = self.get_total_length()
+
+        print(
+            "Loaded {:d} motions with a total length of {:.3f}s.".format(
+                num_motions, total_len
+            )
+        )
+
+        num_sub_motions = self.num_sub_motions()
+        total_trainable_len = self.get_total_trainable_length()
+
+        print(
+            "Loaded {:d} sub motions with a total trainable length of {:.3f}s.".format(
+                num_sub_motions, total_trainable_len
+            )
+        )
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/__init__.py b/source/unitreelab/unitreelab/MotionLib/poselib/__init__.py
new file mode 100644
index 0000000..fd13ae7
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/__init__.py
@@ -0,0 +1,3 @@
+__version__ = "0.0.1"
+
+from .core import *
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/core/__init__.py b/source/unitreelab/unitreelab/MotionLib/poselib/core/__init__.py
new file mode 100644
index 0000000..e3c0f9d
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/core/__init__.py
@@ -0,0 +1,3 @@
+from .tensor_utils import *
+from .rotation3d import *
+from .backend import Serializable, logger
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/core/backend/__init__.py b/source/unitreelab/unitreelab/MotionLib/poselib/core/backend/__init__.py
new file mode 100644
index 0000000..49705b2
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/core/backend/__init__.py
@@ -0,0 +1,3 @@
+from .abstract import Serializable
+
+from .logger import logger
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/core/backend/abstract.py b/source/unitreelab/unitreelab/MotionLib/poselib/core/backend/abstract.py
new file mode 100644
index 0000000..caef630
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/core/backend/abstract.py
@@ -0,0 +1,128 @@
+# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
+# NVIDIA CORPORATION and its licensors retain all intellectual property
+# and proprietary rights in and to this software, related documentation
+# and any modifications thereto.  Any use, reproduction, disclosure or
+# distribution of this software and related documentation without an express
+# license agreement from NVIDIA CORPORATION is strictly prohibited.
+
+from abc import ABCMeta, abstractmethod, abstractclassmethod
+from collections import OrderedDict
+import json
+
+import numpy as np
+import os
+
+TENSOR_CLASS = {}
+
+
+def register(name):
+    global TENSOR_CLASS
+
+    def core(tensor_cls):
+        TENSOR_CLASS[name] = tensor_cls
+        return tensor_cls
+
+    return core
+
+
+def _get_cls(name):
+    global TENSOR_CLASS
+    return TENSOR_CLASS[name]
+
+
+class NumpyEncoder(json.JSONEncoder):
+    """ Special json encoder for numpy types """
+
+    def default(self, obj):
+        if isinstance(
+            obj,
+            (
+                np.int_,
+                np.intc,
+                np.intp,
+                np.int8,
+                np.int16,
+                np.int32,
+                np.int64,
+                np.uint8,
+                np.uint16,
+                np.uint32,
+                np.uint64,
+            ),
+        ):
+            return int(obj)
+        elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):
+            return float(obj)
+        elif isinstance(obj, (np.ndarray,)):
+            return dict(__ndarray__=obj.tolist(), dtype=str(obj.dtype), shape=obj.shape)
+        return json.JSONEncoder.default(self, obj)
+
+
+def json_numpy_obj_hook(dct):
+    if isinstance(dct, dict) and "__ndarray__" in dct:
+        data = np.asarray(dct["__ndarray__"], dtype=dct["dtype"])
+        return data.reshape(dct["shape"])
+    return dct
+
+
+class Serializable:
+    """ Implementation to read/write to file.
+    All class the is inherited from this class needs to implement to_dict() and 
+    from_dict()
+    """
+
+    @abstractclassmethod
+    def from_dict(cls, dict_repr, *args, **kwargs):
+        """ Read the object from an ordered dictionary
+
+        :param dict_repr: the ordered dictionary that is used to construct the object
+        :type dict_repr: OrderedDict
+        :param args, kwargs: the arguments that need to be passed into from_dict()
+        :type args, kwargs: additional arguments
+        """
+        pass
+
+    @abstractmethod
+    def to_dict(self):
+        """ Construct an ordered dictionary from the object
+        
+        :rtype: OrderedDict
+        """
+        pass
+
+    @classmethod
+    def from_file(cls, path, *args, **kwargs):
+        """ Read the object from a file (either .npy or .json)
+
+        :param path: path of the file
+        :type path: string
+        :param args, kwargs: the arguments that need to be passed into from_dict()
+        :type args, kwargs: additional arguments
+        """
+        if path.endswith(".json"):
+            with open(path, "r") as f:
+                d = json.load(f, object_hook=json_numpy_obj_hook)
+        elif path.endswith(".npy"):
+            d = np.load(path, allow_pickle=True).item()
+        else:
+            assert False, "failed to load {} from {}".format(cls.__name__, path)
+        assert d["__name__"] == cls.__name__, "the file belongs to {}, not {}".format(
+            d["__name__"], cls.__name__
+        )
+        return cls.from_dict(d, *args, **kwargs)
+
+    def to_file(self, path: str) -> None:
+        """ Write the object to a file (either .npy or .json)
+
+        :param path: path of the file
+        :type path: string
+        """
+        if os.path.dirname(path) != "" and not os.path.exists(os.path.dirname(path)):
+            os.makedirs(os.path.dirname(path))
+        d = self.to_dict()
+        d["__name__"] = self.__class__.__name__
+        if path.endswith(".json"):
+            with open(path, "w") as f:
+                json.dump(d, f, cls=NumpyEncoder, indent=4)
+        elif path.endswith(".npy"):
+            np.save(path, d)
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/core/backend/logger.py b/source/unitreelab/unitreelab/MotionLib/poselib/core/backend/logger.py
new file mode 100644
index 0000000..369cae9
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/core/backend/logger.py
@@ -0,0 +1,20 @@
+# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
+# NVIDIA CORPORATION and its licensors retain all intellectual property
+# and proprietary rights in and to this software, related documentation
+# and any modifications thereto.  Any use, reproduction, disclosure or
+# distribution of this software and related documentation without an express
+# license agreement from NVIDIA CORPORATION is strictly prohibited.
+
+import logging
+
+logger = logging.getLogger("poselib")
+logger.setLevel(logging.INFO)
+
+if not len(logger.handlers):
+    formatter = logging.Formatter(
+        fmt="%(asctime)-15s - %(levelname)s - %(module)s - %(message)s"
+    )
+    handler = logging.StreamHandler()
+    handler.setFormatter(formatter)
+    logger.addHandler(handler)
+    logger.info("logger initialized")
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/core/rotation3d.py b/source/unitreelab/unitreelab/MotionLib/poselib/core/rotation3d.py
new file mode 100644
index 0000000..877ce2b
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/core/rotation3d.py
@@ -0,0 +1,480 @@
+# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
+# NVIDIA CORPORATION and its licensors retain all intellectual property
+# and proprietary rights in and to this software, related documentation
+# and any modifications thereto.  Any use, reproduction, disclosure or
+# distribution of this software and related documentation without an express
+# license agreement from NVIDIA CORPORATION is strictly prohibited.
+
+from typing import List, Optional
+
+import math
+import torch
+
+
+@torch.jit.script
+def quat_mul(a, b):
+    """
+    quaternion multiplication
+    """
+    x1, y1, z1, w1 = a[..., 0], a[..., 1], a[..., 2], a[..., 3]
+    x2, y2, z2, w2 = b[..., 0], b[..., 1], b[..., 2], b[..., 3]
+
+    w = w1 * w2 - x1 * x2 - y1 * y2 - z1 * z2
+    x = w1 * x2 + x1 * w2 + y1 * z2 - z1 * y2
+    y = w1 * y2 + y1 * w2 + z1 * x2 - x1 * z2
+    z = w1 * z2 + z1 * w2 + x1 * y2 - y1 * x2
+
+    return torch.stack([x, y, z, w], dim=-1)
+
+
+@torch.jit.script
+def quat_pos(x):
+    """
+    make all the real part of the quaternion positive
+    """
+    q = x
+    z = (q[..., 3:] < 0).float()
+    q = (1 - 2 * z) * q
+    return q
+
+
+@torch.jit.script
+def quat_abs(x):
+    """
+    quaternion norm (unit quaternion represents a 3D rotation, which has norm of 1)
+    """
+    x = x.norm(p=2, dim=-1)
+    return x
+
+
+@torch.jit.script
+def quat_unit(x):
+    """
+    normalized quaternion with norm of 1
+    """
+    norm = quat_abs(x).unsqueeze(-1)
+    return x / (norm.clamp(min=1e-9))
+
+
+@torch.jit.script
+def quat_conjugate(x):
+    """
+    quaternion with its imaginary part negated
+    """
+    return torch.cat([-x[..., :3], x[..., 3:]], dim=-1)
+
+
+@torch.jit.script
+def quat_real(x):
+    """
+    real component of the quaternion
+    """
+    return x[..., 3]
+
+
+@torch.jit.script
+def quat_imaginary(x):
+    """
+    imaginary components of the quaternion
+    """
+    return x[..., :3]
+
+
+@torch.jit.script
+def quat_norm_check(x):
+    """
+    verify that a quaternion has norm 1
+    """
+    assert bool(
+        (abs(x.norm(p=2, dim=-1) - 1) < 1e-3).all()
+    ), "the quaternion is has non-1 norm: {}".format(abs(x.norm(p=2, dim=-1) - 1))
+    assert bool((x[..., 3] >= 0).all()), "the quaternion has negative real part"
+
+
+@torch.jit.script
+def quat_normalize(q):
+    """
+    Construct 3D rotation from quaternion (the quaternion needs not to be normalized).
+    """
+    q = quat_unit(quat_pos(q))  # normalized to positive and unit quaternion
+    return q
+
+
+@torch.jit.script
+def quat_from_xyz(xyz):
+    """
+    Construct 3D rotation from the imaginary component
+    """
+    w = (1.0 - xyz.norm()).unsqueeze(-1)
+    assert bool((w >= 0).all()), "xyz has its norm greater than 1"
+    return torch.cat([xyz, w], dim=-1)
+
+
+@torch.jit.script
+def quat_identity(shape: List[int]):
+    """
+    Construct 3D identity rotation given shape
+    """
+    w = torch.ones(shape + [1])
+    xyz = torch.zeros(shape + [3])
+    q = torch.cat([xyz, w], dim=-1)
+    return quat_normalize(q)
+
+
+@torch.jit.script
+def quat_from_angle_axis(angle, axis, degree: bool = False):
+    """Create a 3D rotation from angle and axis of rotation. The rotation is counter-clockwise
+    along the axis.
+
+    The rotation can be interpreted as a_R_b where frame "b" is the new frame that
+    gets rotated counter-clockwise along the axis from frame "a"
+
+    :param angle: angle of rotation
+    :type angle: Tensor
+    :param axis: axis of rotation
+    :type axis: Tensor
+    :param degree: put True here if the angle is given by degree
+    :type degree: bool, optional, default=False
+    """
+    if degree:
+        angle = angle / 180.0 * math.pi
+    theta = (angle / 2).unsqueeze(-1)
+    axis = axis / (axis.norm(p=2, dim=-1, keepdim=True).clamp(min=1e-9))
+    xyz = axis * theta.sin()
+    w = theta.cos()
+    return quat_normalize(torch.cat([xyz, w], dim=-1))
+
+
+@torch.jit.script
+def quat_from_rotation_matrix(m):
+    """
+    Construct a 3D rotation from a valid 3x3 rotation matrices.
+    Reference can be found here:
+    http://www.cg.info.hiroshima-cu.ac.jp/~miyazaki/knowledge/teche52.html
+
+    :param m: 3x3 orthogonal rotation matrices.
+    :type m: Tensor
+
+    :rtype: Tensor
+    """
+    m = m.unsqueeze(0)
+    diag0 = m[..., 0, 0]
+    diag1 = m[..., 1, 1]
+    diag2 = m[..., 2, 2]
+
+    # Math stuff.
+    w = (((diag0 + diag1 + diag2 + 1.0) / 4.0).clamp(0.0, None)) ** 0.5
+    x = (((diag0 - diag1 - diag2 + 1.0) / 4.0).clamp(0.0, None)) ** 0.5
+    y = (((-diag0 + diag1 - diag2 + 1.0) / 4.0).clamp(0.0, None)) ** 0.5
+    z = (((-diag0 - diag1 + diag2 + 1.0) / 4.0).clamp(0.0, None)) ** 0.5
+
+    # Only modify quaternions where w > x, y, z.
+    c0 = (w >= x) & (w >= y) & (w >= z)
+    x[c0] *= (m[..., 2, 1][c0] - m[..., 1, 2][c0]).sign()
+    y[c0] *= (m[..., 0, 2][c0] - m[..., 2, 0][c0]).sign()
+    z[c0] *= (m[..., 1, 0][c0] - m[..., 0, 1][c0]).sign()
+
+    # Only modify quaternions where x > w, y, z
+    c1 = (x >= w) & (x >= y) & (x >= z)
+    w[c1] *= (m[..., 2, 1][c1] - m[..., 1, 2][c1]).sign()
+    y[c1] *= (m[..., 1, 0][c1] + m[..., 0, 1][c1]).sign()
+    z[c1] *= (m[..., 0, 2][c1] + m[..., 2, 0][c1]).sign()
+
+    # Only modify quaternions where y > w, x, z.
+    c2 = (y >= w) & (y >= x) & (y >= z)
+    w[c2] *= (m[..., 0, 2][c2] - m[..., 2, 0][c2]).sign()
+    x[c2] *= (m[..., 1, 0][c2] + m[..., 0, 1][c2]).sign()
+    z[c2] *= (m[..., 2, 1][c2] + m[..., 1, 2][c2]).sign()
+
+    # Only modify quaternions where z > w, x, y.
+    c3 = (z >= w) & (z >= x) & (z >= y)
+    w[c3] *= (m[..., 1, 0][c3] - m[..., 0, 1][c3]).sign()
+    x[c3] *= (m[..., 2, 0][c3] + m[..., 0, 2][c3]).sign()
+    y[c3] *= (m[..., 2, 1][c3] + m[..., 1, 2][c3]).sign()
+
+    return quat_normalize(torch.stack([x, y, z, w], dim=-1)).squeeze(0)
+
+
+@torch.jit.script
+def quat_mul_norm(x, y):
+    """
+    Combine two set of 3D rotations together using \**\* operator. The shape needs to be
+    broadcastable
+    """
+    return quat_normalize(quat_mul(x, y))
+
+
+@torch.jit.script
+def quat_rotate(rot, vec):
+    """
+    Rotate a 3D vector with the 3D rotation
+    """
+    other_q = torch.cat([vec, torch.zeros_like(vec[..., :1])], dim=-1)
+    return quat_imaginary(quat_mul(quat_mul(rot, other_q), quat_conjugate(rot)))
+
+
+@torch.jit.script
+def quat_inverse(x):
+    """
+    The inverse of the rotation
+    """
+    return quat_conjugate(x)
+
+
+@torch.jit.script
+def quat_identity_like(x):
+    """
+    Construct identity 3D rotation with the same shape
+    """
+    return quat_identity(x.shape[:-1])
+
+
+@torch.jit.script
+def quat_angle_axis(x):
+    """
+    The (angle, axis) representation of the rotation. The axis is normalized to unit length.
+    The angle is guaranteed to be between [0, pi].
+    """
+    s = 2 * (x[..., 3] ** 2) - 1
+    angle = s.clamp(-1, 1).arccos()  # just to be safe
+    axis = x[..., :3]
+    axis /= axis.norm(p=2, dim=-1, keepdim=True).clamp(min=1e-9)
+    return angle, axis
+
+
+@torch.jit.script
+def quat_yaw_rotation(x, z_up: bool = True):
+    """
+    Yaw rotation (rotation along z-axis)
+    """
+    q = x
+    if z_up:
+        q = torch.cat([torch.zeros_like(q[..., 0:2]), q[..., 2:3], q[..., 3:]], dim=-1)
+    else:
+        q = torch.cat(
+            [
+                torch.zeros_like(q[..., 0:1]),
+                q[..., 1:2],
+                torch.zeros_like(q[..., 2:3]),
+                q[..., 3:4],
+            ],
+            dim=-1,
+        )
+    return quat_normalize(q)
+
+
+@torch.jit.script
+def transform_from_rotation_translation(
+    r: Optional[torch.Tensor] = None, t: Optional[torch.Tensor] = None
+):
+    """
+    Construct a transform from a quaternion and 3D translation. Only one of them can be None.
+    """
+    assert r is not None or t is not None, "rotation and translation can't be all None"
+    if r is None:
+        assert t is not None
+        r = quat_identity(list(t.shape))
+    if t is None:
+        t = torch.zeros(list(r.shape) + [3])
+    return torch.cat([r, t], dim=-1)
+
+
+@torch.jit.script
+def transform_identity(shape: List[int]):
+    """
+    Identity transformation with given shape
+    """
+    r = quat_identity(shape)
+    t = torch.zeros(shape + [3])
+    return transform_from_rotation_translation(r, t)
+
+
+@torch.jit.script
+def transform_rotation(x):
+    """Get rotation from transform"""
+    return x[..., :4]
+
+
+@torch.jit.script
+def transform_translation(x):
+    """Get translation from transform"""
+    return x[..., 4:]
+
+
+@torch.jit.script
+def transform_inverse(x):
+    """
+    Inverse transformation
+    """
+    inv_so3 = quat_inverse(transform_rotation(x))
+    return transform_from_rotation_translation(
+        r=inv_so3, t=quat_rotate(inv_so3, -transform_translation(x))
+    )
+
+
+@torch.jit.script
+def transform_identity_like(x):
+    """
+    identity transformation with the same shape
+    """
+    return transform_identity(x.shape)
+
+
+@torch.jit.script
+def transform_mul(x, y):
+    """
+    Combine two transformation together
+    """
+    z = transform_from_rotation_translation(
+        r=quat_mul_norm(transform_rotation(x), transform_rotation(y)),
+        t=quat_rotate(transform_rotation(x), transform_translation(y))
+        + transform_translation(x),
+    )
+    return z
+
+
+@torch.jit.script
+def transform_apply(rot, vec):
+    """
+    Transform a 3D vector
+    """
+    assert isinstance(vec, torch.Tensor)
+    return quat_rotate(transform_rotation(rot), vec) + transform_translation(rot)
+
+
+@torch.jit.script
+def rot_matrix_det(x):
+    """
+    Return the determinant of the 3x3 matrix. The shape of the tensor will be as same as the
+    shape of the matrix
+    """
+    a, b, c = x[..., 0, 0], x[..., 0, 1], x[..., 0, 2]
+    d, e, f = x[..., 1, 0], x[..., 1, 1], x[..., 1, 2]
+    g, h, i = x[..., 2, 0], x[..., 2, 1], x[..., 2, 2]
+    t1 = a * (e * i - f * h)
+    t2 = b * (d * i - f * g)
+    t3 = c * (d * h - e * g)
+    return t1 - t2 + t3
+
+
+@torch.jit.script
+def rot_matrix_integrity_check(x):
+    """
+    Verify that a rotation matrix has a determinant of one and is orthogonal
+    """
+    det = rot_matrix_det(x)
+    assert bool((abs(det - 1) < 1e-3).all()), "the matrix has non-one determinant"
+    rtr = x @ x.permute(torch.arange(x.dim() - 2), -1, -2)
+    rtr_gt = rtr.zeros_like()
+    rtr_gt[..., 0, 0] = 1
+    rtr_gt[..., 1, 1] = 1
+    rtr_gt[..., 2, 2] = 1
+    assert bool(((rtr - rtr_gt) < 1e-3).all()), "the matrix is not orthogonal"
+
+
+@torch.jit.script
+def rot_matrix_from_quaternion(q):
+    """
+    Construct rotation matrix from quaternion
+    """
+    # Shortcuts for individual elements (using wikipedia's convention)
+    qi, qj, qk, qr = q[..., 0], q[..., 1], q[..., 2], q[..., 3]
+
+    # Set individual elements
+    R00 = 1.0 - 2.0 * (qj**2 + qk**2)
+    R01 = 2 * (qi * qj - qk * qr)
+    R02 = 2 * (qi * qk + qj * qr)
+    R10 = 2 * (qi * qj + qk * qr)
+    R11 = 1.0 - 2.0 * (qi**2 + qk**2)
+    R12 = 2 * (qj * qk - qi * qr)
+    R20 = 2 * (qi * qk - qj * qr)
+    R21 = 2 * (qj * qk + qi * qr)
+    R22 = 1.0 - 2.0 * (qi**2 + qj**2)
+
+    R0 = torch.stack([R00, R01, R02], dim=-1)
+    R1 = torch.stack([R10, R11, R12], dim=-1)
+    R2 = torch.stack([R10, R21, R22], dim=-1)
+
+    R = torch.stack([R0, R1, R2], dim=-2)
+
+    return R
+
+
+# @torch.jit.script
+# def rot_matrix_from_quaternion(quaternions: torch.Tensor) -> torch.Tensor:
+#     """
+#     Convert rotations given as quaternions to rotation matrices.
+#
+#     Args:
+#         quaternions: quaternions with real part first,
+#             as tensor of shape (..., 4).
+#
+#     Returns:
+#         Rotation matrices as tensor of shape (..., 3, 3).
+#     """
+#     i, j, k, r = torch.unbind(quaternions, -1)
+#     two_s = 2.0 / (quaternions * quaternions).sum(-1)
+#
+#     o = torch.stack(
+#         (
+#             1 - two_s * (j * j + k * k),
+#             two_s * (i * j - k * r),
+#             two_s * (i * k + j * r),
+#             two_s * (i * j + k * r),
+#             1 - two_s * (i * i + k * k),
+#             two_s * (j * k - i * r),
+#             two_s * (i * k - j * r),
+#             two_s * (j * k + i * r),
+#             1 - two_s * (i * i + j * j),
+#         ),
+#         -1,
+#     )
+#     return o.reshape(quaternions.shape[:-1] + (3, 3))
+
+
+@torch.jit.script
+def euclidean_to_rotation_matrix(x):
+    """
+    Get the rotation matrix on the top-left corner of a Euclidean transformation matrix
+    """
+    return x[..., :3, :3]
+
+
+@torch.jit.script
+def euclidean_integrity_check(x):
+    euclidean_to_rotation_matrix(x)  # check 3d-rotation matrix
+    assert bool((x[..., 3, :3] == 0).all()), "the last row is illegal"
+    assert bool((x[..., 3, 3] == 1).all()), "the last row is illegal"
+
+
+@torch.jit.script
+def euclidean_translation(x):
+    """
+    Get the translation vector located at the last column of the matrix
+    """
+    return x[..., :3, 3]
+
+
+@torch.jit.script
+def euclidean_inverse(x):
+    """
+    Compute the matrix that represents the inverse rotation
+    """
+    s = x.zeros_like()
+    irot = quat_inverse(quat_from_rotation_matrix(x))
+    s[..., :3, :3] = irot
+    s[..., :3, 4] = quat_rotate(irot, -euclidean_translation(x))
+    return s
+
+
+@torch.jit.script
+def euclidean_to_transform(transformation_matrix):
+    """
+    Construct a transform from a Euclidean transformation matrix
+    """
+    return transform_from_rotation_translation(
+        r=quat_from_rotation_matrix(
+            m=euclidean_to_rotation_matrix(transformation_matrix)
+        ),
+        t=euclidean_translation(transformation_matrix),
+    )
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/core/tensor_utils.py b/source/unitreelab/unitreelab/MotionLib/poselib/core/tensor_utils.py
new file mode 100644
index 0000000..2646556
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/core/tensor_utils.py
@@ -0,0 +1,45 @@
+# -*- coding: utf-8 -*-
+
+# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
+# NVIDIA CORPORATION and its licensors retain all intellectual property
+# and proprietary rights in and to this software, related documentation
+# and any modifications thereto.  Any use, reproduction, disclosure or
+# distribution of this software and related documentation without an express
+# license agreement from NVIDIA CORPORATION is strictly prohibited.
+
+from collections import OrderedDict
+from .backend import Serializable
+import torch
+
+
+class TensorUtils(Serializable):
+    @classmethod
+    def from_dict(cls, dict_repr, *args, **kwargs):
+        """ Read the object from an ordered dictionary
+
+        :param dict_repr: the ordered dictionary that is used to construct the object
+        :type dict_repr: OrderedDict
+        :param kwargs: the arguments that need to be passed into from_dict()
+        :type kwargs: additional arguments
+        """
+        return torch.from_numpy(dict_repr["arr"].astype(dict_repr["context"]["dtype"]))
+
+    def to_dict(self):
+        """ Construct an ordered dictionary from the object
+        
+        :rtype: OrderedDict
+        """
+        return NotImplemented
+
+def tensor_to_dict(x):
+    """ Construct an ordered dictionary from the object
+    
+    :rtype: OrderedDict
+    """
+    x_np = x.numpy()
+    return {
+        "arr": x_np,
+        "context": {
+            "dtype": x_np.dtype.name
+        }
+    }
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/core/tests/__init__.py b/source/unitreelab/unitreelab/MotionLib/poselib/core/tests/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/core/tests/test_rotation.py b/source/unitreelab/unitreelab/MotionLib/poselib/core/tests/test_rotation.py
new file mode 100644
index 0000000..c5b6802
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/core/tests/test_rotation.py
@@ -0,0 +1,56 @@
+# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
+# NVIDIA CORPORATION and its licensors retain all intellectual property
+# and proprietary rights in and to this software, related documentation
+# and any modifications thereto.  Any use, reproduction, disclosure or
+# distribution of this software and related documentation without an express
+# license agreement from NVIDIA CORPORATION is strictly prohibited.
+
+from ..rotation3d import *
+import numpy as np
+import torch
+
+q = torch.from_numpy(np.array([[0, 1, 2, 3], [-2, 3, -1, 5]], dtype=np.float32))
+print("q", q)
+r = quat_normalize(q)
+x = torch.from_numpy(np.array([[1, 0, 0], [0, -1, 0]], dtype=np.float32))
+print(r)
+print(quat_rotate(r, x))
+
+angle = torch.from_numpy(np.array(np.random.rand() * 10.0, dtype=np.float32))
+axis = torch.from_numpy(np.array([1, np.random.rand() * 10.0, np.random.rand() * 10.0], dtype=np.float32),)
+
+print(repr(angle))
+print(repr(axis))
+
+rot = quat_from_angle_axis(angle, axis)
+x = torch.from_numpy(np.random.rand(5, 6, 3))
+y = quat_rotate(quat_inverse(rot), quat_rotate(rot, x))
+print(x.numpy())
+print(y.numpy())
+assert np.allclose(x.numpy(), y.numpy())
+
+m = torch.from_numpy(np.array([[1, 0, 0], [0, 0, -1], [0, 1, 0]], dtype=np.float32))
+r = quat_from_rotation_matrix(m)
+t = torch.from_numpy(np.array([0, 1, 0], dtype=np.float32))
+se3 = transform_from_rotation_translation(r=r, t=t)
+print(se3)
+print(transform_apply(se3, t))
+
+rot = quat_from_angle_axis(
+    torch.from_numpy(np.array([45, -54], dtype=np.float32)),
+    torch.from_numpy(np.array([[1, 0, 0], [0, 1, 0]], dtype=np.float32)),
+    degree=True,
+)
+trans = torch.from_numpy(np.array([[1, 1, 0], [1, 1, 0]], dtype=np.float32))
+transform = transform_from_rotation_translation(r=rot, t=trans)
+
+t = transform_mul(transform, transform_inverse(transform))
+gt = np.zeros((2, 7))
+gt[:, 0] = 1.0
+print(t.numpy())
+print(gt)
+# assert np.allclose(t.numpy(), gt)
+
+transform2 = torch.from_numpy(np.array([[1, 0, 0, 1], [0, 0, -1, 0], [0, 1, 0, 0], [0, 0, 0, 1]], dtype=np.float32),)
+transform2 = euclidean_to_transform(transform2)
+print(transform2)
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/__init__.py b/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/backend/__init__.py b/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/backend/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/backend/fbx/__init__.py b/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/backend/fbx/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/backend/fbx/fbx_py27_backend.py b/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/backend/fbx/fbx_py27_backend.py
new file mode 100644
index 0000000..bee7f9f
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/backend/fbx/fbx_py27_backend.py
@@ -0,0 +1,308 @@
+"""
+Copyright (c) 2021, NVIDIA CORPORATION. All rights reserved.
+
+NVIDIA CORPORATION and its licensors retain all intellectual property and proprietary 
+rights in and to this software, related documentation and any modifications thereto. Any 
+use, reproduction, disclosure or distribution of this software and related documentation 
+without an express license agreement from NVIDIA CORPORATION is strictly prohibited.
+"""
+
+"""
+This script reads an fbx file and saves the joint names, parents, and transforms to a 
+numpy array.
+
+NOTE: It must be run from python 2.7 with the fbx SDK installed. To use this script, 
+please use the read_fbx file
+"""
+
+import sys
+
+import numpy as np
+
+try:
+    import fbx
+    import FbxCommon
+except ImportError as e:
+    print("Error: FBX Import Failed. Message: {}".format(e))
+    if sys.version_info[0] >= 3:
+        print(
+            "WARNING: you are using python 3 when this script should only be run from "
+            "python 2"
+        )
+    else:
+        print(
+            "You are using python 2 but importing fbx failed. You must install it from "
+            "http://help.autodesk.com/view/FBX/2018/ENU/?guid=FBX_Developer_Help_"
+            "scripting_with_python_fbx_html"
+        )
+    print("Exiting")
+    exit()
+
+
+def fbx_to_npy(file_name_in, file_name_out, root_joint_name, fps):
+    """
+    This function reads in an fbx file, and saves the relevant info to a numpy array
+
+    Fbx files have a series of animation curves, each of which has animations at different 
+    times. This script assumes that for mocap data, there is only one animation curve that
+    contains all the joints. Otherwise it is unclear how to read in the data.
+
+    If this condition isn't met, then the method throws an error
+
+    :param file_name_in: str, file path in. Should be .fbx file
+    :param file_name_out: str, file path out. Should be .npz file
+    :return: nothing, it just writes a file.
+    """
+
+    # Create the fbx scene object and load the .fbx file
+    fbx_sdk_manager, fbx_scene = FbxCommon.InitializeSdkObjects()
+    FbxCommon.LoadScene(fbx_sdk_manager, fbx_scene, file_name_in)
+
+    """
+    To read in the animation, we must find the root node of the skeleton.
+    
+    Unfortunately fbx files can have "scene parents" and other parts of the tree that are 
+    not joints
+    
+    As a crude fix, this reader just takes and finds the first thing which has an 
+    animation curve attached
+    """
+
+    search_root = (root_joint_name is None or root_joint_name == "")
+
+    # Get the root node of the skeleton, which is the child of the scene's root node
+    possible_root_nodes = [fbx_scene.GetRootNode()]
+    found_root_node = False
+    max_key_count = 0
+    root_joint = None
+    while len(possible_root_nodes) > 0:
+        joint = possible_root_nodes.pop(0)
+        if not search_root:
+            if joint.GetName() == root_joint_name:
+                root_joint = joint
+        try:
+            curve, anim_layer = _get_animation_curve(joint, fbx_scene)
+        except RuntimeError:
+            curve = None
+        if curve is not None:
+            key_count = curve.KeyGetCount()
+            if key_count > max_key_count:
+                found_root_node = True
+                max_key_count = key_count
+                root_curve = curve
+            if search_root and not root_joint:
+                root_joint = joint
+
+        if not search_root and curve is not None and root_joint is not None:
+            break
+
+        for child_index in range(joint.GetChildCount()):
+            possible_root_nodes.append(joint.GetChild(child_index))
+
+    if not found_root_node:
+        raise RuntimeError("No root joint found!! Exiting")
+
+    joint_list, joint_names, parents = _get_skeleton(root_joint)
+
+    """
+    Read in the transformation matrices of the animation, taking the scaling into account
+    """
+
+    anim_range, frame_count, frame_rate = _get_frame_count(fbx_scene)
+
+    local_transforms = []
+    #for frame in range(frame_count):
+    time_sec = anim_range.GetStart().GetSecondDouble()
+    time_range_sec = anim_range.GetStop().GetSecondDouble() - time_sec
+    fbx_fps = frame_count / time_range_sec
+    if fps != 120:
+        fbx_fps = fps
+    print("FPS: ", fbx_fps)
+    while time_sec < anim_range.GetStop().GetSecondDouble():
+        fbx_time = fbx.FbxTime()
+        fbx_time.SetSecondDouble(time_sec)
+        fbx_time = fbx_time.GetFramedTime()
+        transforms_current_frame = []
+
+        # Fbx has a unique time object which you need
+        #fbx_time = root_curve.KeyGetTime(frame)
+        for joint in joint_list:
+            arr = np.array(_recursive_to_list(joint.EvaluateLocalTransform(fbx_time)))
+            scales = np.array(_recursive_to_list(joint.EvaluateLocalScaling(fbx_time)))
+
+            lcl_trans = joint.LclTranslation.Get()
+            lcl_rot = joint.LclRotation.Get()
+            lcl_matrix = fbx.FbxAMatrix()
+            # lcl_matrix.SetR(fbx.FbxVector4(lcl_rot[0], lcl_rot[1], lcl_rot[2], 1.0))
+            # lcl_matrix.SetT(fbx.FbxVector4(lcl_trans[0], lcl_trans[1], lcl_trans[2], 1.0))
+            # lcl_matrix = np.array(_recursive_to_list(lcl_matrix))
+            curve = joint.LclTranslation.GetCurve(anim_layer, "X")
+            transX = curve.Evaluate(fbx_time)[0] if curve else lcl_trans[0]
+            curve = joint.LclTranslation.GetCurve(anim_layer, "Y")
+            transY = curve.Evaluate(fbx_time)[0] if curve else lcl_trans[1]
+            curve = joint.LclTranslation.GetCurve(anim_layer, "Z")
+            transZ = curve.Evaluate(fbx_time)[0] if curve else lcl_trans[2]
+
+            curve = joint.LclRotation.GetCurve(anim_layer, "X")
+            rotX = curve.Evaluate(fbx_time)[0] if curve else lcl_rot[0]
+            curve = joint.LclRotation.GetCurve(anim_layer, "Y")
+            rotY = curve.Evaluate(fbx_time)[0] if curve else lcl_rot[1]
+            curve = joint.LclRotation.GetCurve(anim_layer, "Z")
+            rotZ = curve.Evaluate(fbx_time)[0] if curve else lcl_rot[2]
+            
+            lcl_matrix.SetR(fbx.FbxVector4(rotX, rotY, rotZ, 1.0))
+            lcl_matrix.SetT(fbx.FbxVector4(transX, transY, transZ, 1.0))
+            lcl_matrix = np.array(_recursive_to_list(lcl_matrix))
+            # if not np.allclose(scales[0:3], scales[0]):
+            #     raise ValueError(
+            #         "Different X, Y and Z scaling. Unsure how this should be handled. "
+            #         "To solve this, look at this link and try to upgrade the script "
+            #         "http://help.autodesk.com/view/FBX/2017/ENU/?guid=__files_GUID_10CDD"
+            #         "63C_79C1_4F2D_BB28_AD2BE65A02ED_htm"
+            #     )
+            # Adjust the array for scaling
+            arr /= scales[0]
+            arr[3, 3] = 1.0
+            lcl_matrix[3, 3] = 1.0
+            transforms_current_frame.append(lcl_matrix)
+        local_transforms.append(transforms_current_frame)
+
+        time_sec += (1.0/fbx_fps)
+
+    local_transforms = np.array(local_transforms)
+    print("Frame Count: ", len(local_transforms))
+
+    # Write to numpy array
+    np.savez_compressed(
+        file_name_out, names=joint_names, parents=parents, transforms=local_transforms, fps=fbx_fps
+    )
+
+def _get_frame_count(fbx_scene):
+    # Get the animation stacks and layers, in order to pull off animation curves later
+    num_anim_stacks = fbx_scene.GetSrcObjectCount(
+        FbxCommon.FbxCriteria.ObjectType(FbxCommon.FbxAnimStack.ClassId)
+    )
+    # if num_anim_stacks != 1:
+    #     raise RuntimeError(
+    #         "More than one animation stack was found. "
+    #         "This script must be modified to handle this case. Exiting"
+    #     )
+    if num_anim_stacks > 1:
+        index = 1
+    else:
+        index = 0
+    anim_stack = fbx_scene.GetSrcObject(
+        FbxCommon.FbxCriteria.ObjectType(FbxCommon.FbxAnimStack.ClassId), index
+    )
+
+    anim_range = anim_stack.GetLocalTimeSpan()
+    duration = anim_range.GetDuration()
+    fps = duration.GetFrameRate(duration.GetGlobalTimeMode())
+    frame_count = duration.GetFrameCount(True)
+
+    return anim_range, frame_count, fps
+
+def _get_animation_curve(joint, fbx_scene):
+    # Get the animation stacks and layers, in order to pull off animation curves later
+    num_anim_stacks = fbx_scene.GetSrcObjectCount(
+        FbxCommon.FbxCriteria.ObjectType(FbxCommon.FbxAnimStack.ClassId)
+    )
+    # if num_anim_stacks != 1:
+    #     raise RuntimeError(
+    #         "More than one animation stack was found. "
+    #         "This script must be modified to handle this case. Exiting"
+    #     )
+    if num_anim_stacks > 1:
+        index = 1
+    else:
+        index = 0
+    anim_stack = fbx_scene.GetSrcObject(
+        FbxCommon.FbxCriteria.ObjectType(FbxCommon.FbxAnimStack.ClassId), index
+    )
+
+    num_anim_layers = anim_stack.GetSrcObjectCount(
+        FbxCommon.FbxCriteria.ObjectType(FbxCommon.FbxAnimLayer.ClassId)
+    )
+    if num_anim_layers != 1:
+        raise RuntimeError(
+            "More than one animation layer was found. "
+            "This script must be modified to handle this case. Exiting"
+        )
+    animation_layer = anim_stack.GetSrcObject(
+        FbxCommon.FbxCriteria.ObjectType(FbxCommon.FbxAnimLayer.ClassId), 0
+    )
+
+    def _check_longest_curve(curve, max_curve_key_count):
+        longest_curve = None
+        if curve and curve.KeyGetCount() > max_curve_key_count[0]:
+            max_curve_key_count[0] = curve.KeyGetCount()
+            return True
+
+        return False
+
+    max_curve_key_count = [0]
+    longest_curve = None
+    for c in ["X", "Y", "Z"]:
+        curve = joint.LclTranslation.GetCurve(
+            animation_layer, c
+        )  # sample curve for translation
+        if _check_longest_curve(curve, max_curve_key_count):
+            longest_curve = curve
+
+        curve = joint.LclRotation.GetCurve(
+            animation_layer, "X"
+        )
+        if _check_longest_curve(curve, max_curve_key_count):
+            longest_curve = curve
+
+    return longest_curve, animation_layer
+
+
+def _get_skeleton(root_joint):
+
+    # Do a depth first search of the skeleton to extract all the joints
+    joint_list = [root_joint]
+    joint_names = [root_joint.GetName()]
+    parents = [-1]  # -1 means no parent
+
+    def append_children(joint, pos):
+        """
+        Depth first search function
+        :param joint: joint item in the fbx
+        :param pos: position of current element (for parenting)
+        :return: Nothing
+        """
+        for child_index in range(joint.GetChildCount()):
+            child = joint.GetChild(child_index)
+            joint_list.append(child)
+            joint_names.append(child.GetName())
+            parents.append(pos)
+            append_children(child, len(parents) - 1)
+
+    append_children(root_joint, 0)
+    return joint_list, joint_names, parents
+
+
+def _recursive_to_list(array):
+    """
+    Takes some iterable that might contain iterables and converts it to a list of lists 
+    [of lists... etc]
+
+    Mainly used for converting the strange fbx wrappers for c++ arrays into python lists
+    :param array: array to be converted
+    :return: array converted to lists
+    """
+    try:
+        return float(array)
+    except TypeError:
+        return [_recursive_to_list(a) for a in array]
+
+
+if __name__ == "__main__":
+
+    # Read in the input and output files, then read the fbx
+    file_name_in, file_name_out = sys.argv[1:3]
+    root_joint_name = sys.argv[3]
+    fps = int(sys.argv[4])
+
+    fbx_to_npy(file_name_in, file_name_out, root_joint_name, fps)
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/backend/fbx/fbx_read_wrapper.py b/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/backend/fbx/fbx_read_wrapper.py
new file mode 100644
index 0000000..a42b302
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/backend/fbx/fbx_read_wrapper.py
@@ -0,0 +1,75 @@
+"""
+Copyright (c) 2021, NVIDIA CORPORATION. All rights reserved.
+
+NVIDIA CORPORATION and its licensors retain all intellectual property and proprietary 
+rights in and to this software, related documentation and any modifications thereto. Any 
+use, reproduction, disclosure or distribution of this software and related documentation 
+without an express license agreement from NVIDIA CORPORATION is strictly prohibited.
+"""
+
+"""
+Script that reads in fbx files from python 2
+
+This requires a configs file, which contains the command necessary to switch conda
+environments to run the fbx reading script from python 2
+"""
+
+from ....core import logger
+
+import inspect
+import os
+
+import numpy as np
+
+# Get the current folder to import the config file
+current_folder = os.path.realpath(
+    os.path.abspath(os.path.split(inspect.getfile(inspect.currentframe()))[0])
+)
+
+
+def fbx_to_array(fbx_file_path, fbx_configs, root_joint, fps):
+    """
+    Reads an fbx file to an array.
+
+    Currently reading of the frame time is not supported. 120 fps is hard coded TODO
+
+    :param fbx_file_path: str, file path to fbx
+    :return: tuple with joint_names, parents, transforms, frame time
+    """
+
+    # Ensure the file path is valid
+    fbx_file_path = os.path.abspath(fbx_file_path)
+    assert os.path.exists(fbx_file_path)
+
+    # Switch directories to the env_utils folder to ensure the reading works
+    previous_cwd = os.getcwd()
+    os.chdir(current_folder)
+
+    # Call the python 2.7 script
+    temp_file_path = os.path.join(current_folder, fbx_configs["tmp_path"])
+    python_path = fbx_configs["fbx_py27_path"]
+    logger.info("executing python script to read fbx data using Autodesk FBX SDK...")
+    command = '{} fbx_py27_backend.py "{}" "{}" "{}" "{}"'.format(
+        python_path, fbx_file_path, temp_file_path, root_joint, fps
+    )
+    logger.debug("executing command: {}".format(command))
+    os.system(command)
+    logger.info(
+        "executing python script to read fbx data using Autodesk FBX SDK... done"
+    )
+
+    with open(temp_file_path, "rb") as f:
+        data = np.load(f)
+        output = (
+            data["names"],
+            data["parents"],
+            data["transforms"],
+            data["fps"],
+        )
+
+    # Remove the temporary file
+    os.remove(temp_file_path)
+
+    # Return the os to its previous cwd, otherwise reading multiple files might fail
+    os.chdir(previous_cwd)
+    return output
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/skeleton3d.py b/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/skeleton3d.py
new file mode 100644
index 0000000..9c19aa3
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/skeleton3d.py
@@ -0,0 +1,1410 @@
+# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
+# NVIDIA CORPORATION and its licensors retain all intellectual property
+# and proprietary rights in and to this software, related documentation
+# and any modifications thereto.  Any use, reproduction, disclosure or
+# distribution of this software and related documentation without an express
+# license agreement from NVIDIA CORPORATION is strictly prohibited.
+
+import os
+import xml.etree.ElementTree as ET
+from collections import OrderedDict
+from typing import List, Optional, Type, Dict
+
+import numpy as np
+import torch
+
+from ..core import *
+from .backend.fbx.fbx_read_wrapper import fbx_to_array
+import scipy.ndimage.filters as filters
+
+
+class SkeletonTree(Serializable):
+    """
+    A skeleton tree gives a complete description of a rigid skeleton. It describes a tree structure
+    over a list of nodes with their names indicated by strings. Each edge in the tree has a local
+    translation associated with it which describes the distance between the two nodes that it
+    connects.
+
+    Basic Usage:
+        >>> t = SkeletonTree.from_mjcf(SkeletonTree.__example_mjcf_path__)
+        >>> t
+        SkeletonTree(
+            node_names=['torso', 'front_left_leg', 'aux_1', 'front_left_foot', 'front_right_leg', 'aux_2', 'front_right_foot', 'left_back_leg', 'aux_3', 'left_back_foot', 'right_back_leg', 'aux_4', 'right_back_foot'],
+            parent_indices=tensor([-1,  0,  1,  2,  0,  4,  5,  0,  7,  8,  0, 10, 11]),
+            local_translation=tensor([[ 0.0000,  0.0000,  0.7500],
+                    [ 0.0000,  0.0000,  0.0000],
+                    [ 0.2000,  0.2000,  0.0000],
+                    [ 0.2000,  0.2000,  0.0000],
+                    [ 0.0000,  0.0000,  0.0000],
+                    [-0.2000,  0.2000,  0.0000],
+                    [-0.2000,  0.2000,  0.0000],
+                    [ 0.0000,  0.0000,  0.0000],
+                    [-0.2000, -0.2000,  0.0000],
+                    [-0.2000, -0.2000,  0.0000],
+                    [ 0.0000,  0.0000,  0.0000],
+                    [ 0.2000, -0.2000,  0.0000],
+                    [ 0.2000, -0.2000,  0.0000]])
+        )
+        >>> t.node_names
+        ['torso', 'front_left_leg', 'aux_1', 'front_left_foot', 'front_right_leg', 'aux_2', 'front_right_foot', 'left_back_leg', 'aux_3', 'left_back_foot', 'right_back_leg', 'aux_4', 'right_back_foot']
+        >>> t.parent_indices
+        tensor([-1,  0,  1,  2,  0,  4,  5,  0,  7,  8,  0, 10, 11])
+        >>> t.local_translation
+        tensor([[ 0.0000,  0.0000,  0.7500],
+                [ 0.0000,  0.0000,  0.0000],
+                [ 0.2000,  0.2000,  0.0000],
+                [ 0.2000,  0.2000,  0.0000],
+                [ 0.0000,  0.0000,  0.0000],
+                [-0.2000,  0.2000,  0.0000],
+                [-0.2000,  0.2000,  0.0000],
+                [ 0.0000,  0.0000,  0.0000],
+                [-0.2000, -0.2000,  0.0000],
+                [-0.2000, -0.2000,  0.0000],
+                [ 0.0000,  0.0000,  0.0000],
+                [ 0.2000, -0.2000,  0.0000],
+                [ 0.2000, -0.2000,  0.0000]])
+        >>> t.parent_of('front_left_leg')
+        'torso'
+        >>> t.index('front_right_foot')
+        6
+        >>> t[2]
+        'aux_1'
+    """
+
+    __example_mjcf_path__ = os.path.join(
+        os.path.dirname(os.path.realpath(__file__)), "tests/ant.xml"
+    )
+
+    def __init__(self, node_names, parent_indices, local_translation):
+        """
+        :param node_names: a list of names for each tree node
+        :type node_names: List[str]
+        :param parent_indices: an int32-typed tensor that represents the edge to its parent.\
+        -1 represents the root node
+        :type parent_indices: Tensor
+        :param local_translation: a 3d vector that gives local translation information
+        :type local_translation: Tensor
+        """
+        ln, lp, ll = len(node_names), len(parent_indices), len(local_translation)
+        assert len(set((ln, lp, ll))) == 1
+        self._node_names = node_names
+        self._parent_indices = parent_indices.long()
+        self._local_translation = local_translation
+        self._node_indices = {self.node_names[i]: i for i in range(len(self))}
+
+    def __len__(self):
+        """number of nodes in the skeleton tree"""
+        return len(self.node_names)
+
+    def __iter__(self):
+        """iterator that iterate through the name of each node"""
+        yield from self.node_names
+
+    def __getitem__(self, item):
+        """get the name of the node given the index"""
+        return self.node_names[item]
+
+    def __repr__(self):
+        return (
+            "SkeletonTree(\n    node_names={},\n    parent_indices={},"
+            "\n    local_translation={}\n)".format(
+                self._indent(repr(self.node_names)),
+                self._indent(repr(self.parent_indices)),
+                self._indent(repr(self.local_translation)),
+            )
+        )
+
+    def _indent(self, s):
+        return "\n    ".join(s.split("\n"))
+
+    @property
+    def node_names(self):
+        return self._node_names
+
+    @property
+    def parent_indices(self):
+        return self._parent_indices
+
+    @property
+    def local_translation(self):
+        return self._local_translation
+
+    @property
+    def num_joints(self):
+        """number of nodes in the skeleton tree"""
+        return len(self)
+
+    @classmethod
+    def from_dict(cls, dict_repr, *args, **kwargs):
+        return cls(
+            list(map(str, dict_repr["node_names"])),
+            TensorUtils.from_dict(dict_repr["parent_indices"], *args, **kwargs),
+            TensorUtils.from_dict(dict_repr["local_translation"], *args, **kwargs),
+        )
+
+    def to_dict(self):
+        return OrderedDict(
+            [
+                ("node_names", self.node_names),
+                ("parent_indices", tensor_to_dict(self.parent_indices)),
+                ("local_translation", tensor_to_dict(self.local_translation)),
+            ]
+        )
+
+    @classmethod
+    def from_mjcf(cls, path: str) -> "SkeletonTree":
+        """
+        Parses a mujoco xml scene description file and returns a Skeleton Tree.
+        We use the model attribute at the root as the name of the tree.
+
+        :param path:
+        :type path: string
+        :return: The skeleton tree constructed from the mjcf file
+        :rtype: SkeletonTree
+        """
+        tree = ET.parse(path)
+        xml_doc_root = tree.getroot()
+        xml_world_body = xml_doc_root.find("worldbody")
+        if xml_world_body is None:
+            raise ValueError("MJCF parsed incorrectly please verify it.")
+        # assume this is the root
+        xml_body_root = xml_world_body.find("body")
+        if xml_body_root is None:
+            raise ValueError("MJCF parsed incorrectly please verify it.")
+
+        node_names = []
+        parent_indices = []
+        local_translation = []
+
+        # recursively adding all nodes into the skel_tree
+        def _add_xml_node(xml_node, parent_index, node_index):
+            node_name = xml_node.attrib.get("name")
+            # parse the local translation into float list
+            pos = np.fromstring(xml_node.attrib.get("pos"), dtype=float, sep=" ")
+            node_names.append(node_name)
+            parent_indices.append(parent_index)
+            local_translation.append(pos)
+            curr_index = node_index
+            node_index += 1
+            for next_node in xml_node.findall("body"):
+                node_index = _add_xml_node(next_node, curr_index, node_index)
+            return node_index
+
+        _add_xml_node(xml_body_root, -1, 0)
+
+        return cls(
+            node_names,
+            torch.from_numpy(np.array(parent_indices, dtype=np.int32)),
+            torch.from_numpy(np.array(local_translation, dtype=np.float32)),
+        )
+
+    def parent_of(self, node_name):
+        """get the name of the parent of the given node
+
+        :param node_name: the name of the node
+        :type node_name: string
+        :rtype: string
+        """
+        return self[int(self.parent_indices[self.index(node_name)].item())]
+
+    def index(self, node_name):
+        """get the index of the node
+
+        :param node_name: the name of the node
+        :type node_name: string
+        :rtype: int
+        """
+        return self._node_indices[node_name]
+
+    def drop_nodes_by_names(
+        self, node_names: List[str], pairwise_translation=None
+    ) -> "SkeletonTree":
+        new_length = len(self) - len(node_names)
+        new_node_names = []
+        new_local_translation = torch.zeros(
+            new_length, 3, dtype=self.local_translation.dtype
+        )
+        new_parent_indices = torch.zeros(new_length, dtype=self.parent_indices.dtype)
+        parent_indices = self.parent_indices.numpy()
+        new_node_indices: dict = {}
+        new_node_index = 0
+        for node_index in range(len(self)):
+            if self[node_index] in node_names:
+                continue
+            tb_node_index = parent_indices[node_index]
+            if tb_node_index != -1:
+                local_translation = self.local_translation[node_index, :]
+                while tb_node_index != -1 and self[tb_node_index] in node_names:
+                    local_translation += self.local_translation[tb_node_index, :]
+                    tb_node_index = parent_indices[tb_node_index]
+                assert tb_node_index != -1, "the root node cannot be dropped"
+
+                if pairwise_translation is not None:
+                    local_translation = pairwise_translation[
+                        tb_node_index, node_index, :
+                    ]
+            else:
+                local_translation = self.local_translation[node_index, :]
+
+            new_node_names.append(self[node_index])
+            new_local_translation[new_node_index, :] = local_translation
+            if tb_node_index == -1:
+                new_parent_indices[new_node_index] = -1
+            else:
+                new_parent_indices[new_node_index] = new_node_indices[
+                    self[tb_node_index]
+                ]
+            new_node_indices[self[node_index]] = new_node_index
+            new_node_index += 1
+
+        return SkeletonTree(new_node_names, new_parent_indices, new_local_translation)
+
+    def keep_nodes_by_names(
+        self, node_names: List[str], pairwise_translation=None
+    ) -> "SkeletonTree":
+        nodes_to_drop = list(filter(lambda x: x not in node_names, self))
+        return self.drop_nodes_by_names(nodes_to_drop, pairwise_translation)
+
+
+class SkeletonState(Serializable):
+    """
+    A skeleton state contains all the information needed to describe a static state of a skeleton.
+    It requires a skeleton tree, local/global rotation at each joint and the root translation.
+
+    Example:
+        >>> t = SkeletonTree.from_mjcf(SkeletonTree.__example_mjcf_path__)
+        >>> zero_pose = SkeletonState.zero_pose(t)
+        >>> plot_skeleton_state(zero_pose)  # can be imported from `.visualization.base_interface`
+        [plot of the ant at zero pose
+        >>> local_rotation = zero_pose.local_rotation.clone()
+        >>> local_rotation[2] = torch.tensor([0, 0, 1, 0])
+        >>> new_pose = SkeletonState.from_rotation_and_root_translation(
+        ...             skeleton_tree=t,
+        ...             r=local_rotation,
+        ...             t=zero_pose.root_translation,
+        ...             is_local=True
+        ...         )
+        >>> new_pose.local_rotation
+        tensor([[0., 0., 0., 1.],
+                [0., 0., 0., 1.],
+                [0., 1., 0., 0.],
+                [0., 0., 0., 1.],
+                [0., 0., 0., 1.],
+                [0., 0., 0., 1.],
+                [0., 0., 0., 1.],
+                [0., 0., 0., 1.],
+                [0., 0., 0., 1.],
+                [0., 0., 0., 1.],
+                [0., 0., 0., 1.],
+                [0., 0., 0., 1.],
+                [0., 0., 0., 1.]])
+        >>> plot_skeleton_state(new_pose)  # you should be able to see one of ant's leg is bent
+        [plot of the ant with the new pose
+        >>> new_pose.global_rotation  # the local rotation is propagated to the global rotation at joint #3
+        tensor([[0., 0., 0., 1.],
+                [0., 0., 0., 1.],
+                [0., 1., 0., 0.],
+                [0., 1., 0., 0.],
+                [0., 0., 0., 1.],
+                [0., 0., 0., 1.],
+                [0., 0., 0., 1.],
+                [0., 0., 0., 1.],
+                [0., 0., 0., 1.],
+                [0., 0., 0., 1.],
+                [0., 0., 0., 1.],
+                [0., 0., 0., 1.],
+                [0., 0., 0., 1.]])
+
+    Global/Local Representation (cont. from the previous example)
+        >>> new_pose.is_local
+        True
+        >>> new_pose.tensor  # this will return the local rotation followed by the root translation
+        tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
+                0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1.,
+                0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
+                0.])
+        >>> new_pose.tensor.shape  # 4 * 13 (joint rotation) + 3 (root translatio
+        torch.Size([55])
+        >>> new_pose.global_repr().is_local
+        False
+        >>> new_pose.global_repr().tensor  # this will return the global rotation followed by the root translation instead
+        tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
+                0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1.,
+                0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
+                0.])
+        >>> new_pose.global_repr().tensor.shape  # 4 * 13 (joint rotation) + 3 (root translation
+        torch.Size([55])
+    """
+
+    def __init__(self, tensor_backend, skeleton_tree, is_local):
+        self._skeleton_tree = skeleton_tree
+        self._is_local = is_local
+        self.tensor = tensor_backend.clone()
+
+    def __len__(self):
+        return self.tensor.shape[0]
+
+    @property
+    def rotation(self):
+        if not hasattr(self, "_rotation"):
+            self._rotation = self.tensor[..., : self.num_joints * 4].reshape(
+                *(self.tensor.shape[:-1] + (self.num_joints, 4))
+            )
+        return self._rotation
+
+    @property
+    def _local_rotation(self):
+        if self._is_local:
+            return self.rotation
+        else:
+            return None
+
+    @property
+    def _global_rotation(self):
+        if not self._is_local:
+            return self.rotation
+        else:
+            return None
+
+    @property
+    def is_local(self):
+        """is the rotation represented in local frame?
+
+        :rtype: bool
+        """
+        return self._is_local
+
+    @property
+    def invariant_property(self):
+        return {"skeleton_tree": self.skeleton_tree, "is_local": self.is_local}
+
+    @property
+    def num_joints(self):
+        """number of joints in the skeleton tree
+
+        :rtype: int
+        """
+        return self.skeleton_tree.num_joints
+
+    @property
+    def skeleton_tree(self):
+        """skeleton tree
+
+        :rtype: SkeletonTree
+        """
+        return self._skeleton_tree
+
+    @property
+    def root_translation(self):
+        """root translation
+
+        :rtype: Tensor
+        """
+        if not hasattr(self, "_root_translation"):
+            self._root_translation = self.tensor[
+                ..., self.num_joints * 4 : self.num_joints * 4 + 3
+            ]
+        return self._root_translation
+
+    @property
+    def global_transformation(self):
+        """global transformation of each joint (transform from joint frame to global frame)"""
+        # Forward Kinematics
+        if not hasattr(self, "_global_transformation"):
+            local_transformation = self.local_transformation
+            global_transformation = []
+            parent_indices = self.skeleton_tree.parent_indices.numpy()
+            # global_transformation = local_transformation.identity_like()
+            for node_index in range(len(self.skeleton_tree)):
+                parent_index = parent_indices[node_index]
+                if parent_index == -1:
+                    global_transformation.append(
+                        local_transformation[..., node_index, :]
+                    )
+                else:
+                    global_transformation.append(
+                        transform_mul(
+                            global_transformation[parent_index],
+                            local_transformation[..., node_index, :],
+                        )
+                    )
+            self._global_transformation = torch.stack(global_transformation, axis=-2)
+        return self._global_transformation
+
+    @property
+    def global_rotation(self):
+        """global rotation of each joint (rotation matrix to rotate from joint's F.O.R to global
+        F.O.R)"""
+        if self._global_rotation is None:
+            if not hasattr(self, "_comp_global_rotation"):
+                self._comp_global_rotation = transform_rotation(
+                    self.global_transformation
+                )
+            return self._comp_global_rotation
+        else:
+            return self._global_rotation
+
+    @property
+    def global_translation(self):
+        """global translation of each joint"""
+        if not hasattr(self, "_global_translation"):
+            self._global_translation = transform_translation(self.global_transformation)
+        return self._global_translation
+
+    @property
+    def global_translation_xy(self):
+        """global translation in xy"""
+        trans_xy_data = self.global_translation.zeros_like()
+        trans_xy_data[..., 0:2] = self.global_translation[..., 0:2]
+        return trans_xy_data
+
+    @property
+    def global_translation_xz(self):
+        """global translation in xz"""
+        trans_xz_data = self.global_translation.zeros_like()
+        trans_xz_data[..., 0:1] = self.global_translation[..., 0:1]
+        trans_xz_data[..., 2:3] = self.global_translation[..., 2:3]
+        return trans_xz_data
+
+    @property
+    def local_rotation(self):
+        """the rotation from child frame to parent frame given in the order of child nodes appeared
+        in `.skeleton_tree.node_names`"""
+        if self._local_rotation is None:
+            if not hasattr(self, "_comp_local_rotation"):
+                local_rotation = quat_identity_like(self.global_rotation)
+                for node_index in range(len(self.skeleton_tree)):
+                    parent_index = self.skeleton_tree.parent_indices[node_index]
+                    if parent_index == -1:
+                        local_rotation[..., node_index, :] = self.global_rotation[
+                            ..., node_index, :
+                        ]
+                    else:
+                        local_rotation[..., node_index, :] = quat_mul_norm(
+                            quat_inverse(self.global_rotation[..., parent_index, :]),
+                            self.global_rotation[..., node_index, :],
+                        )
+                self._comp_local_rotation = local_rotation
+            return self._comp_local_rotation
+        else:
+            return self._local_rotation
+
+    @property
+    def local_transformation(self):
+        """local translation + local rotation. It describes the transformation from child frame to
+        parent frame given in the order of child nodes appeared in `.skeleton_tree.node_names`
+        """
+        if not hasattr(self, "_local_transformation"):
+            self._local_transformation = transform_from_rotation_translation(
+                r=self.local_rotation, t=self.local_translation
+            )
+        return self._local_transformation
+
+    @property
+    def local_translation(self):
+        """local translation of the skeleton state. It is identical to the local translation in
+        `.skeleton_tree.local_translation` except the root translation. The root translation is
+        identical to `.root_translation`"""
+        if not hasattr(self, "_local_translation"):
+            broadcast_shape = (
+                tuple(self.tensor.shape[:-1])
+                + (len(self.skeleton_tree),)
+                + tuple(self.skeleton_tree.local_translation.shape[-1:])
+            )
+            local_translation = self.skeleton_tree.local_translation.broadcast_to(
+                *broadcast_shape
+            ).clone()
+            local_translation[..., 0, :] = self.root_translation
+            self._local_translation = local_translation
+        return self._local_translation
+
+    # Root Properties
+    @property
+    def root_translation_xy(self):
+        """root translation on xy"""
+        if not hasattr(self, "_root_translation_xy"):
+            self._root_translation_xy = self.global_translation_xy[..., 0, :]
+        return self._root_translation_xy
+
+    @property
+    def global_root_rotation(self):
+        """root rotation"""
+        if not hasattr(self, "_global_root_rotation"):
+            self._global_root_rotation = self.global_rotation[..., 0, :]
+        return self._global_root_rotation
+
+    @property
+    def global_root_yaw_rotation(self):
+        """root yaw rotation"""
+        if not hasattr(self, "_global_root_yaw_rotation"):
+            self._global_root_yaw_rotation = self.global_root_rotation.yaw_rotation()
+        return self._global_root_yaw_rotation
+
+    # Properties relative to root
+    @property
+    def local_translation_to_root(self):
+        """The 3D translation from joint frame to the root frame."""
+        if not hasattr(self, "_local_translation_to_root"):
+            self._local_translation_to_root = (
+                self.global_translation - self.root_translation.unsqueeze(-1)
+            )
+        return self._local_translation_to_root
+
+    @property
+    def local_rotation_to_root(self):
+        """The 3D rotation from joint frame to the root frame. It is equivalent to
+        The root_R_world * world_R_node"""
+        return (
+            quat_inverse(self.global_root_rotation).unsqueeze(-1) * self.global_rotation
+        )
+
+    def compute_forward_vector(
+        self,
+        left_shoulder_index,
+        right_shoulder_index,
+        left_hip_index,
+        right_hip_index,
+        gaussian_filter_width=20,
+    ):
+        """Computes forward vector based on cross product of the up vector with
+        average of the right->left shoulder and hip vectors"""
+        global_positions = self.global_translation
+        # Perpendicular to the forward direction.
+        # Uses the shoulders and hips to find this.
+        side_direction = (
+            global_positions[:, left_shoulder_index].numpy()
+            - global_positions[:, right_shoulder_index].numpy()
+            + global_positions[:, left_hip_index].numpy()
+            - global_positions[:, right_hip_index].numpy()
+        )
+        side_direction = (
+            side_direction / np.sqrt((side_direction**2).sum(axis=-1))[..., np.newaxis]
+        )
+
+        # Forward direction obtained by crossing with the up direction.
+        forward_direction = np.cross(side_direction, np.array([[0, 1, 0]]))
+
+        # Smooth the forward direction with a Gaussian.
+        # Axis 0 is the time/frame axis.
+        forward_direction = filters.gaussian_filter1d(
+            forward_direction, gaussian_filter_width, axis=0, mode="nearest"
+        )
+        forward_direction = (
+            forward_direction
+            / np.sqrt((forward_direction**2).sum(axis=-1))[..., np.newaxis]
+        )
+
+        return torch.from_numpy(forward_direction)
+
+    @staticmethod
+    def _to_state_vector(rot, rt):
+        state_shape = rot.shape[:-2]
+        vr = rot.reshape(*(state_shape + (-1,)))
+        vt = rt.broadcast_to(*state_shape + rt.shape[-1:]).reshape(
+            *(state_shape + (-1,))
+        )
+        v = torch.cat([vr, vt], axis=-1)
+        return v
+
+    @classmethod
+    def from_dict(
+        cls: Type["SkeletonState"], dict_repr: OrderedDict, *args, **kwargs
+    ) -> "SkeletonState":
+        rot = TensorUtils.from_dict(dict_repr["rotation"], *args, **kwargs)
+        rt = TensorUtils.from_dict(dict_repr["root_translation"], *args, **kwargs)
+        return cls(
+            SkeletonState._to_state_vector(rot, rt),
+            SkeletonTree.from_dict(dict_repr["skeleton_tree"], *args, **kwargs),
+            dict_repr["is_local"],
+        )
+
+    def to_dict(self) -> OrderedDict:
+        return OrderedDict(
+            [
+                ("rotation", tensor_to_dict(self.rotation)),
+                ("root_translation", tensor_to_dict(self.root_translation)),
+                ("skeleton_tree", self.skeleton_tree.to_dict()),
+                ("is_local", self.is_local),
+            ]
+        )
+
+    @classmethod
+    def from_rotation_and_root_translation(cls, skeleton_tree, r, t, is_local=True):
+        """
+        Construct a skeleton state from rotation and root translation
+
+        :param skeleton_tree: the skeleton tree
+        :type skeleton_tree: SkeletonTree
+        :param r: rotation (either global or local)
+        :type r: Tensor
+        :param t: root translation
+        :type t: Tensor
+        :param is_local: to indicate that whether the rotation is local or global
+        :type is_local: bool, optional, default=True
+        """
+        assert (
+            r.dim() > 0
+        ), "the rotation needs to have at least 1 dimension (dim = {})".format(r.dim)
+        state_vec = SkeletonState._to_state_vector(r, t)
+
+        return cls(
+            state_vec,
+            skeleton_tree=skeleton_tree,
+            is_local=is_local,
+        )
+
+    @classmethod
+    def zero_pose(cls, skeleton_tree):
+        """
+        Construct a zero-pose skeleton state from the skeleton tree by assuming that all the local
+        rotation is 0 and root translation is also 0.
+
+        :param skeleton_tree: the skeleton tree as the rigid body
+        :type skeleton_tree: SkeletonTree
+        """
+        return cls.from_rotation_and_root_translation(
+            skeleton_tree=skeleton_tree,
+            r=quat_identity([skeleton_tree.num_joints]),
+            t=torch.zeros(3, dtype=skeleton_tree.local_translation.dtype),
+            is_local=True,
+        )
+
+    def local_repr(self):
+        """
+        Convert the skeleton state into local representation. This will only affects the values of
+        .tensor. If the skeleton state already has `is_local=True`. This method will do nothing.
+
+        :rtype: SkeletonState
+        """
+        if self.is_local:
+            return self
+        return SkeletonState.from_rotation_and_root_translation(
+            self.skeleton_tree,
+            r=self.local_rotation,
+            t=self.root_translation,
+            is_local=True,
+        )
+
+    def global_repr(self):
+        """
+        Convert the skeleton state into global representation. This will only affects the values of
+        .tensor. If the skeleton state already has `is_local=False`. This method will do nothing.
+
+        :rtype: SkeletonState
+        """
+        if not self.is_local:
+            return self
+        return SkeletonState.from_rotation_and_root_translation(
+            self.skeleton_tree,
+            r=self.global_rotation,
+            t=self.root_translation,
+            is_local=False,
+        )
+
+    def _get_pairwise_average_translation(self):
+        global_transform_inv = transform_inverse(self.global_transformation)
+        p1 = global_transform_inv.unsqueeze(-2)
+        p2 = self.global_transformation.unsqueeze(-3)
+
+        pairwise_translation = (
+            transform_translation(transform_mul(p1, p2))
+            .reshape(-1, len(self.skeleton_tree), len(self.skeleton_tree), 3)
+            .mean(axis=0)
+        )
+        return pairwise_translation
+
+    def _transfer_to(self, new_skeleton_tree: SkeletonTree):
+        old_indices = list(map(self.skeleton_tree.index, new_skeleton_tree))
+        return SkeletonState.from_rotation_and_root_translation(
+            new_skeleton_tree,
+            r=self.global_rotation[..., old_indices, :],
+            t=self.root_translation,
+            is_local=False,
+        )
+
+    def drop_nodes_by_names(
+        self, node_names: List[str], estimate_local_translation_from_states: bool = True
+    ) -> "SkeletonState":
+        """ 
+        Drop a list of nodes from the skeleton and re-compute the local rotation to match the 
+        original joint position as much as possible. 
+
+        :param node_names: a list node names that specifies the nodes need to be dropped
+        :type node_names: List of strings
+        :param estimate_local_translation_from_states: the boolean indicator that specifies whether\
+        or not to re-estimate the local translation from the states (avg.)
+        :type estimate_local_translation_from_states: boolean
+        :rtype: SkeletonState
+        """
+        if estimate_local_translation_from_states:
+            pairwise_translation = self._get_pairwise_average_translation()
+        else:
+            pairwise_translation = None
+        new_skeleton_tree = self.skeleton_tree.drop_nodes_by_names(
+            node_names, pairwise_translation
+        )
+        return self._transfer_to(new_skeleton_tree)
+
+    def keep_nodes_by_names(
+        self, node_names: List[str], estimate_local_translation_from_states: bool = True
+    ) -> "SkeletonState":
+        """ 
+        Keep a list of nodes and drop all other nodes from the skeleton and re-compute the local 
+        rotation to match the original joint position as much as possible. 
+
+        :param node_names: a list node names that specifies the nodes need to be dropped
+        :type node_names: List of strings
+        :param estimate_local_translation_from_states: the boolean indicator that specifies whether\
+        or not to re-estimate the local translation from the states (avg.)
+        :type estimate_local_translation_from_states: boolean
+        :rtype: SkeletonState
+        """
+        return self.drop_nodes_by_names(
+            list(filter(lambda x: (x not in node_names), self)),
+            estimate_local_translation_from_states,
+        )
+
+    def _remapped_to(
+        self, joint_mapping: Dict[str, str], target_skeleton_tree: SkeletonTree
+    ):
+        joint_mapping_inv = {target: source for source, target in joint_mapping.items()}
+        reduced_target_skeleton_tree = target_skeleton_tree.keep_nodes_by_names(
+            list(joint_mapping_inv)
+        )
+        n_joints = (
+            len(joint_mapping),
+            len(self.skeleton_tree),
+            len(reduced_target_skeleton_tree),
+        )
+        assert (
+            len(set(n_joints)) == 1
+        ), "the joint mapping is not consistent with the skeleton trees"
+        source_indices = list(
+            map(
+                lambda x: self.skeleton_tree.index(joint_mapping_inv[x]),
+                reduced_target_skeleton_tree,
+            )
+        )
+        target_local_rotation = self.local_rotation[..., source_indices, :]
+        return SkeletonState.from_rotation_and_root_translation(
+            skeleton_tree=reduced_target_skeleton_tree,
+            r=target_local_rotation,
+            t=self.root_translation,
+            is_local=True,
+        )
+
+    def retarget_to(
+        self,
+        joint_mapping: Dict[str, str],
+        source_tpose_local_rotation,
+        source_tpose_root_translation: np.ndarray,
+        target_skeleton_tree: SkeletonTree,
+        target_tpose_local_rotation,
+        target_tpose_root_translation: np.ndarray,
+        rotation_to_target_skeleton,
+        scale_to_target_skeleton: float,
+        z_up: bool = True,
+    ) -> "SkeletonState":
+        """ 
+        Retarget the skeleton state to a target skeleton tree. This is a naive retarget
+        implementation with rough approximations. The function follows the procedures below.
+
+        Steps:
+            1. Drop the joints from the source (self) that do not belong to the joint mapping\
+            with an implementation that is similar to "keep_nodes_by_names()" - take a\
+            look at the function doc for more details (same for source_tpose)
+            
+            2. Rotate the source state and the source tpose by "rotation_to_target_skeleton"\
+            to align the source with the target orientation
+            
+            3. Extract the root translation and normalize it to match the scale of the target\
+            skeleton
+            
+            4. Extract the global rotation from source state relative to source tpose and\
+            re-apply the relative rotation to the target tpose to construct the global\
+            rotation after retargetting
+            
+            5. Combine the computed global rotation and the root translation from 3 and 4 to\
+            complete the retargeting.
+            
+            6. Make feet on the ground (global translation z)
+
+        :param joint_mapping: a dictionary of that maps the joint node from the source skeleton to \
+        the target skeleton
+        :type joint_mapping: Dict[str, str]
+        
+        :param source_tpose_local_rotation: the local rotation of the source skeleton
+        :type source_tpose_local_rotation: Tensor
+        
+        :param source_tpose_root_translation: the root translation of the source tpose
+        :type source_tpose_root_translation: np.ndarray
+        
+        :param target_skeleton_tree: the target skeleton tree
+        :type target_skeleton_tree: SkeletonTree
+        
+        :param target_tpose_local_rotation: the local rotation of the target skeleton
+        :type target_tpose_local_rotation: Tensor
+        
+        :param target_tpose_root_translation: the root translation of the target tpose
+        :type target_tpose_root_translation: Tensor
+        
+        :param rotation_to_target_skeleton: the rotation that needs to be applied to the source\
+        skeleton to align with the target skeleton. Essentially the rotation is t_R_s, where t is\
+        the frame of reference of the target skeleton and s is the frame of reference of the source\
+        skeleton
+        :type rotation_to_target_skeleton: Tensor
+        :param scale_to_target_skeleton: the factor that needs to be multiplied from source\
+        skeleton to target skeleton (unit in distance). For example, to go from `cm` to `m`, the \
+        factor needs to be 0.01.
+        :type scale_to_target_skeleton: float
+        :rtype: SkeletonState
+        """
+
+        # STEP 0: Preprocess
+        source_tpose = SkeletonState.from_rotation_and_root_translation(
+            skeleton_tree=self.skeleton_tree,
+            r=source_tpose_local_rotation,
+            t=source_tpose_root_translation,
+            is_local=True,
+        )
+        target_tpose = SkeletonState.from_rotation_and_root_translation(
+            skeleton_tree=target_skeleton_tree,
+            r=target_tpose_local_rotation,
+            t=target_tpose_root_translation,
+            is_local=True,
+        )
+
+        # STEP 1: Drop the irrelevant joints
+        pairwise_translation = self._get_pairwise_average_translation()
+        node_names = list(joint_mapping)
+        new_skeleton_tree = self.skeleton_tree.keep_nodes_by_names(
+            node_names, pairwise_translation
+        )
+
+        # TODO: combine the following steps before STEP 3
+        source_tpose = source_tpose._transfer_to(new_skeleton_tree)
+        source_state = self._transfer_to(new_skeleton_tree)
+
+        source_tpose = source_tpose._remapped_to(joint_mapping, target_skeleton_tree)
+        source_state = source_state._remapped_to(joint_mapping, target_skeleton_tree)
+
+        # STEP 2: Rotate the source to align with the target
+        new_local_rotation = source_tpose.local_rotation.clone()
+        new_local_rotation[..., 0, :] = quat_mul_norm(
+            rotation_to_target_skeleton, source_tpose.local_rotation[..., 0, :]
+        )
+
+        source_tpose = SkeletonState.from_rotation_and_root_translation(
+            skeleton_tree=source_tpose.skeleton_tree,
+            r=new_local_rotation,
+            t=quat_rotate(rotation_to_target_skeleton, source_tpose.root_translation),
+            is_local=True,
+        )
+
+        new_local_rotation = source_state.local_rotation.clone()
+        new_local_rotation[..., 0, :] = quat_mul_norm(
+            rotation_to_target_skeleton, source_state.local_rotation[..., 0, :]
+        )
+        source_state = SkeletonState.from_rotation_and_root_translation(
+            skeleton_tree=source_state.skeleton_tree,
+            r=new_local_rotation,
+            t=quat_rotate(rotation_to_target_skeleton, source_state.root_translation),
+            is_local=True,
+        )
+
+        # STEP 3: Normalize to match the target scale
+        root_translation_diff = (
+            source_state.root_translation - source_tpose.root_translation
+        ) * scale_to_target_skeleton
+
+        # STEP 4: the global rotation from source state relative to source tpose and
+        # re-apply to the target
+        current_skeleton_tree = source_state.skeleton_tree
+        target_tpose_global_rotation = source_state.global_rotation[0, :].clone()
+        for current_index, name in enumerate(current_skeleton_tree):
+            if name in target_tpose.skeleton_tree:
+                target_tpose_global_rotation[current_index, :] = (
+                    target_tpose.global_rotation[
+                        target_tpose.skeleton_tree.index(name), :
+                    ]
+                )
+
+        global_rotation_diff = quat_mul_norm(
+            source_state.global_rotation, quat_inverse(source_tpose.global_rotation)
+        )
+        new_global_rotation = quat_mul_norm(
+            global_rotation_diff, target_tpose_global_rotation
+        )
+
+        # STEP 5: Putting 3 and 4 together
+        current_skeleton_tree = source_state.skeleton_tree
+        shape = source_state.global_rotation.shape[:-1]
+        shape = shape[:-1] + target_tpose.global_rotation.shape[-2:-1]
+        new_global_rotation_output = quat_identity(shape)
+        for current_index, name in enumerate(target_skeleton_tree):
+            while name not in current_skeleton_tree:
+                name = target_skeleton_tree.parent_of(name)
+            parent_index = current_skeleton_tree.index(name)
+            new_global_rotation_output[:, current_index, :] = new_global_rotation[
+                :, parent_index, :
+            ]
+
+        source_state = SkeletonState.from_rotation_and_root_translation(
+            skeleton_tree=target_skeleton_tree,
+            r=new_global_rotation_output,
+            t=target_tpose.root_translation + root_translation_diff,
+            is_local=False,
+        ).local_repr()
+
+        return source_state
+
+    def retarget_to_by_tpose(
+        self,
+        joint_mapping: Dict[str, str],
+        source_tpose: "SkeletonState",
+        target_tpose: "SkeletonState",
+        rotation_to_target_skeleton,
+        scale_to_target_skeleton: float,
+    ) -> "SkeletonState":
+        """ 
+        Retarget the skeleton state to a target skeleton tree. This is a naive retarget
+        implementation with rough approximations. See the method `retarget_to()` for more information
+
+        :param joint_mapping: a dictionary of that maps the joint node from the source skeleton to \
+        the target skeleton
+        :type joint_mapping: Dict[str, str]
+        
+        :param source_tpose: t-pose of the source skeleton
+        :type source_tpose: SkeletonState
+        
+        :param target_tpose: t-pose of the target skeleton
+        :type target_tpose: SkeletonState
+        
+        :param rotation_to_target_skeleton: the rotation that needs to be applied to the source\
+        skeleton to align with the target skeleton. Essentially the rotation is t_R_s, where t is\
+        the frame of reference of the target skeleton and s is the frame of reference of the source\
+        skeleton
+        :type rotation_to_target_skeleton: Tensor
+        :param scale_to_target_skeleton: the factor that needs to be multiplied from source\
+        skeleton to target skeleton (unit in distance). For example, to go from `cm` to `m`, the \
+        factor needs to be 0.01.
+        :type scale_to_target_skeleton: float
+        :rtype: SkeletonState
+        """
+        assert (
+            len(source_tpose.shape) == 0 and len(target_tpose.shape) == 0
+        ), "the retargeting script currently doesn't support vectorized operations"
+        return self.retarget_to(
+            joint_mapping,
+            source_tpose.local_rotation,
+            source_tpose.root_translation,
+            target_tpose.skeleton_tree,
+            target_tpose.local_rotation,
+            target_tpose.root_translation,
+            rotation_to_target_skeleton,
+            scale_to_target_skeleton,
+        )
+
+
+class SkeletonMotion(SkeletonState):
+
+    def __init__(self, tensor_backend, skeleton_tree, is_local, fps, *args, **kwargs):
+        self._fps = fps
+        super().__init__(tensor_backend, skeleton_tree, is_local, *args, **kwargs)
+
+    def clone(self):
+        return SkeletonMotion(
+            self.tensor.clone(), self.skeleton_tree, self._is_local, self._fps
+        )
+
+    @property
+    def invariant_property(self):
+        return {
+            "skeleton_tree": self.skeleton_tree,
+            "is_local": self.is_local,
+            "fps": self.fps,
+        }
+
+    @property
+    def global_velocity(self):
+        """global velocity"""
+        curr_index = self.num_joints * 4 + 3
+        return self.tensor[..., curr_index : curr_index + self.num_joints * 3].reshape(
+            *(self.tensor.shape[:-1] + (self.num_joints, 3))
+        )
+
+    @property
+    def global_angular_velocity(self):
+        """global angular velocity"""
+        curr_index = self.num_joints * 7 + 3
+        return self.tensor[..., curr_index : curr_index + self.num_joints * 3].reshape(
+            *(self.tensor.shape[:-1] + (self.num_joints, 3))
+        )
+
+    @property
+    def fps(self):
+        """number of frames per second"""
+        return self._fps
+
+    @property
+    def time_delta(self):
+        """time between two adjacent frames"""
+        return 1.0 / self.fps
+
+    @property
+    def global_root_velocity(self):
+        """global root velocity"""
+        return self.global_velocity[..., 0, :]
+
+    @property
+    def global_root_angular_velocity(self):
+        """global root angular velocity"""
+        return self.global_angular_velocity[..., 0, :]
+
+    @classmethod
+    def from_state_vector_and_velocity(
+        cls,
+        skeleton_tree,
+        state_vector,
+        global_velocity,
+        global_angular_velocity,
+        is_local,
+        fps,
+    ):
+        """
+        Construct a skeleton motion from a skeleton state vector, global velocity and angular
+        velocity at each joint.
+
+        :param skeleton_tree: the skeleton tree that the motion is based on
+        :type skeleton_tree: SkeletonTree
+        :param state_vector: the state vector from the skeleton state by `.tensor`
+        :type state_vector: Tensor
+        :param global_velocity: the global velocity at each joint
+        :type global_velocity: Tensor
+        :param global_angular_velocity: the global angular velocity at each joint
+        :type global_angular_velocity: Tensor
+        :param is_local: if the rotation ins the state vector is given in local frame
+        :type is_local: boolean
+        :param fps: number of frames per second
+        :type fps: int
+
+        :rtype: SkeletonMotion
+        """
+        state_shape = state_vector.shape[:-1]
+        v = global_velocity.reshape(*(state_shape + (-1,)))
+        av = global_angular_velocity.reshape(*(state_shape + (-1,)))
+        new_state_vector = torch.cat([state_vector, v, av], axis=-1)
+        return cls(
+            new_state_vector,
+            skeleton_tree=skeleton_tree,
+            is_local=is_local,
+            fps=fps,
+        )
+
+    @classmethod
+    def from_skeleton_state(
+        cls: Type["SkeletonMotion"], skeleton_state: SkeletonState, fps: int
+    ):
+        """
+        Construct a skeleton motion from a skeleton state. The velocities are estimated using second
+        order guassian filter along the last axis. The skeleton state must have at least .dim >= 1
+
+        :param skeleton_state: the skeleton state that the motion is based on
+        :type skeleton_state: SkeletonState
+        :param fps: number of frames per second
+        :type fps: int
+
+        :rtype: SkeletonMotion
+        """
+        assert (
+            type(skeleton_state) == SkeletonState
+        ), "expected type of {}, got {}".format(SkeletonState, type(skeleton_state))
+        global_velocity = SkeletonMotion._compute_velocity(
+            p=skeleton_state.global_translation, time_delta=1 / fps
+        )
+        global_angular_velocity = SkeletonMotion._compute_angular_velocity(
+            r=skeleton_state.global_rotation, time_delta=1 / fps
+        )
+        return cls.from_state_vector_and_velocity(
+            skeleton_tree=skeleton_state.skeleton_tree,
+            state_vector=skeleton_state.tensor,
+            global_velocity=global_velocity,
+            global_angular_velocity=global_angular_velocity,
+            is_local=skeleton_state.is_local,
+            fps=fps,
+        )
+
+    @staticmethod
+    def _to_state_vector(rot, rt, vel, avel):
+        state_shape = rot.shape[:-2]
+        skeleton_state_v = SkeletonState._to_state_vector(rot, rt)
+        v = vel.reshape(*(state_shape + (-1,)))
+        av = avel.reshape(*(state_shape + (-1,)))
+        skeleton_motion_v = torch.cat([skeleton_state_v, v, av], axis=-1)
+        return skeleton_motion_v
+
+    @classmethod
+    def from_dict(
+        cls: Type["SkeletonMotion"], dict_repr: OrderedDict, *args, **kwargs
+    ) -> "SkeletonMotion":
+        rot = TensorUtils.from_dict(dict_repr["rotation"], *args, **kwargs)
+        rt = TensorUtils.from_dict(dict_repr["root_translation"], *args, **kwargs)
+        vel = TensorUtils.from_dict(dict_repr["global_velocity"], *args, **kwargs)
+        avel = TensorUtils.from_dict(
+            dict_repr["global_angular_velocity"], *args, **kwargs
+        )
+        return cls(
+            SkeletonMotion._to_state_vector(rot, rt, vel, avel),
+            skeleton_tree=SkeletonTree.from_dict(
+                dict_repr["skeleton_tree"], *args, **kwargs
+            ),
+            is_local=dict_repr["is_local"],
+            fps=dict_repr["fps"],
+        )
+
+    def to_dict(self) -> OrderedDict:
+        return OrderedDict(
+            [
+                ("rotation", tensor_to_dict(self.rotation)),
+                ("root_translation", tensor_to_dict(self.root_translation)),
+                ("global_velocity", tensor_to_dict(self.global_velocity)),
+                (
+                    "global_angular_velocity",
+                    tensor_to_dict(self.global_angular_velocity),
+                ),
+                ("skeleton_tree", self.skeleton_tree.to_dict()),
+                ("is_local", self.is_local),
+                ("fps", self.fps),
+            ]
+        )
+
+    @classmethod
+    def from_fbx(
+        cls: Type["SkeletonMotion"],
+        fbx_file_path,
+        fbx_configs,
+        skeleton_tree=None,
+        is_local=True,
+        fps=120,
+        root_joint="",
+        root_trans_index=0,
+        *args,
+        **kwargs,
+    ) -> "SkeletonMotion":
+        """
+        Construct a skeleton motion from a fbx file (TODO - generalize this). If the skeleton tree
+        is not given, it will use the first frame of the mocap to construct the skeleton tree.
+
+        :param fbx_file_path: the path of the fbx file
+        :type fbx_file_path: string
+        :param fbx_configs: the configuration in terms of {"tmp_path": ..., "fbx_py27_path": ...}
+        :type fbx_configs: dict
+        :param skeleton_tree: the optional skeleton tree that the rotation will be applied to
+        :type skeleton_tree: SkeletonTree, optional
+        :param is_local: the state vector uses local or global rotation as the representation
+        :type is_local: bool, optional, default=True
+        :rtype: SkeletonMotion
+        """
+        joint_names, joint_parents, transforms, fps = fbx_to_array(
+            fbx_file_path, fbx_configs, root_joint, fps
+        )
+        # swap the last two axis to match the convention
+        local_transform = euclidean_to_transform(
+            transformation_matrix=torch.from_numpy(
+                np.swapaxes(np.array(transforms), -1, -2),
+            ).float()
+        )
+        local_rotation = transform_rotation(local_transform)
+        root_translation = transform_translation(local_transform)[
+            ..., root_trans_index, :
+        ]
+        joint_parents = torch.from_numpy(np.array(joint_parents)).int()
+
+        if skeleton_tree is None:
+            local_translation = transform_translation(local_transform).reshape(
+                -1, len(joint_parents), 3
+            )[0]
+            skeleton_tree = SkeletonTree(joint_names, joint_parents, local_translation)
+        skeleton_state = SkeletonState.from_rotation_and_root_translation(
+            skeleton_tree, r=local_rotation, t=root_translation, is_local=True
+        )
+        if not is_local:
+            skeleton_state = skeleton_state.global_repr()
+        return cls.from_skeleton_state(skeleton_state=skeleton_state, fps=fps)
+
+    @staticmethod
+    def _compute_velocity(p, time_delta, guassian_filter=True):
+        velocity = np.gradient(p.numpy(), axis=-3) / time_delta
+        if guassian_filter:
+            velocity = torch.from_numpy(
+                filters.gaussian_filter1d(velocity, 2, axis=-3, mode="nearest")
+            ).to(p)
+        else:
+            velocity = torch.from_numpy(velocity).to(p)
+
+        return velocity
+
+    @staticmethod
+    def _compute_angular_velocity(r, time_delta: float, guassian_filter=True):
+        # assume the second last dimension is the time axis
+        diff_quat_data = quat_identity_like(r).to(r)
+        diff_quat_data[..., :-1, :, :] = quat_mul_norm(
+            r[..., 1:, :, :], quat_inverse(r[..., :-1, :, :])
+        )
+        diff_angle, diff_axis = quat_angle_axis(diff_quat_data)
+        angular_velocity = diff_axis * diff_angle.unsqueeze(-1) / time_delta
+        if guassian_filter:
+            angular_velocity = torch.from_numpy(
+                filters.gaussian_filter1d(
+                    angular_velocity.numpy(), 2, axis=-3, mode="nearest"
+                ),
+            )
+        return angular_velocity
+
+    def crop(self, start: int, end: int, fps: Optional[int] = None):
+        """
+        Crop the motion along its last axis. This is equivalent to performing a slicing on the
+        object with [..., start: end: skip_every] where skip_every = old_fps / fps. Note that the
+        new fps provided must be a factor of the original fps.
+
+        :param start: the beginning frame index
+        :type start: int
+        :param end: the ending frame index
+        :type end: int
+        :param fps: number of frames per second in the output (if not given the original fps will be used)
+        :type fps: int, optional
+        :rtype: SkeletonMotion
+        """
+        if fps is None:
+            new_fps = int(self.fps)
+            old_fps = int(self.fps)
+        else:
+            new_fps = int(fps)
+            old_fps = int(self.fps)
+            assert old_fps % fps == 0, (
+                "the resampling doesn't support fps with non-integer division "
+                "from the original fps: {} => {}".format(old_fps, fps)
+            )
+        skip_every = old_fps // new_fps
+        s = slice(start, end, skip_every)
+        z = self[..., s]
+
+        rot = z.local_rotation if z.is_local else z.global_rotation
+        rt = z.root_translation
+        vel = z.global_velocity
+        avel = z.global_angular_velocity
+        return SkeletonMotion(
+            SkeletonMotion._to_state_vector(rot, rt, vel, avel),
+            skeleton_tree=z.skeleton_tree,
+            is_local=z.is_local,
+            fps=new_fps,
+        )
+
+    def retarget_to(
+        self,
+        joint_mapping: Dict[str, str],
+        source_tpose_local_rotation,
+        source_tpose_root_translation: np.ndarray,
+        target_skeleton_tree: "SkeletonTree",
+        target_tpose_local_rotation,
+        target_tpose_root_translation: np.ndarray,
+        rotation_to_target_skeleton,
+        scale_to_target_skeleton: float,
+        z_up: bool = True,
+    ) -> "SkeletonMotion":
+        """ 
+        Same as the one in :class:`SkeletonState`. This method discards all velocity information before
+        retargeting and re-estimate the velocity after the retargeting. The same fps is used in the
+        new retargetted motion.
+
+        :param joint_mapping: a dictionary of that maps the joint node from the source skeleton to \
+        the target skeleton
+        :type joint_mapping: Dict[str, str]
+        
+        :param source_tpose_local_rotation: the local rotation of the source skeleton
+        :type source_tpose_local_rotation: Tensor
+        
+        :param source_tpose_root_translation: the root translation of the source tpose
+        :type source_tpose_root_translation: np.ndarray
+        
+        :param target_skeleton_tree: the target skeleton tree
+        :type target_skeleton_tree: SkeletonTree
+        
+        :param target_tpose_local_rotation: the local rotation of the target skeleton
+        :type target_tpose_local_rotation: Tensor
+        
+        :param target_tpose_root_translation: the root translation of the target tpose
+        :type target_tpose_root_translation: Tensor
+        
+        :param rotation_to_target_skeleton: the rotation that needs to be applied to the source\
+        skeleton to align with the target skeleton. Essentially the rotation is t_R_s, where t is\
+        the frame of reference of the target skeleton and s is the frame of reference of the source\
+        skeleton
+        :type rotation_to_target_skeleton: Tensor
+        :param scale_to_target_skeleton: the factor that needs to be multiplied from source\
+        skeleton to target skeleton (unit in distance). For example, to go from `cm` to `m`, the \
+        factor needs to be 0.01.
+        :type scale_to_target_skeleton: float
+        :rtype: SkeletonMotion
+        """
+        return SkeletonMotion.from_skeleton_state(
+            super().retarget_to(
+                joint_mapping,
+                source_tpose_local_rotation,
+                source_tpose_root_translation,
+                target_skeleton_tree,
+                target_tpose_local_rotation,
+                target_tpose_root_translation,
+                rotation_to_target_skeleton,
+                scale_to_target_skeleton,
+                z_up,
+            ),
+            self.fps,
+        )
+
+    def retarget_to_by_tpose(
+        self,
+        joint_mapping: Dict[str, str],
+        source_tpose: "SkeletonState",
+        target_tpose: "SkeletonState",
+        rotation_to_target_skeleton,
+        scale_to_target_skeleton: float,
+        z_up: bool = True,
+    ) -> "SkeletonMotion":
+        """ 
+        Same as the one in :class:`SkeletonState`. This method discards all velocity information before
+        retargeting and re-estimate the velocity after the retargeting. The same fps is used in the
+        new retargetted motion.
+
+        :param joint_mapping: a dictionary of that maps the joint node from the source skeleton to \
+        the target skeleton
+        :type joint_mapping: Dict[str, str]
+        
+        :param source_tpose: t-pose of the source skeleton
+        :type source_tpose: SkeletonState
+        
+        :param target_tpose: t-pose of the target skeleton
+        :type target_tpose: SkeletonState
+        
+        :param rotation_to_target_skeleton: the rotation that needs to be applied to the source\
+        skeleton to align with the target skeleton. Essentially the rotation is t_R_s, where t is\
+        the frame of reference of the target skeleton and s is the frame of reference of the source\
+        skeleton
+        :type rotation_to_target_skeleton: Tensor
+        :param scale_to_target_skeleton: the factor that needs to be multiplied from source\
+        skeleton to target skeleton (unit in distance). For example, to go from `cm` to `m`, the \
+        factor needs to be 0.01.
+        :type scale_to_target_skeleton: float
+        :rtype: SkeletonMotion
+        """
+        return self.retarget_to(
+            joint_mapping,
+            source_tpose.local_rotation,
+            source_tpose.root_translation,
+            target_tpose.skeleton_tree,
+            target_tpose.local_rotation,
+            target_tpose.root_translation,
+            rotation_to_target_skeleton,
+            scale_to_target_skeleton,
+            z_up,
+        )
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/tests/__init__.py b/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/tests/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/tests/ant.xml b/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/tests/ant.xml
new file mode 100644
index 0000000..311d96f
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/tests/ant.xml
@@ -0,0 +1,71 @@
+<mujoco model="ant">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true"/>
+  <option integrator="RK4" timestep="0.01"/>
+  <custom>
+    <numeric data="0.0 0.0 0.55 1.0 0.0 0.0 0.0 0.0 1.0 0.0 -1.0 0.0 -1.0 0.0 1.0" name="init_qpos"/>
+  </custom>
+  <default>
+    <joint armature="1" damping="1" limited="true"/>
+    <geom conaffinity="0" condim="3" density="5.0" friction="1.5 0.1 0.1" margin="0.01" rgba="0.8 0.6 0.4 1"/>
+  </default>
+  <worldbody>
+    <body name="torso" pos="0 0 0.75">
+      <geom name="torso_geom" pos="0 0 0" size="0.25" type="sphere"/>
+      <!--joint armature="0" damping="0" limited="false" margin="0.01" name="root" pos="0 0 0" type="free"/-->
+      <body name="front_left_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 0.2 0.2 0.0" name="aux_1_geom" size="0.08" type="capsule" rgba=".8 .5 .3 1"/>
+        <body name="aux_1" pos="0.2 0.2 0">
+          <joint axis="0 0 1" name="hip_1" pos="0.0 0.0 0.0" range="-40 40" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 0.2 0.2 0.0" name="left_leg_geom" size="0.08" type="capsule" rgba=".8 .5 .3 1"/>
+          <body pos="0.2 0.2 0" name="front_left_foot">
+            <joint axis="-1 1 0" name="ankle_1" pos="0.0 0.0 0.0" range="30 100" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 0.4 0.4 0.0" name="left_ankle_geom" size="0.08" type="capsule" rgba=".8 .5 .3 1"/>
+          </body>
+        </body>
+      </body>
+      <body name="front_right_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 -0.2 0.2 0.0" name="aux_2_geom" size="0.08" type="capsule"/>
+        <body name="aux_2" pos="-0.2 0.2 0">
+          <joint axis="0 0 1" name="hip_2" pos="0.0 0.0 0.0" range="-40 40" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 -0.2 0.2 0.0" name="right_leg_geom" size="0.08" type="capsule"/>
+          <body pos="-0.2 0.2 0" name="front_right_foot">
+            <joint axis="1 1 0" name="ankle_2" pos="0.0 0.0 0.0" range="-100 -30" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 -0.4 0.4 0.0" name="right_ankle_geom" size="0.08" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="left_back_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 -0.2 -0.2 0.0" name="aux_3_geom" size="0.08" type="capsule"/>
+        <body name="aux_3" pos="-0.2 -0.2 0">
+          <joint axis="0 0 1" name="hip_3" pos="0.0 0.0 0.0" range="-40 40" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 -0.2 -0.2 0.0" name="back_leg_geom" size="0.08" type="capsule"/>
+          <body pos="-0.2 -0.2 0" name="left_back_foot">
+            <joint axis="-1 1 0" name="ankle_3" pos="0.0 0.0 0.0" range="-100 -30" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 -0.4 -0.4 0.0" name="third_ankle_geom" size="0.08" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="right_back_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 0.2 -0.2 0.0" name="aux_4_geom" size="0.08" type="capsule" rgba=".8 .5 .3 1"/>
+        <body name="aux_4" pos="0.2 -0.2 0">
+          <joint axis="0 0 1" name="hip_4" pos="0.0 0.0 0.0" range="-40 40" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 0.2 -0.2 0.0" name="rightback_leg_geom" size="0.08" type="capsule" rgba=".8 .5 .3 1"/>
+          <body pos="0.2 -0.2 0" name="right_back_foot">
+            <joint axis="1 1 0" name="ankle_4" pos="0.0 0.0 0.0" range="30 100" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 0.4 -0.4 0.0" name="fourth_ankle_geom" size="0.08" type="capsule" rgba=".8 .5 .3 1"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_4" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_4" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_1" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_1" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_2" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_2" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_3" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_3" gear="150"/>
+  </actuator>
+</mujoco>
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/tests/test_skeleton.py b/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/tests/test_skeleton.py
new file mode 100644
index 0000000..aa9edc3
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/tests/test_skeleton.py
@@ -0,0 +1,132 @@
+# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
+# NVIDIA CORPORATION and its licensors retain all intellectual property
+# and proprietary rights in and to this software, related documentation
+# and any modifications thereto.  Any use, reproduction, disclosure or
+# distribution of this software and related documentation without an express
+# license agreement from NVIDIA CORPORATION is strictly prohibited.
+
+from ...core import *
+from ..skeleton3d import SkeletonTree, SkeletonState, SkeletonMotion
+
+import numpy as np
+import torch
+
+from ...visualization.common import (
+    plot_skeleton_state,
+    plot_skeleton_motion_interactive,
+)
+
+from ...visualization.plt_plotter import Matplotlib3DPlotter
+from ...visualization.skeleton_plotter_tasks import (
+    Draw3DSkeletonMotion,
+    Draw3DSkeletonState,
+)
+
+
+def test_skel_tree():
+    skel_tree = SkeletonTree.from_mjcf(
+        "/home/serfcx/DL_Animation/rl_mimic/data/skeletons/humanoid_mimic_mod_2_noind.xml",
+        backend="pytorch",
+    )
+    skel_tree_rec = SkeletonTree.from_dict(skel_tree.to_dict(), backend="pytorch")
+    # assert skel_tree.to_str() == skel_tree_rec.to_str()
+    print(skel_tree.node_names)
+    print(skel_tree.local_translation)
+    print(skel_tree.parent_indices)
+    skel_state = SkeletonState.zero_pose(skeleton_tree=skel_tree)
+    plot_skeleton_state(task_name="draw_skeleton", skeleton_state=skel_state)
+    skel_state = skel_state.drop_nodes_by_names(["right_hip", "left_hip"])
+    plot_skeleton_state(task_name="draw_skeleton", skeleton_state=skel_state)
+
+
+def test_skel_motion():
+    skel_motion = SkeletonMotion.from_file(
+        "/tmp/tmp.npy", backend="pytorch", load_context=True
+    )
+
+    plot_skeleton_motion_interactive(skel_motion)
+
+
+def test_grad():
+    source_motion = SkeletonMotion.from_file(
+        "c:\\Users\\bmatusch\\carbmimic\\data\\motions\\JogFlatTerrain_01_ase.npy",
+        backend="pytorch",
+        device="cuda:0",
+    )
+    source_tpose = SkeletonState.from_file(
+        "c:\\Users\\bmatusch\\carbmimic\\data\\skeletons\\fox_tpose.npy",
+        backend="pytorch",
+        device="cuda:0",
+    )
+
+    target_tpose = SkeletonState.from_file(
+        "c:\\Users\\bmatusch\\carbmimic\\data\\skeletons\\flex_tpose.npy",
+        backend="pytorch",
+        device="cuda:0",
+    )
+    target_skeleton_tree = target_tpose.skeleton_tree
+
+    joint_mapping = {
+        "upArm_r": "right_shoulder",
+        "upArm_l": "left_shoulder",
+        "loArm_r": "right_elbow",
+        "loArm_l": "left_elbow",
+        "upLeg_r": "right_hip",
+        "upLeg_l": "left_hip",
+        "loLeg_r": "right_knee",
+        "loLeg_l": "left_knee",
+        "foot_r": "right_ankle",
+        "foot_l": "left_ankle",
+        "hips": "pelvis",
+        "neckA": "neck",
+        "spineA": "abdomen",
+    }
+
+    rotation_to_target_skeleton = quat_from_angle_axis(
+        angle=torch.tensor(90.0).float(),
+        axis=torch.tensor([1, 0, 0]).float(),
+        degree=True,
+    )
+
+    target_motion = source_motion.retarget_to(
+        joint_mapping=joint_mapping,
+        source_tpose_local_rotation=source_tpose.local_rotation,
+        source_tpose_root_translation=source_tpose.root_translation,
+        target_skeleton_tree=target_skeleton_tree,
+        target_tpose_local_rotation=target_tpose.local_rotation,
+        target_tpose_root_translation=target_tpose.root_translation,
+        rotation_to_target_skeleton=rotation_to_target_skeleton,
+        scale_to_target_skeleton=0.01,
+    )
+
+    target_state = SkeletonState(
+        target_motion.tensor[800, :],
+        target_motion.skeleton_tree,
+        target_motion.is_local,
+    )
+
+    skeleton_tree = target_state.skeleton_tree
+    root_translation = target_state.root_translation
+    global_translation = target_state.global_translation
+
+    q = np.zeros((len(skeleton_tree), 4), dtype=np.float32)
+    q[..., 3] = 1.0
+    q = torch.from_numpy(q)
+    max_its = 10000
+
+    task = Draw3DSkeletonState(task_name="", skeleton_state=target_state)
+    plotter = Matplotlib3DPlotter(task)
+
+    for i in range(max_its):
+        r = quat_normalize(q)
+        s = SkeletonState.from_rotation_and_root_translation(
+            skeleton_tree, r=r, t=root_translation, is_local=True
+        )
+        print("  quat norm: {}".format(q.norm(p=2, dim=-1).mean().numpy()))
+
+        task.update(s)
+        plotter.update()
+    plotter.show()
+
+
+test_grad()
\ No newline at end of file
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/tests/transfer_npy.py b/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/tests/transfer_npy.py
new file mode 100644
index 0000000..dfcd96b
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/skeleton/tests/transfer_npy.py
@@ -0,0 +1,31 @@
+# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
+# NVIDIA CORPORATION and its licensors retain all intellectual property
+# and proprietary rights in and to this software, related documentation
+# and any modifications thereto.  Any use, reproduction, disclosure or
+# distribution of this software and related documentation without an express
+# license agreement from NVIDIA CORPORATION is strictly prohibited.
+
+import numpy as np
+from ...core import Tensor, SO3, Quaternion, Vector3D
+from ..skeleton3d import SkeletonTree, SkeletonState, SkeletonMotion
+
+tpose = np.load(
+    "/home/serfcx/DL_Animation/rl_mimic/data/skeletons/flex_tpose.npy"
+).item()
+
+local_rotation = SO3.from_numpy(tpose["local_rotation"], dtype="float32")
+root_translation = Vector3D.from_numpy(tpose["root_translation"], dtype="float32")
+skeleton_tree = tpose["skeleton_tree"]
+parent_indices = Tensor.from_numpy(skeleton_tree["parent_indices"], dtype="int32")
+local_translation = Vector3D.from_numpy(
+    skeleton_tree["local_translation"], dtype="float32"
+)
+node_names = skeleton_tree["node_names"]
+skeleton_tree = SkeletonTree(node_names, parent_indices, local_translation)
+skeleton_state = SkeletonState.from_rotation_and_root_translation(
+    skeleton_tree=skeleton_tree, r=local_rotation, t=root_translation, is_local=True
+)
+
+skeleton_state.to_file(
+    "/home/serfcx/DL_Animation/rl_mimic/data/skeletons/flex_tpose_new.npy"
+)
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/visualization/__init__.py b/source/unitreelab/unitreelab/MotionLib/poselib/visualization/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/visualization/common.py b/source/unitreelab/unitreelab/MotionLib/poselib/visualization/common.py
new file mode 100644
index 0000000..8e5dc04
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/visualization/common.py
@@ -0,0 +1,188 @@
+# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
+# NVIDIA CORPORATION and its licensors retain all intellectual property
+# and proprietary rights in and to this software, related documentation
+# and any modifications thereto.  Any use, reproduction, disclosure or
+# distribution of this software and related documentation without an express
+# license agreement from NVIDIA CORPORATION is strictly prohibited.
+
+import os
+
+from ..core import logger
+from .plt_plotter import Matplotlib3DPlotter
+from .skeleton_plotter_tasks import Draw3DSkeletonMotion, Draw3DSkeletonState
+
+
+def plot_skeleton_state(skeleton_state, task_name=""):
+    """
+    Visualize a skeleton state
+
+    :param skeleton_state:
+    :param task_name:
+    :type skeleton_state: SkeletonState
+    :type task_name: string, optional
+    """
+    logger.info("plotting {}".format(task_name))
+    task = Draw3DSkeletonState(task_name=task_name, skeleton_state=skeleton_state)
+    plotter = Matplotlib3DPlotter(task)
+    plotter.show()
+
+
+def plot_skeleton_states(skeleton_state, skip_n=1, task_name=""):
+    """
+    Visualize a sequence of skeleton state. The dimension of the skeleton state must be 1
+
+    :param skeleton_state:
+    :param task_name:
+    :type skeleton_state: SkeletonState
+    :type task_name: string, optional
+    """
+    logger.info("plotting {} motion".format(task_name))
+    assert len(skeleton_state.shape) == 1, "the state must have only one dimension"
+    task = Draw3DSkeletonState(task_name=task_name, skeleton_state=skeleton_state[0])
+    plotter = Matplotlib3DPlotter(task)
+    for frame_id in range(skeleton_state.shape[0]):
+        if frame_id % skip_n != 0:
+            continue
+        task.update(skeleton_state[frame_id])
+        plotter.update()
+    plotter.show()
+
+
+def plot_skeleton_motion(skeleton_motion, skip_n=1, task_name=""):
+    """
+    Visualize a skeleton motion along its first dimension.
+
+    :param skeleton_motion:
+    :param task_name:
+    :type skeleton_motion: SkeletonMotion
+    :type task_name: string, optional
+    """
+    logger.info("plotting {} motion".format(task_name))
+    task = Draw3DSkeletonMotion(
+        task_name=task_name, skeleton_motion=skeleton_motion, frame_index=0
+    )
+    plotter = Matplotlib3DPlotter(task)
+    for frame_id in range(len(skeleton_motion)):
+        if frame_id % skip_n != 0:
+            continue
+        task.update(frame_id)
+        plotter.update()
+    plotter.show()
+
+
+def plot_skeleton_motion_interactive_base(skeleton_motion, task_name=""):
+    class PlotParams:
+        def __init__(self, total_num_frames):
+            self.current_frame = 0
+            self.playing = False
+            self.looping = False
+            self.confirmed = False
+            self.playback_speed = 4
+            self.total_num_frames = total_num_frames
+
+        def sync(self, other):
+            self.current_frame = other.current_frame
+            self.playing = other.playing
+            self.looping = other.current_frame
+            self.confirmed = other.confirmed
+            self.playback_speed = other.playback_speed
+            self.total_num_frames = other.total_num_frames
+
+    task = Draw3DSkeletonMotion(
+        task_name=task_name, skeleton_motion=skeleton_motion, frame_index=0
+    )
+    plotter = Matplotlib3DPlotter(task)
+
+    plot_params = PlotParams(total_num_frames=len(skeleton_motion))
+    print("Entered interactive plot - press 'n' to quit, 'h' for a list of commands")
+
+    def press(event):
+        if event.key == "x":
+            plot_params.playing = not plot_params.playing
+        elif event.key == "z":
+            plot_params.current_frame = plot_params.current_frame - 1
+        elif event.key == "c":
+            plot_params.current_frame = plot_params.current_frame + 1
+        elif event.key == "a":
+            plot_params.current_frame = plot_params.current_frame - 20
+        elif event.key == "d":
+            plot_params.current_frame = plot_params.current_frame + 20
+        elif event.key == "w":
+            plot_params.looping = not plot_params.looping
+            print("Looping: {}".format(plot_params.looping))
+        elif event.key == "v":
+            plot_params.playback_speed *= 2
+            print("playback speed: {}".format(plot_params.playback_speed))
+        elif event.key == "b":
+            if plot_params.playback_speed != 1:
+                plot_params.playback_speed //= 2
+            print("playback speed: {}".format(plot_params.playback_speed))
+        elif event.key == "n":
+            plot_params.confirmed = True
+        elif event.key == "h":
+            rows, columns = os.popen("stty size", "r").read().split()
+            columns = int(columns)
+            print("=" * columns)
+            print("x: play/pause")
+            print("z: previous frame")
+            print("c: next frame")
+            print("a: jump 10 frames back")
+            print("d: jump 10 frames forward")
+            print("w: looping/non-looping")
+            print("v: double speed (this can be applied multiple times)")
+            print("b: half speed (this can be applied multiple times)")
+            print("n: quit")
+            print("h: help")
+            print("=" * columns)
+
+        print(
+            'current frame index: {}/{} (press "n" to quit)'.format(
+                plot_params.current_frame, plot_params.total_num_frames - 1
+            )
+        )
+
+    plotter.fig.canvas.mpl_connect("key_press_event", press)
+    while True:
+        reset_trail = False
+        if plot_params.confirmed:
+            break
+        if plot_params.playing:
+            plot_params.current_frame += plot_params.playback_speed
+        if plot_params.current_frame >= plot_params.total_num_frames:
+            if plot_params.looping:
+                plot_params.current_frame %= plot_params.total_num_frames
+                reset_trail = True
+            else:
+                plot_params.current_frame = plot_params.total_num_frames - 1
+        if plot_params.current_frame < 0:
+            if plot_params.looping:
+                plot_params.current_frame %= plot_params.total_num_frames
+                reset_trail = True
+            else:
+                plot_params.current_frame = 0
+        yield plot_params
+        task.update(plot_params.current_frame, reset_trail)
+        plotter.update()
+
+
+def plot_skeleton_motion_interactive(skeleton_motion, task_name=""):
+    """
+    Visualize a skeleton motion along its first dimension interactively.
+
+    :param skeleton_motion:
+    :param task_name:
+    :type skeleton_motion: SkeletonMotion
+    :type task_name: string, optional
+    """
+    for _ in plot_skeleton_motion_interactive_base(skeleton_motion, task_name):
+        pass
+
+
+def plot_skeleton_motion_interactive_multiple(*callables, sync=True):
+    for _ in zip(*callables):
+        if sync:
+            for p1, p2 in zip(_[:-1], _[1:]):
+                p2.sync(p1)
+
+
+# def plot_skeleton_motion_interactive_multiple_same(skeleton_motions, task_name=""):
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/visualization/core.py b/source/unitreelab/unitreelab/MotionLib/poselib/visualization/core.py
new file mode 100644
index 0000000..3c7a176
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/visualization/core.py
@@ -0,0 +1,78 @@
+# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
+# NVIDIA CORPORATION and its licensors retain all intellectual property
+# and proprietary rights in and to this software, related documentation
+# and any modifications thereto.  Any use, reproduction, disclosure or
+# distribution of this software and related documentation without an express
+# license agreement from NVIDIA CORPORATION is strictly prohibited.
+
+"""
+The base abstract classes for plotter and the plotting tasks. It describes how the plotter
+deals with the tasks in the general cases
+"""
+from typing import List
+
+
+class BasePlotterTask(object):
+    _task_name: str  # unique name of the task
+    _task_type: str  # type of the task is used to identify which callable
+
+    def __init__(self, task_name: str, task_type: str) -> None:
+        self._task_name = task_name
+        self._task_type = task_type
+
+    @property
+    def task_name(self):
+        return self._task_name
+
+    @property
+    def task_type(self):
+        return self._task_type
+
+    def get_scoped_name(self, name):
+        return self._task_name + "/" + name
+
+    def __iter__(self):
+        """Should override this function to return a list of task primitives
+        """
+        raise NotImplementedError
+
+
+class BasePlotterTasks(object):
+    def __init__(self, tasks) -> None:
+        self._tasks = tasks
+
+    def __iter__(self):
+        for task in self._tasks:
+            yield from task
+
+
+class BasePlotter(object):
+    """An abstract plotter which deals with a plotting task. The children class needs to implement
+    the functions to create/update the objects according to the task given
+    """
+
+    _task_primitives: List[BasePlotterTask]
+
+    def __init__(self, task: BasePlotterTask) -> None:
+        self._task_primitives = []
+        self.create(task)
+
+    @property
+    def task_primitives(self):
+        return self._task_primitives
+
+    def create(self, task: BasePlotterTask) -> None:
+        """Create more task primitives from a task for the plotter"""
+        new_task_primitives = list(task)  # get all task primitives
+        self._task_primitives += new_task_primitives  # append them
+        self._create_impl(new_task_primitives)
+
+    def update(self) -> None:
+        """Update the plotter for any updates in the task primitives"""
+        self._update_impl(self._task_primitives)
+
+    def _update_impl(self, task_list: List[BasePlotterTask]) -> None:
+        raise NotImplementedError
+
+    def _create_impl(self, task_list: List[BasePlotterTask]) -> None:
+        raise NotImplementedError
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/visualization/plt_plotter.py b/source/unitreelab/unitreelab/MotionLib/poselib/visualization/plt_plotter.py
new file mode 100644
index 0000000..0984020
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/visualization/plt_plotter.py
@@ -0,0 +1,402 @@
+# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
+# NVIDIA CORPORATION and its licensors retain all intellectual property
+# and proprietary rights in and to this software, related documentation
+# and any modifications thereto.  Any use, reproduction, disclosure or
+# distribution of this software and related documentation without an express
+# license agreement from NVIDIA CORPORATION is strictly prohibited.
+
+"""
+The matplotlib plotter implementation for all the primitive tasks (in our case: lines and
+dots)
+"""
+from typing import Any, Callable, Dict, List
+
+import matplotlib.pyplot as plt
+import mpl_toolkits.mplot3d.axes3d as p3
+
+import numpy as np
+
+from .core import BasePlotter, BasePlotterTask
+
+
+class Matplotlib2DPlotter(BasePlotter):
+    _fig: plt.figure  # plt figure
+    _ax: plt.axis  # plt axis
+    # stores artist objects for each task (task name as the key)
+    _artist_cache: Dict[str, Any]
+    # callables for each task primitives
+    _create_impl_callables: Dict[str, Callable]
+    _update_impl_callables: Dict[str, Callable]
+
+    def __init__(self, task: "BasePlotterTask") -> None:
+        fig, ax = plt.subplots()
+        self._fig = fig
+        self._ax = ax
+        self._artist_cache = {}
+
+        self._create_impl_callables = {
+            "Draw2DLines": self._lines_create_impl,
+            "Draw2DDots": self._dots_create_impl,
+            "Draw2DTrail": self._trail_create_impl,
+        }
+        self._update_impl_callables = {
+            "Draw2DLines": self._lines_update_impl,
+            "Draw2DDots": self._dots_update_impl,
+            "Draw2DTrail": self._trail_update_impl,
+        }
+        self._init_lim()
+        super().__init__(task)
+
+    @property
+    def ax(self):
+        return self._ax
+
+    @property
+    def fig(self):
+        return self._fig
+
+    def show(self):
+        plt.show()
+
+    def _min(self, x, y):
+        if x is None:
+            return y
+        if y is None:
+            return x
+        return min(x, y)
+
+    def _max(self, x, y):
+        if x is None:
+            return y
+        if y is None:
+            return x
+        return max(x, y)
+
+    def _init_lim(self):
+        self._curr_x_min = None
+        self._curr_y_min = None
+        self._curr_x_max = None
+        self._curr_y_max = None
+
+    def _update_lim(self, xs, ys):
+        self._curr_x_min = self._min(np.min(xs), self._curr_x_min)
+        self._curr_y_min = self._min(np.min(ys), self._curr_y_min)
+        self._curr_x_max = self._max(np.max(xs), self._curr_x_max)
+        self._curr_y_max = self._max(np.max(ys), self._curr_y_max)
+
+    def _set_lim(self):
+        if not (
+            self._curr_x_min is None
+            or self._curr_x_max is None
+            or self._curr_y_min is None
+            or self._curr_y_max is None
+        ):
+            self._ax.set_xlim(self._curr_x_min, self._curr_x_max)
+            self._ax.set_ylim(self._curr_y_min, self._curr_y_max)
+        self._init_lim()
+
+    @staticmethod
+    def _lines_extract_xy_impl(index, lines_task):
+        return lines_task[index, :, 0], lines_task[index, :, 1]
+
+    @staticmethod
+    def _trail_extract_xy_impl(index, trail_task):
+        return (trail_task[index : index + 2, 0], trail_task[index : index + 2, 1])
+
+    def _lines_create_impl(self, lines_task):
+        color = lines_task.color
+        self._artist_cache[lines_task.task_name] = [
+            self._ax.plot(
+                *Matplotlib2DPlotter._lines_extract_xy_impl(i, lines_task),
+                color=color,
+                linewidth=lines_task.line_width,
+                alpha=lines_task.alpha
+            )[0]
+            for i in range(len(lines_task))
+        ]
+
+    def _lines_update_impl(self, lines_task):
+        lines_artists = self._artist_cache[lines_task.task_name]
+        for i in range(len(lines_task)):
+            artist = lines_artists[i]
+            xs, ys = Matplotlib2DPlotter._lines_extract_xy_impl(i, lines_task)
+            artist.set_data(xs, ys)
+            if lines_task.influence_lim:
+                self._update_lim(xs, ys)
+
+    def _dots_create_impl(self, dots_task):
+        color = dots_task.color
+        self._artist_cache[dots_task.task_name] = self._ax.plot(
+            dots_task[:, 0],
+            dots_task[:, 1],
+            c=color,
+            linestyle="",
+            marker=".",
+            markersize=dots_task.marker_size,
+            alpha=dots_task.alpha,
+        )[0]
+
+    def _dots_update_impl(self, dots_task):
+        dots_artist = self._artist_cache[dots_task.task_name]
+        dots_artist.set_data(dots_task[:, 0], dots_task[:, 1])
+        if dots_task.influence_lim:
+            self._update_lim(dots_task[:, 0], dots_task[:, 1])
+
+    def _trail_create_impl(self, trail_task):
+        color = trail_task.color
+        trail_length = len(trail_task) - 1
+        self._artist_cache[trail_task.task_name] = [
+            self._ax.plot(
+                *Matplotlib2DPlotter._trail_extract_xy_impl(i, trail_task),
+                color=trail_task.color,
+                linewidth=trail_task.line_width,
+                alpha=trail_task.alpha * (1.0 - i / (trail_length - 1))
+            )[0]
+            for i in range(trail_length)
+        ]
+
+    def _trail_update_impl(self, trail_task):
+        trails_artists = self._artist_cache[trail_task.task_name]
+        for i in range(len(trail_task) - 1):
+            artist = trails_artists[i]
+            xs, ys = Matplotlib2DPlotter._trail_extract_xy_impl(i, trail_task)
+            artist.set_data(xs, ys)
+            if trail_task.influence_lim:
+                self._update_lim(xs, ys)
+
+    def _create_impl(self, task_list):
+        for task in task_list:
+            self._create_impl_callables[task.task_type](task)
+        self._draw()
+
+    def _update_impl(self, task_list):
+        for task in task_list:
+            self._update_impl_callables[task.task_type](task)
+        self._draw()
+
+    def _set_aspect_equal_2d(self, zero_centered=True):
+        xlim = self._ax.get_xlim()
+        ylim = self._ax.get_ylim()
+
+        if not zero_centered:
+            xmean = np.mean(xlim)
+            ymean = np.mean(ylim)
+        else:
+            xmean = 0
+            ymean = 0
+
+        plot_radius = max(
+            [
+                abs(lim - mean_)
+                for lims, mean_ in ((xlim, xmean), (ylim, ymean))
+                for lim in lims
+            ]
+        )
+
+        self._ax.set_xlim([xmean - plot_radius, xmean + plot_radius])
+        self._ax.set_ylim([ymean - plot_radius, ymean + plot_radius])
+
+    def _draw(self):
+        self._set_lim()
+        self._set_aspect_equal_2d()
+        self._fig.canvas.draw()
+        self._fig.canvas.flush_events()
+        plt.pause(0.00001)
+
+
+class Matplotlib3DPlotter(BasePlotter):
+    _fig: plt.figure  # plt figure
+    _ax: p3.Axes3D  # plt 3d axis
+    # stores artist objects for each task (task name as the key)
+    _artist_cache: Dict[str, Any]
+    # callables for each task primitives
+    _create_impl_callables: Dict[str, Callable]
+    _update_impl_callables: Dict[str, Callable]
+
+    def __init__(self, task: "BasePlotterTask") -> None:
+        self._fig = plt.figure()
+        self._ax = p3.Axes3D(self._fig)
+        self._artist_cache = {}
+
+        self._create_impl_callables = {
+            "Draw3DLines": self._lines_create_impl,
+            "Draw3DDots": self._dots_create_impl,
+            "Draw3DTrail": self._trail_create_impl,
+        }
+        self._update_impl_callables = {
+            "Draw3DLines": self._lines_update_impl,
+            "Draw3DDots": self._dots_update_impl,
+            "Draw3DTrail": self._trail_update_impl,
+        }
+        self._init_lim()
+        super().__init__(task)
+
+    @property
+    def ax(self):
+        return self._ax
+
+    @property
+    def fig(self):
+        return self._fig
+
+    def show(self):
+        plt.show()
+
+    def _min(self, x, y):
+        if x is None:
+            return y
+        if y is None:
+            return x
+        return min(x, y)
+
+    def _max(self, x, y):
+        if x is None:
+            return y
+        if y is None:
+            return x
+        return max(x, y)
+
+    def _init_lim(self):
+        self._curr_x_min = None
+        self._curr_y_min = None
+        self._curr_z_min = None
+        self._curr_x_max = None
+        self._curr_y_max = None
+        self._curr_z_max = None
+
+    def _update_lim(self, xs, ys, zs):
+        self._curr_x_min = self._min(np.min(xs), self._curr_x_min)
+        self._curr_y_min = self._min(np.min(ys), self._curr_y_min)
+        self._curr_z_min = self._min(np.min(zs), self._curr_z_min)
+        self._curr_x_max = self._max(np.max(xs), self._curr_x_max)
+        self._curr_y_max = self._max(np.max(ys), self._curr_y_max)
+        self._curr_z_max = self._max(np.max(zs), self._curr_z_max)
+
+    def _set_lim(self):
+        if not (
+            self._curr_x_min is None
+            or self._curr_x_max is None
+            or self._curr_y_min is None
+            or self._curr_y_max is None
+            or self._curr_z_min is None
+            or self._curr_z_max is None
+        ):
+            self._ax.set_xlim3d(self._curr_x_min, self._curr_x_max)
+            self._ax.set_ylim3d(self._curr_y_min, self._curr_y_max)
+            self._ax.set_zlim3d(self._curr_z_min, self._curr_z_max)
+        self._init_lim()
+
+    @staticmethod
+    def _lines_extract_xyz_impl(index, lines_task):
+        return lines_task[index, :, 0], lines_task[index, :, 1], lines_task[index, :, 2]
+
+    @staticmethod
+    def _trail_extract_xyz_impl(index, trail_task):
+        return (
+            trail_task[index : index + 2, 0],
+            trail_task[index : index + 2, 1],
+            trail_task[index : index + 2, 2],
+        )
+
+    def _lines_create_impl(self, lines_task):
+        color = lines_task.color
+        self._artist_cache[lines_task.task_name] = [
+            self._ax.plot(
+                *Matplotlib3DPlotter._lines_extract_xyz_impl(i, lines_task),
+                color=color,
+                linewidth=lines_task.line_width,
+                alpha=lines_task.alpha
+            )[0]
+            for i in range(len(lines_task))
+        ]
+
+    def _lines_update_impl(self, lines_task):
+        lines_artists = self._artist_cache[lines_task.task_name]
+        for i in range(len(lines_task)):
+            artist = lines_artists[i]
+            xs, ys, zs = Matplotlib3DPlotter._lines_extract_xyz_impl(i, lines_task)
+            artist.set_data(xs, ys)
+            artist.set_3d_properties(zs)
+            if lines_task.influence_lim:
+                self._update_lim(xs, ys, zs)
+
+    def _dots_create_impl(self, dots_task):
+        color = dots_task.color
+        self._artist_cache[dots_task.task_name] = self._ax.plot(
+            dots_task[:, 0],
+            dots_task[:, 1],
+            dots_task[:, 2],
+            c=color,
+            linestyle="",
+            marker=".",
+            markersize=dots_task.marker_size,
+            alpha=dots_task.alpha,
+        )[0]
+
+    def _dots_update_impl(self, dots_task):
+        dots_artist = self._artist_cache[dots_task.task_name]
+        dots_artist.set_data(dots_task[:, 0], dots_task[:, 1])
+        dots_artist.set_3d_properties(dots_task[:, 2])
+        if dots_task.influence_lim:
+            self._update_lim(dots_task[:, 0], dots_task[:, 1], dots_task[:, 2])
+
+    def _trail_create_impl(self, trail_task):
+        color = trail_task.color
+        trail_length = len(trail_task) - 1
+        self._artist_cache[trail_task.task_name] = [
+            self._ax.plot(
+                *Matplotlib3DPlotter._trail_extract_xyz_impl(i, trail_task),
+                color=trail_task.color,
+                linewidth=trail_task.line_width,
+                alpha=trail_task.alpha * (1.0 - i / (trail_length - 1))
+            )[0]
+            for i in range(trail_length)
+        ]
+
+    def _trail_update_impl(self, trail_task):
+        trails_artists = self._artist_cache[trail_task.task_name]
+        for i in range(len(trail_task) - 1):
+            artist = trails_artists[i]
+            xs, ys, zs = Matplotlib3DPlotter._trail_extract_xyz_impl(i, trail_task)
+            artist.set_data(xs, ys)
+            artist.set_3d_properties(zs)
+            if trail_task.influence_lim:
+                self._update_lim(xs, ys, zs)
+
+    def _create_impl(self, task_list):
+        for task in task_list:
+            self._create_impl_callables[task.task_type](task)
+        self._draw()
+
+    def _update_impl(self, task_list):
+        for task in task_list:
+            self._update_impl_callables[task.task_type](task)
+        self._draw()
+
+    def _set_aspect_equal_3d(self):
+        xlim = self._ax.get_xlim3d()
+        ylim = self._ax.get_ylim3d()
+        zlim = self._ax.get_zlim3d()
+
+        xmean = np.mean(xlim)
+        ymean = np.mean(ylim)
+        zmean = np.mean(zlim)
+
+        plot_radius = max(
+            [
+                abs(lim - mean_)
+                for lims, mean_ in ((xlim, xmean), (ylim, ymean), (zlim, zmean))
+                for lim in lims
+            ]
+        )
+
+        self._ax.set_xlim3d([xmean - plot_radius, xmean + plot_radius])
+        self._ax.set_ylim3d([ymean - plot_radius, ymean + plot_radius])
+        self._ax.set_zlim3d([zmean - plot_radius, zmean + plot_radius])
+
+    def _draw(self):
+        self._set_lim()
+        self._set_aspect_equal_3d()
+        self._fig.canvas.draw()
+        self._fig.canvas.flush_events()
+        plt.pause(0.00001)
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/visualization/simple_plotter_tasks.py b/source/unitreelab/unitreelab/MotionLib/poselib/visualization/simple_plotter_tasks.py
new file mode 100644
index 0000000..635a3a2
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/visualization/simple_plotter_tasks.py
@@ -0,0 +1,191 @@
+# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
+# NVIDIA CORPORATION and its licensors retain all intellectual property
+# and proprietary rights in and to this software, related documentation
+# and any modifications thereto.  Any use, reproduction, disclosure or
+# distribution of this software and related documentation without an express
+# license agreement from NVIDIA CORPORATION is strictly prohibited.
+
+"""
+This is where all the task primitives are defined
+"""
+import numpy as np
+
+from .core import BasePlotterTask
+
+
+class DrawXDLines(BasePlotterTask):
+    _lines: np.ndarray
+    _color: str
+    _line_width: int
+    _alpha: float
+    _influence_lim: bool
+
+    def __init__(
+        self,
+        task_name: str,
+        lines: np.ndarray,
+        color: str = "blue",
+        line_width: int = 2,
+        alpha: float = 1.0,
+        influence_lim: bool = True,
+    ) -> None:
+        super().__init__(task_name=task_name, task_type=self.__class__.__name__)
+        self._color = color
+        self._line_width = line_width
+        self._alpha = alpha
+        self._influence_lim = influence_lim
+        self.update(lines)
+
+    @property
+    def influence_lim(self) -> bool:
+        return self._influence_lim
+
+    @property
+    def raw_data(self):
+        return self._lines
+
+    @property
+    def color(self):
+        return self._color
+
+    @property
+    def line_width(self):
+        return self._line_width
+
+    @property
+    def alpha(self):
+        return self._alpha
+
+    @property
+    def dim(self):
+        raise NotImplementedError
+
+    @property
+    def name(self):
+        return "{}DLines".format(self.dim)
+
+    def update(self, lines):
+        self._lines = np.array(lines)
+        shape = self._lines.shape
+        assert shape[-1] == self.dim and shape[-2] == 2 and len(shape) == 3
+
+    def __getitem__(self, index):
+        return self._lines[index]
+
+    def __len__(self):
+        return self._lines.shape[0]
+
+    def __iter__(self):
+        yield self
+
+
+class DrawXDDots(BasePlotterTask):
+    _dots: np.ndarray
+    _color: str
+    _marker_size: int
+    _alpha: float
+    _influence_lim: bool
+
+    def __init__(
+        self,
+        task_name: str,
+        dots: np.ndarray,
+        color: str = "blue",
+        marker_size: int = 10,
+        alpha: float = 1.0,
+        influence_lim: bool = True,
+    ) -> None:
+        super().__init__(task_name=task_name, task_type=self.__class__.__name__)
+        self._color = color
+        self._marker_size = marker_size
+        self._alpha = alpha
+        self._influence_lim = influence_lim
+        self.update(dots)
+
+    def update(self, dots):
+        self._dots = np.array(dots)
+        shape = self._dots.shape
+        assert shape[-1] == self.dim and len(shape) == 2
+
+    def __getitem__(self, index):
+        return self._dots[index]
+
+    def __len__(self):
+        return self._dots.shape[0]
+
+    def __iter__(self):
+        yield self
+
+    @property
+    def influence_lim(self) -> bool:
+        return self._influence_lim
+
+    @property
+    def raw_data(self):
+        return self._dots
+
+    @property
+    def color(self):
+        return self._color
+
+    @property
+    def marker_size(self):
+        return self._marker_size
+
+    @property
+    def alpha(self):
+        return self._alpha
+
+    @property
+    def dim(self):
+        raise NotImplementedError
+
+    @property
+    def name(self):
+        return "{}DDots".format(self.dim)
+
+
+class DrawXDTrail(DrawXDDots):
+    @property
+    def line_width(self):
+        return self.marker_size
+
+    @property
+    def name(self):
+        return "{}DTrail".format(self.dim)
+
+
+class Draw2DLines(DrawXDLines):
+    @property
+    def dim(self):
+        return 2
+
+
+class Draw3DLines(DrawXDLines):
+    @property
+    def dim(self):
+        return 3
+
+
+class Draw2DDots(DrawXDDots):
+    @property
+    def dim(self):
+        return 2
+
+
+class Draw3DDots(DrawXDDots):
+    @property
+    def dim(self):
+        return 3
+
+
+class Draw2DTrail(DrawXDTrail):
+    @property
+    def dim(self):
+        return 2
+
+
+class Draw3DTrail(DrawXDTrail):
+    @property
+    def dim(self):
+        return 3
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/visualization/skeleton_plotter_tasks.py b/source/unitreelab/unitreelab/MotionLib/poselib/visualization/skeleton_plotter_tasks.py
new file mode 100644
index 0000000..637497b
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/visualization/skeleton_plotter_tasks.py
@@ -0,0 +1,194 @@
+# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
+# NVIDIA CORPORATION and its licensors retain all intellectual property
+# and proprietary rights in and to this software, related documentation
+# and any modifications thereto.  Any use, reproduction, disclosure or
+# distribution of this software and related documentation without an express
+# license agreement from NVIDIA CORPORATION is strictly prohibited.
+
+"""
+This is where all skeleton related complex tasks are defined (skeleton state and skeleton
+motion)
+"""
+import numpy as np
+
+from .core import BasePlotterTask
+from .simple_plotter_tasks import Draw3DDots, Draw3DLines, Draw3DTrail
+
+
+class Draw3DSkeletonState(BasePlotterTask):
+    _lines_task: Draw3DLines  # sub-task for drawing lines
+    _dots_task: Draw3DDots  # sub-task for drawing dots
+
+    def __init__(
+        self,
+        task_name: str,
+        skeleton_state,
+        joints_color: str = "red",
+        lines_color: str = "blue",
+        alpha=1.0,
+    ) -> None:
+        super().__init__(task_name=task_name, task_type="3DSkeletonState")
+        lines, dots = Draw3DSkeletonState._get_lines_and_dots(skeleton_state)
+        self._lines_task = Draw3DLines(
+            self.get_scoped_name("bodies"), lines, joints_color, alpha=alpha
+        )
+        self._dots_task = Draw3DDots(
+            self.get_scoped_name("joints"), dots, lines_color, alpha=alpha
+        )
+
+    @property
+    def name(self):
+        return "3DSkeleton"
+
+    def update(self, skeleton_state) -> None:
+        self._update(*Draw3DSkeletonState._get_lines_and_dots(skeleton_state))
+
+    @staticmethod
+    def _get_lines_and_dots(skeleton_state):
+        """Get all the lines and dots needed to draw the skeleton state
+        """
+        assert (
+            len(skeleton_state.tensor.shape) == 1
+        ), "the state has to be zero dimensional"
+        dots = skeleton_state.global_translation.numpy()
+        skeleton_tree = skeleton_state.skeleton_tree
+        parent_indices = skeleton_tree.parent_indices.numpy()
+        lines = []
+        for node_index in range(len(skeleton_tree)):
+            parent_index = parent_indices[node_index]
+            if parent_index != -1:
+                lines.append([dots[node_index], dots[parent_index]])
+        lines = np.array(lines)
+        return lines, dots
+
+    def _update(self, lines, dots) -> None:
+        self._lines_task.update(lines)
+        self._dots_task.update(dots)
+
+    def __iter__(self):
+        yield from self._lines_task
+        yield from self._dots_task
+
+
+class Draw3DSkeletonMotion(BasePlotterTask):
+    def __init__(
+        self,
+        task_name: str,
+        skeleton_motion,
+        frame_index=None,
+        joints_color="red",
+        lines_color="blue",
+        velocity_color="green",
+        angular_velocity_color="purple",
+        trail_color="black",
+        trail_length=10,
+        alpha=1.0,
+    ) -> None:
+        super().__init__(task_name=task_name, task_type="3DSkeletonMotion")
+        self._trail_length = trail_length
+        self._skeleton_motion = skeleton_motion
+        # if frame_index is None:
+        curr_skeleton_motion = self._skeleton_motion.clone()
+        if frame_index is not None:
+            curr_skeleton_motion.tensor = self._skeleton_motion.tensor[frame_index, :]
+        # else:
+        #     curr_skeleton_motion = self._skeleton_motion[frame_index, :]
+        self._skeleton_state_task = Draw3DSkeletonState(
+            self.get_scoped_name("skeleton_state"),
+            curr_skeleton_motion,
+            joints_color=joints_color,
+            lines_color=lines_color,
+            alpha=alpha,
+        )
+        vel_lines, avel_lines = Draw3DSkeletonMotion._get_vel_and_avel(
+            curr_skeleton_motion
+        )
+        self._com_pos = curr_skeleton_motion.root_translation.numpy()[
+            np.newaxis, ...
+        ].repeat(trail_length, axis=0)
+        self._vel_task = Draw3DLines(
+            self.get_scoped_name("velocity"),
+            vel_lines,
+            velocity_color,
+            influence_lim=False,
+            alpha=alpha,
+        )
+        self._avel_task = Draw3DLines(
+            self.get_scoped_name("angular_velocity"),
+            avel_lines,
+            angular_velocity_color,
+            influence_lim=False,
+            alpha=alpha,
+        )
+        self._com_trail_task = Draw3DTrail(
+            self.get_scoped_name("com_trail"),
+            self._com_pos,
+            trail_color,
+            marker_size=2,
+            influence_lim=True,
+            alpha=alpha,
+        )
+
+    @property
+    def name(self):
+        return "3DSkeletonMotion"
+
+    def update(self, frame_index=None, reset_trail=False, skeleton_motion=None) -> None:
+        if skeleton_motion is not None:
+            self._skeleton_motion = skeleton_motion
+
+        curr_skeleton_motion = self._skeleton_motion.clone()
+        if frame_index is not None:
+            curr_skeleton_motion.tensor = curr_skeleton_motion.tensor[frame_index, :]
+        if reset_trail:
+            self._com_pos = curr_skeleton_motion.root_translation.numpy()[
+                np.newaxis, ...
+            ].repeat(self._trail_length, axis=0)
+        else:
+            self._com_pos = np.concatenate(
+                (
+                    curr_skeleton_motion.root_translation.numpy()[np.newaxis, ...],
+                    self._com_pos[:-1],
+                ),
+                axis=0,
+            )
+        self._skeleton_state_task.update(curr_skeleton_motion)
+        self._com_trail_task.update(self._com_pos)
+        self._update(*Draw3DSkeletonMotion._get_vel_and_avel(curr_skeleton_motion))
+
+    @staticmethod
+    def _get_vel_and_avel(skeleton_motion):
+        """Get all the velocity and angular velocity lines
+        """
+        pos = skeleton_motion.global_translation.numpy()
+        vel = skeleton_motion.global_velocity.numpy()
+        avel = skeleton_motion.global_angular_velocity.numpy()
+
+        vel_lines = np.stack((pos, pos + vel * 0.02), axis=1)
+        avel_lines = np.stack((pos, pos + avel * 0.01), axis=1)
+        return vel_lines, avel_lines
+
+    def _update(self, vel_lines, avel_lines) -> None:
+        self._vel_task.update(vel_lines)
+        self._avel_task.update(avel_lines)
+
+    def __iter__(self):
+        yield from self._skeleton_state_task
+        yield from self._vel_task
+        yield from self._avel_task
+        yield from self._com_trail_task
+
+
+class Draw3DSkeletonMotions(BasePlotterTask):
+    def __init__(self, skeleton_motion_tasks) -> None:
+        self._skeleton_motion_tasks = skeleton_motion_tasks
+
+    @property
+    def name(self):
+        return "3DSkeletonMotions"
+
+    def update(self, frame_index) -> None:
+        list(map(lambda x: x.update(frame_index), self._skeleton_motion_tasks))
+
+    def __iter__(self):
+        yield from self._skeleton_state_tasks
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/visualization/tests/__init__.py b/source/unitreelab/unitreelab/MotionLib/poselib/visualization/tests/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/source/unitreelab/unitreelab/MotionLib/poselib/visualization/tests/test_plotter.py b/source/unitreelab/unitreelab/MotionLib/poselib/visualization/tests/test_plotter.py
new file mode 100644
index 0000000..7ef1fb6
--- /dev/null
+++ b/source/unitreelab/unitreelab/MotionLib/poselib/visualization/tests/test_plotter.py
@@ -0,0 +1,16 @@
+from typing import cast
+
+import matplotlib.pyplot as plt
+import numpy as np
+
+from ..core import BasePlotterTask, BasePlotterTasks
+from ..plt_plotter import Matplotlib3DPlotter
+from ..simple_plotter_tasks import Draw3DDots, Draw3DLines
+
+task = Draw3DLines(task_name="test", 
+    lines=np.array([[[0, 0, 0], [0, 0, 1]], [[0, 1, 1], [0, 1, 0]]]), color="blue")
+task2 = Draw3DDots(task_name="test2", 
+    dots=np.array([[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 1, 0]]), color="red")
+task3 = BasePlotterTasks([task, task2])
+plotter = Matplotlib3DPlotter(cast(BasePlotterTask, task3))
+plt.show()
diff --git a/source/unitreelab/unitreelab/__init__.py b/source/unitreelab/unitreelab/__init__.py
index 4f1d8e9..22865b2 100644
--- a/source/unitreelab/unitreelab/__init__.py
+++ b/source/unitreelab/unitreelab/__init__.py
@@ -4,6 +4,7 @@ Python module serving as a project/extension template.
 
 # Register Gym environments.
 from .tasks import *
+from .managers import *
 
 # Register UI extensions.
 from .ui_extension_example import *
diff --git a/source/unitreelab/unitreelab/assets/g1/.asset_hash b/source/unitreelab/unitreelab/assets/g1/.asset_hash
index f9a253d..b4e431e 100644
--- a/source/unitreelab/unitreelab/assets/g1/.asset_hash
+++ b/source/unitreelab/unitreelab/assets/g1/.asset_hash
@@ -1 +1 @@
-4acc268b3ca8040c5afad39daf5d83c2
\ No newline at end of file
+b676860308a7cfce7d48621ccce8a433
\ No newline at end of file
diff --git a/source/unitreelab/unitreelab/assets/g1/Props/instanceable_meshes.usd b/source/unitreelab/unitreelab/assets/g1/Props/instanceable_meshes.usd
deleted file mode 100644
index 380804e..0000000
--- a/source/unitreelab/unitreelab/assets/g1/Props/instanceable_meshes.usd
+++ /dev/null
@@ -1,3 +0,0 @@
-version https://git-lfs.github.com/spec/v1
-oid sha256:8800ce8ed5e5471b24ace905eca9921e808394df80c6266a702383c5b32f3e37
-size 27238254
diff --git a/source/unitreelab/unitreelab/assets/g1/config.yaml b/source/unitreelab/unitreelab/assets/g1/config.yaml
index b5e2897..93905e5 100644
--- a/source/unitreelab/unitreelab/assets/g1/config.yaml
+++ b/source/unitreelab/unitreelab/assets/g1/config.yaml
@@ -1,18 +1,23 @@
-asset_path: /home/hellod035/projects/IsaacLab/g1_description/g1_29dof_rev_1_0.urdf
-usd_dir: /home/hellod035/projects/IsaacLab/g1_test
+asset_path: /home/hellod035/software/IsaacLab/g1/g1.urdf
+usd_dir: /home/hellod035/software/IsaacLab/g1_out
 usd_file_name: g1.usd
 force_usd_conversion: true
 make_instanceable: true
-import_inertia_tensor: true
 fix_base: false
+root_link_name: null
+link_density: 0.0
 merge_fixed_joints: true
+convert_mimic_joints_to_normal_joints: false
+joint_drive:
+  drive_type: force
+  target_type: position
+  gains:
+    stiffness: 100.0
+    damping: 1.0
+collider_type: convex_hull
 self_collision: false
-default_drive_type: none
-override_joint_dynamics: false
-default_drive_stiffness: 0.0
-default_drive_damping: 0.0
-link_density: 0.0
-convex_decompose_mesh: false
+replace_cylinders_with_capsules: false
+collision_from_visuals: false
 ##
-# Generated by UrdfConverter on 2025-01-21 at 14:23:22.
+# Generated by UrdfConverter on 2025-02-06 at 11:39:25.
 ##
diff --git a/source/unitreelab/unitreelab/assets/g1/configuration/g1_base.usd b/source/unitreelab/unitreelab/assets/g1/configuration/g1_base.usd
new file mode 100644
index 0000000..0830d53
--- /dev/null
+++ b/source/unitreelab/unitreelab/assets/g1/configuration/g1_base.usd
@@ -0,0 +1,3 @@
+version https://git-lfs.github.com/spec/v1
+oid sha256:3563f3a0995ba3877a63f28c4fcee078b387ea9099da37359aca7b7fa3a5a382
+size 28336462
diff --git a/source/unitreelab/unitreelab/assets/g1/configuration/g1_physics.usd b/source/unitreelab/unitreelab/assets/g1/configuration/g1_physics.usd
new file mode 100644
index 0000000..00fea2a
--- /dev/null
+++ b/source/unitreelab/unitreelab/assets/g1/configuration/g1_physics.usd
@@ -0,0 +1,3 @@
+version https://git-lfs.github.com/spec/v1
+oid sha256:1f37dd1309cd19ab94250284023905e0c0188cf336362e9bba17f018340acd8d
+size 8626
diff --git a/source/unitreelab/unitreelab/assets/g1/configuration/g1_sensor.usd b/source/unitreelab/unitreelab/assets/g1/configuration/g1_sensor.usd
new file mode 100644
index 0000000..3eb3f8b
--- /dev/null
+++ b/source/unitreelab/unitreelab/assets/g1/configuration/g1_sensor.usd
@@ -0,0 +1,3 @@
+version https://git-lfs.github.com/spec/v1
+oid sha256:5115364b53bffbb37007af0fd1038336679b66abdae134bc87d0fca9829f9b60
+size 656
diff --git a/source/unitreelab/unitreelab/assets/g1/g1.usd b/source/unitreelab/unitreelab/assets/g1/g1.usd
index 50bb858..8c1ff3d 100644
--- a/source/unitreelab/unitreelab/assets/g1/g1.usd
+++ b/source/unitreelab/unitreelab/assets/g1/g1.usd
@@ -1,3 +1,3 @@
 version https://git-lfs.github.com/spec/v1
-oid sha256:d11dda071a0edfeb621b60eff971d693e66e7752caa1c6b3d4824725a489ca1a
-size 27250682
+oid sha256:388c387c642057036c7932ffe8c0e200d27b8e62fea0490798d292d0f3629f3e
+size 1600
diff --git a/source/unitreelab/unitreelab/assets/h1/.asset_hash b/source/unitreelab/unitreelab/assets/h1/.asset_hash
new file mode 100644
index 0000000..22a2629
--- /dev/null
+++ b/source/unitreelab/unitreelab/assets/h1/.asset_hash
@@ -0,0 +1 @@
+da74e4a7a0e4454406cf6ebafc93655b
\ No newline at end of file
diff --git a/source/unitreelab/unitreelab/assets/h1/config.yaml b/source/unitreelab/unitreelab/assets/h1/config.yaml
new file mode 100644
index 0000000..2a7d00a
--- /dev/null
+++ b/source/unitreelab/unitreelab/assets/h1/config.yaml
@@ -0,0 +1,23 @@
+asset_path: /home/hellod035/software/IsaacLab/h1/urdf/h1_extended.urdf
+usd_dir: /home/hellod035/software/IsaacLab/h1_out
+usd_file_name: h1.usd
+force_usd_conversion: true
+make_instanceable: true
+fix_base: false
+root_link_name: null
+link_density: 0.0
+merge_fixed_joints: false
+convert_mimic_joints_to_normal_joints: false
+joint_drive:
+  drive_type: force
+  target_type: position
+  gains:
+    stiffness: 100.0
+    damping: 1.0
+collider_type: convex_hull
+self_collision: false
+replace_cylinders_with_capsules: false
+collision_from_visuals: false
+##
+# Generated by UrdfConverter on 2025-02-06 at 11:32:18.
+##
diff --git a/source/unitreelab/unitreelab/assets/h1/configuration/h1_base.usd b/source/unitreelab/unitreelab/assets/h1/configuration/h1_base.usd
new file mode 100644
index 0000000..bf67ef0
--- /dev/null
+++ b/source/unitreelab/unitreelab/assets/h1/configuration/h1_base.usd
@@ -0,0 +1,3 @@
+version https://git-lfs.github.com/spec/v1
+oid sha256:0f7448196f894340daa7ca2bf831b47d86690723a63e7a79fd011cad1500e181
+size 24067809
diff --git a/source/unitreelab/unitreelab/assets/h1/configuration/h1_physics.usd b/source/unitreelab/unitreelab/assets/h1/configuration/h1_physics.usd
new file mode 100644
index 0000000..4fe2a90
--- /dev/null
+++ b/source/unitreelab/unitreelab/assets/h1/configuration/h1_physics.usd
@@ -0,0 +1,3 @@
+version https://git-lfs.github.com/spec/v1
+oid sha256:921a581f4e369a8a944f791ce4b02dbb9a51a65c5f3ddd28afcc91f9e9143593
+size 7297
diff --git a/source/unitreelab/unitreelab/assets/h1/configuration/h1_sensor.usd b/source/unitreelab/unitreelab/assets/h1/configuration/h1_sensor.usd
new file mode 100644
index 0000000..fa06ace
--- /dev/null
+++ b/source/unitreelab/unitreelab/assets/h1/configuration/h1_sensor.usd
@@ -0,0 +1,3 @@
+version https://git-lfs.github.com/spec/v1
+oid sha256:ae888ecf7f281ef2628ad88024a78ae77c6d3cc1c5de41fa56074aba717d3b9f
+size 654
diff --git a/source/unitreelab/unitreelab/assets/h1/h1.usd b/source/unitreelab/unitreelab/assets/h1/h1.usd
index 3cef52d..90e4583 100644
--- a/source/unitreelab/unitreelab/assets/h1/h1.usd
+++ b/source/unitreelab/unitreelab/assets/h1/h1.usd
@@ -1,3 +1,3 @@
 version https://git-lfs.github.com/spec/v1
-oid sha256:82a2ec74a5d91c99cfa8b5e1d18a355a222a18aa91bc4080a53ef2488e2f584c
-size 27487462
+oid sha256:3f128fc0f9d66c4225f03d8a5f80570514c0b25cf2da580ddb12e536e61fd14a
+size 1600
diff --git a/source/unitreelab/unitreelab/assets/unitree.py b/source/unitreelab/unitreelab/assets/unitree.py
index 8194fa4..c787aba 100644
--- a/source/unitreelab/unitreelab/assets/unitree.py
+++ b/source/unitreelab/unitreelab/assets/unitree.py
@@ -31,7 +31,7 @@ G1_CFG = ArticulationCfg(
             max_depenetration_velocity=1.0,
         ),
         articulation_props=sim_utils.ArticulationRootPropertiesCfg(
-            enabled_self_collisions=True, solver_position_iteration_count=4, solver_velocity_iteration_count=0
+            enabled_self_collisions=True, solver_position_iteration_count=4, solver_velocity_iteration_count=1
         ),
     ),
     init_state=ArticulationCfg.InitialStateCfg(
@@ -145,7 +145,7 @@ H1_CFG = ArticulationCfg(
             max_depenetration_velocity=1.0,
         ),
         articulation_props=sim_utils.ArticulationRootPropertiesCfg(
-            enabled_self_collisions=False, solver_position_iteration_count=4, solver_velocity_iteration_count=4
+            enabled_self_collisions=False, solver_position_iteration_count=4, solver_velocity_iteration_count=1
         ),
     ),
     init_state=ArticulationCfg.InitialStateCfg(
diff --git a/source/unitreelab/unitreelab/envs/manager_based_rl_wbc_env.py b/source/unitreelab/unitreelab/envs/manager_based_rl_wbc_env.py
new file mode 100644
index 0000000..fb774d3
--- /dev/null
+++ b/source/unitreelab/unitreelab/envs/manager_based_rl_wbc_env.py
@@ -0,0 +1,31 @@
+# Copyright (c) 2024-2025 Ziqi Fan
+# SPDX-License-Identifier: Apache-2.0
+
+# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
+# All rights reserved.
+#
+# SPDX-License-Identifier: BSD-3-Clause
+
+# needed to import for allowing type-hinting: np.ndarray | None
+from __future__ import annotations
+
+import gymnasium as gym
+import torch
+
+from isaaclab.envs.common import VecEnvStepReturn
+from isaaclab.envs.manager_based_rl_env import ManagerBasedRLEnv
+from unitreelab.envs.manager_based_rl_wbc_env_cfg import ManagerBasedRLWBCEnvCfg
+from unitreelab.managers import MotionManager
+
+
+class ManagerBasedRLWBCEnv(ManagerBasedRLEnv, gym.Env):
+    def __init__(self, cfg: ManagerBasedRLWBCEnvCfg, render_mode: str | None = None, **kwargs):
+        super().__init__(cfg, render_mode, **kwargs)
+
+    def step(self, action: torch.Tensor) -> VecEnvStepReturn:
+        return super().step(action)
+
+    def load_managers(self):
+        super().load_managers()
+        self.motion_manager = MotionManager(self.cfg.motion, self)
+        print("[INFO] Motion Manager: ", self.motion_manager)
diff --git a/source/unitreelab/unitreelab/envs/manager_based_rl_wbc_env_cfg.py b/source/unitreelab/unitreelab/envs/manager_based_rl_wbc_env_cfg.py
new file mode 100644
index 0000000..8be7fb6
--- /dev/null
+++ b/source/unitreelab/unitreelab/envs/manager_based_rl_wbc_env_cfg.py
@@ -0,0 +1,15 @@
+# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
+# All rights reserved.
+#
+# SPDX-License-Identifier: BSD-3-Clause
+
+from dataclasses import MISSING
+
+from isaaclab.utils import configclass
+
+from isaaclab.envs.manager_based_rl_env_cfg import ManagerBasedRLEnvCfg
+
+
+@configclass
+class ManagerBasedRLWBCEnvCfg(ManagerBasedRLEnvCfg):
+    motion: object = MISSING
diff --git a/source/unitreelab/unitreelab/managers/__init__.py b/source/unitreelab/unitreelab/managers/__init__.py
new file mode 100644
index 0000000..5cc3900
--- /dev/null
+++ b/source/unitreelab/unitreelab/managers/__init__.py
@@ -0,0 +1,2 @@
+from .motion_manager import MotionManager
+from .manager_term_cfg import MotionTermCfg
\ No newline at end of file
diff --git a/source/unitreelab/unitreelab/managers/manager_term_cfg.py b/source/unitreelab/unitreelab/managers/manager_term_cfg.py
new file mode 100644
index 0000000..4081ed7
--- /dev/null
+++ b/source/unitreelab/unitreelab/managers/manager_term_cfg.py
@@ -0,0 +1,20 @@
+# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
+# All rights reserved.
+#
+# SPDX-License-Identifier: BSD-3-Clause
+
+"""Configuration terms for different managers."""
+
+from __future__ import annotations
+
+from dataclasses import MISSING
+from typing import TYPE_CHECKING
+
+from isaaclab.utils import configclass
+
+
+@configclass
+class MotionTermCfg:
+    asset_name: str = MISSING
+    motion_file: str = MISSING
+    key_bodies: list[str] = MISSING
diff --git a/source/unitreelab/unitreelab/managers/motion_manager.py b/source/unitreelab/unitreelab/managers/motion_manager.py
new file mode 100644
index 0000000..dd9121a
--- /dev/null
+++ b/source/unitreelab/unitreelab/managers/motion_manager.py
@@ -0,0 +1,82 @@
+# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
+# All rights reserved.
+#
+# SPDX-License-Identifier: BSD-3-Clause
+
+"""Command manager for generating and updating commands."""
+
+from __future__ import annotations
+
+import inspect
+import torch
+import weakref
+from abc import abstractmethod
+from collections.abc import Sequence
+from prettytable import PrettyTable
+from typing import TYPE_CHECKING
+
+import omni.kit.app
+
+from isaaclab.managers.manager_base import ManagerBase
+from isaaclab.assets import Articulation
+from .manager_term_cfg import MotionTermCfg
+from unitreelab.MotionLib.motion_lib_h1 import H1_MotionLib
+
+if TYPE_CHECKING:
+    from isaaclab.envs import ManagerBasedRLEnv
+
+
+class MotionManager(ManagerBase):
+    _env: ManagerBasedRLEnv
+
+    def __init__(self, cfg: MotionTermCfg, env: ManagerBasedRLEnv):
+        # call the base class constructor (this prepares the terms)
+        super().__init__(cfg, env)
+        self.init_motion_lib()
+
+
+    def __str__(self) -> str:
+        """Returns: A string representation for the command manager."""
+        msg = "<MotionManager> contains MoCap motion references.\n"
+        return msg
+
+    def reset(self, env_ids: Sequence[int] | None = None) -> dict[str, torch.Tensor]:
+        extras = {}
+        return extras
+
+    def active_terms(self):
+        pass
+
+    def _prepare_terms(self):
+        pass
+
+    def init_motion_lib(self):
+        asset: Articulation = self._env.scene[self.cfg.asset_name]
+        self.body_names = asset.data.body_names
+        self.dof_offsets = []
+        previous_dof_name = "null"
+        for dof_offset, dof_name in enumerate(asset.data.joint_names):
+            if dof_name[:-2] != previous_dof_name:
+                previous_dof_name = dof_name[:-2]
+                self.dof_offsets.append(dof_offset)
+        self.dof_offsets.append(len(asset.data.joint_names))
+        self.key_body_ids = self.build_body_ids_tensor(self.cfg.key_bodies)
+        self.num_key_bodies = self.key_body_ids.shape[0]
+        self.motion_lib: H1_MotionLib = H1_MotionLib(motion_file=self.cfg.motion_file,
+                                                     dof_body_ids=None,
+                                                     dof_offsets=self.dof_offsets,
+                                                     key_body_ids=self.key_body_ids,
+                                                     device=self.device)
+
+    def build_body_ids_tensor(self, body_names):
+        body_ids = []
+
+        for body_name in body_names:
+            body_id = self.body_names.index(body_name)
+            assert (
+                body_id != -1
+            ), f"Body part {body_name} not found in {self.body_names}"
+            body_ids.append(body_id)
+
+        body_ids = torch.tensor(body_ids, dtype=torch.long, device=self.device, requires_grad=False)
+        return body_ids
diff --git a/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1/agents/rsl_rl_ppo_cfg.py b/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1/agents/rsl_rl_ppo_cfg.py
index a6b0bee..a4af1bd 100644
--- a/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1/agents/rsl_rl_ppo_cfg.py
+++ b/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1/agents/rsl_rl_ppo_cfg.py
@@ -16,6 +16,7 @@ class DaggerCfg:
     dagger_only: bool = False
     dagger_anneal: bool = False
     dagger_coefficient: float = 1.0
+    dagger_lr: float = 5e-4
 
 
 @configclass
diff --git a/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1/flat_env_cfg.py b/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1/flat_env_cfg.py
index 1dfc670..1a62d05 100644
--- a/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1/flat_env_cfg.py
+++ b/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1/flat_env_cfg.py
@@ -54,11 +54,11 @@ class MySceneCfg(InteractiveSceneCfg):
             static_friction=1.0,
             dynamic_friction=1.0,
         ),
-        visual_material=sim_utils.MdlFileCfg(
-            mdl_path=f"{ISAACLAB_NUCLEUS_DIR}/Materials/TilesMarbleSpiderWhiteBrickBondHoned/TilesMarbleSpiderWhiteBrickBondHoned.mdl",
-            project_uvw=True,
-            texture_scale=(0.25, 0.25),
-        ),
+        # visual_material=sim_utils.MdlFileCfg(
+        #     mdl_path=f"{ISAACLAB_NUCLEUS_DIR}/Materials/TilesMarbleSpiderWhiteBrickBondHoned/TilesMarbleSpiderWhiteBrickBondHoned.mdl",
+        #     project_uvw=True,
+        #     texture_scale=(0.25, 0.25),
+        # ),
         debug_vis=False,
     )
     # robots
@@ -70,7 +70,7 @@ class MySceneCfg(InteractiveSceneCfg):
         prim_path="/World/skyLight",
         spawn=sim_utils.DomeLightCfg(
             intensity=750.0,
-            texture_file=f"{ISAAC_NUCLEUS_DIR}/Materials/Textures/Skies/PolyHaven/kloofendal_43d_clear_puresky_4k.hdr",
+            # texture_file=f"{ISAAC_NUCLEUS_DIR}/Materials/Textures/Skies/PolyHaven/kloofendal_43d_clear_puresky_4k.hdr",
         ),
     )
 
diff --git a/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1/rough_env_cfg.py b/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1/rough_env_cfg.py
index ec9c7d5..5822a14 100644
--- a/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1/rough_env_cfg.py
+++ b/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1/rough_env_cfg.py
@@ -18,7 +18,7 @@ from isaaclab.managers import RewardTermCfg as RewTerm
 
 import unitreelab.tasks.locomotion.velocity.mdp as mdp
 # from unitreelab.terrains.terrain_generator_cfg import GRAVEL_TERRAINS_CFG, ROUGH_TERRAINS_CFG  # noqa: F401
-from isaaclab.terrains.config.rough import ROUGH_TERRAINS_CFG
+from unitreelab.terrains.terrain_generator_cfg import ROUGH_TERRAINS_CFG
 
 
 @configclass
@@ -35,11 +35,11 @@ class RoughSceneCfg(MySceneCfg):
             static_friction=1.0,
             dynamic_friction=1.0,
         ),
-        visual_material=sim_utils.MdlFileCfg(
-            mdl_path=f"{ISAACLAB_NUCLEUS_DIR}/Materials/TilesMarbleSpiderWhiteBrickBondHoned/TilesMarbleSpiderWhiteBrickBondHoned.mdl",
-            project_uvw=True,
-            texture_scale=(0.25, 0.25),
-        ),
+        # visual_material=sim_utils.MdlFileCfg(
+        #     mdl_path=f"{ISAACLAB_NUCLEUS_DIR}/Materials/TilesMarbleSpiderWhiteBrickBondHoned/TilesMarbleSpiderWhiteBrickBondHoned.mdl",
+        #     project_uvw=True,
+        #     texture_scale=(0.25, 0.25),
+        # ),
         debug_vis=False,
     )
     height_scanner = RayCasterCfg(
@@ -99,20 +99,20 @@ class RoughRewardsCfg:
     """Reward terms for the MDP."""
     track_lin_vel_xy_exp = RewTerm(
         func=mdp.track_lin_vel_xy_yaw_frame_exp,
-        weight=1.0,
+        weight=1.5,
         params={"command_name": "base_velocity", "std": 0.5},
     )
     track_ang_vel_z_exp = RewTerm(
-        func=mdp.track_ang_vel_z_world_exp, weight=1.0, params={"command_name": "base_velocity", "std": 0.5}
+        func=mdp.track_ang_vel_z_world_exp, weight=1.5, params={"command_name": "base_velocity", "std": 0.5}
     )
-    lin_vel_z_l2 = RewTerm(func=mdp.lin_vel_z_l2, weight=-0.5)
+    lin_vel_z_l2 = RewTerm(func=mdp.lin_vel_z_l2, weight=-0.25)
     ang_vel_xy_l2 = RewTerm(func=mdp.ang_vel_xy_l2, weight=-0.05)
     energy = RewTerm(func=mdp.energy, weight=-1e-3)
     dof_acc_l2 = RewTerm(func=mdp.joint_acc_l2, weight=-2.5e-7)
     action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-0.01)
     undesired_contacts = RewTerm(
         func=mdp.undesired_contacts,
-        weight=-2.0,
+        weight=-1.0,
         params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names="(?!.*ankle.*).*"), "threshold": 1.0},
     )
     fly = RewTerm(
@@ -163,7 +163,7 @@ class RoughRewardsCfg:
     )
     joint_deviation_legs = RewTerm(
         func=mdp.joint_deviation_l1,
-        weight=-0.05,
+        weight=-0.02,
         params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_pitch.*", ".*_knee.*", ".*_ankle.*"])},
     )
 
@@ -211,7 +211,7 @@ class G1RoughEnvCfg_PLAY(G1RoughEnvCfg):
             self.scene.terrain.terrain_generator.num_rows = 5
             self.scene.terrain.terrain_generator.num_cols = 5
             self.scene.terrain.terrain_generator.curriculum = False
-            self.scene.terrain.terrain_generator.difficulty_range = (0.5, 0.5)
+            self.scene.terrain.terrain_generator.difficulty_range = (0.6, 0.6)
 
         self.commands.base_velocity.ranges.lin_vel_x = (0.6, 0.6)
         self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
diff --git a/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1_with_dagger/rough_env_cfg.py b/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1_with_dagger/rough_env_cfg.py
index e2b5213..01d2e83 100644
--- a/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1_with_dagger/rough_env_cfg.py
+++ b/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/g1_with_dagger/rough_env_cfg.py
@@ -54,11 +54,11 @@ class MySceneCfg(InteractiveSceneCfg):
             static_friction=1.0,
             dynamic_friction=1.0,
         ),
-        visual_material=sim_utils.MdlFileCfg(
-            mdl_path=f"{ISAACLAB_NUCLEUS_DIR}/Materials/TilesMarbleSpiderWhiteBrickBondHoned/TilesMarbleSpiderWhiteBrickBondHoned.mdl",
-            project_uvw=True,
-            texture_scale=(0.25, 0.25),
-        ),
+        # visual_material=sim_utils.MdlFileCfg(
+        #     mdl_path=f"{ISAACLAB_NUCLEUS_DIR}/Materials/TilesMarbleSpiderWhiteBrickBondHoned/TilesMarbleSpiderWhiteBrickBondHoned.mdl",
+        #     project_uvw=True,
+        #     texture_scale=(0.25, 0.25),
+        # ),
         debug_vis=False,
     )
     # robots
@@ -70,7 +70,7 @@ class MySceneCfg(InteractiveSceneCfg):
         prim_path="/World/skyLight",
         spawn=sim_utils.DomeLightCfg(
             intensity=750.0,
-            texture_file=f"{ISAAC_NUCLEUS_DIR}/Materials/Textures/Skies/PolyHaven/kloofendal_43d_clear_puresky_4k.hdr",
+            # texture_file=f"{ISAAC_NUCLEUS_DIR}/Materials/Textures/Skies/PolyHaven/kloofendal_43d_clear_puresky_4k.hdr",
         ),
     )
     height_scanner = RayCasterCfg(
@@ -252,11 +252,11 @@ class RewardsCfg:
     """Reward terms for the MDP."""
     track_lin_vel_xy_exp = RewTerm(
         func=mdp.track_lin_vel_xy_yaw_frame_exp,
-        weight=1.0,
+        weight=1.5,
         params={"command_name": "base_velocity", "std": 0.5},
     )
     track_ang_vel_z_exp = RewTerm(
-        func=mdp.track_ang_vel_z_world_exp, weight=1.0, params={"command_name": "base_velocity", "std": 0.5}
+        func=mdp.track_ang_vel_z_world_exp, weight=1.5, params={"command_name": "base_velocity", "std": 0.5}
     )
     lin_vel_z_l2 = RewTerm(func=mdp.lin_vel_z_l2, weight=-0.25)
     ang_vel_xy_l2 = RewTerm(func=mdp.ang_vel_xy_l2, weight=-0.05)
@@ -316,7 +316,7 @@ class RewardsCfg:
     )
     joint_deviation_legs = RewTerm(
         func=mdp.joint_deviation_l1,
-        weight=-0.025,
+        weight=-0.02,
         params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_pitch.*", ".*_knee.*", ".*_ankle.*"])},
     )
 
@@ -406,7 +406,7 @@ class G1RoughTeacherEnvCfg_PLAY(G1RoughTeacherEnvCfg):
             self.scene.terrain.terrain_generator.num_rows = 5
             self.scene.terrain.terrain_generator.num_cols = 5
             self.scene.terrain.terrain_generator.curriculum = False
-            self.scene.terrain.terrain_generator.difficulty_range = (0.5, 0.5)
+            self.scene.terrain.terrain_generator.difficulty_range = (0.4, 0.4)
 
         self.commands.base_velocity.ranges.lin_vel_x = (0.5, 0.5)
         self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
diff --git a/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/h1_wbc/__init__.py b/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/h1_wbc/__init__.py
new file mode 100644
index 0000000..f00276d
--- /dev/null
+++ b/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/h1_wbc/__init__.py
@@ -0,0 +1,53 @@
+# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
+# All rights reserved.
+#
+# SPDX-License-Identifier: BSD-3-Clause
+
+import gymnasium as gym
+
+from . import agents
+
+##
+# Register Gym environments.
+##
+
+
+gym.register(
+    id="H1-WBC-Teacher",
+    entry_point="unitreelab.envs.manager_based_rl_wbc_env:ManagerBasedRLWBCEnv",
+    disable_env_checker=True,
+    kwargs={
+        "env_cfg_entry_point": f"{__name__}.wbc_env_cfg:H1WBCTeacherEnvCfg",
+        "rsl_rl_cfg_entry_point": f"{agents.__name__}.rsl_rl_ppo_cfg:H1WBCTeacherPPORunnerCfg",
+    },
+)
+
+gym.register(
+    id="H1-WBC-Teacher-Play",
+    entry_point="unitreelab.envs.manager_based_rl_wbc_env:ManagerBasedRLWBCEnv",
+    disable_env_checker=True,
+    kwargs={
+        "env_cfg_entry_point": f"{__name__}.wbc_env_cfg:H1WBCTeacherEnvCfg_PLAY",
+        "rsl_rl_cfg_entry_point": f"{agents.__name__}.rsl_rl_ppo_cfg:H1WBCTeacherPPORunnerCfg",
+    },
+)
+
+gym.register(
+    id="H1-WBC-Student",
+    entry_point="unitreelab.envs.manager_based_rl_wbc_env:ManagerBasedRLWBCEnv",
+    disable_env_checker=True,
+    kwargs={
+        "env_cfg_entry_point": f"{__name__}.wbc_env_cfg:H1WBCStudentEnvCfg",
+        "rsl_rl_cfg_entry_point": f"{agents.__name__}.rsl_rl_ppo_cfg:H1WBCStudentPPORunnerCfg",
+    },
+)
+
+gym.register(
+    id="H1-WBC-Student-Play",
+    entry_point="unitreelab.envs.manager_based_rl_wbc_env:ManagerBasedRLWBCEnv",
+    disable_env_checker=True,
+    kwargs={
+        "env_cfg_entry_point": f"{__name__}.wbc_env_cfg:H1WBCStudentEnvCfg_PLAY",
+        "rsl_rl_cfg_entry_point": f"{agents.__name__}.rsl_rl_ppo_cfg:H1WBCStudentPPORunnerCfg",
+    },
+)
diff --git a/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/h1_wbc/agents/__init__.py b/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/h1_wbc/agents/__init__.py
new file mode 100644
index 0000000..c3ee657
--- /dev/null
+++ b/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/h1_wbc/agents/__init__.py
@@ -0,0 +1,4 @@
+# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
+# All rights reserved.
+#
+# SPDX-License-Identifier: BSD-3-Clause
diff --git a/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/h1_wbc/agents/rsl_rl_ppo_cfg.py b/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/h1_wbc/agents/rsl_rl_ppo_cfg.py
new file mode 100644
index 0000000..861c165
--- /dev/null
+++ b/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/h1_wbc/agents/rsl_rl_ppo_cfg.py
@@ -0,0 +1,68 @@
+# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
+# All rights reserved.
+#
+# SPDX-License-Identifier: BSD-3-Clause
+
+from isaaclab.utils import configclass
+
+from isaaclab_rl.rsl_rl import RslRlOnPolicyRunnerCfg, RslRlPpoActorCriticCfg, RslRlPpoAlgorithmCfg
+
+
+@configclass
+class DaggerCfg:
+    teacher_cfg: RslRlOnPolicyRunnerCfg = RslRlOnPolicyRunnerCfg()
+    load_run: str = ""
+    load_checkpoint: str = "model_.*.pt"
+    dagger_only: bool = False
+    dagger_anneal: bool = False
+    dagger_coefficient: float = 1.0
+    dagger_lr: float = 5e-4
+
+
+@configclass
+class Policy(RslRlPpoActorCriticCfg):
+    class_name = "ActorCriticRecurrent"
+    init_noise_std = 1.0
+    actor_hidden_dims = [256, 256, 128]
+    critic_hidden_dims = [256, 256, 128]
+    activation = "elu"
+    rnn_hidden_size = 256
+    rnn_num_layers = 1
+    rnn_type = "lstm"
+
+
+@configclass
+class H1WBCTeacherPPORunnerCfg(RslRlOnPolicyRunnerCfg):
+    distill = False
+    num_steps_per_env = 24
+    max_iterations = 100000
+    save_interval = 100
+    experiment_name = "h1_wbc_teacher"
+    empirical_normalization = False  # Keep it False
+    policy = Policy()
+    algorithm = RslRlPpoAlgorithmCfg(
+        value_loss_coef=1.0,
+        use_clipped_value_loss=True,
+        clip_param=0.2,
+        entropy_coef=0.008,
+        num_learning_epochs=5,
+        num_mini_batches=4,
+        learning_rate=1.0e-3,
+        schedule="adaptive",
+        gamma=0.99,
+        lam=0.95,
+        desired_kl=0.01,
+        max_grad_norm=1.0,
+    )
+    dagger = DaggerCfg()
+    logger = "wandb"
+    wandb_project = "h1_wbc_teacher"
+
+
+@configclass
+class H1WBCStudentPPORunnerCfg(H1WBCTeacherPPORunnerCfg):
+    distill = False
+    policy = Policy(init_noise_std=0.001)
+    experiment_name = "h1_wbc_student"
+    wandb_project = "h1_wbc_student"
+    dagger = DaggerCfg(teacher_cfg=H1WBCTeacherPPORunnerCfg())
diff --git a/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/h1_wbc/wbc_env_cfg.py b/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/h1_wbc/wbc_env_cfg.py
new file mode 100644
index 0000000..50478d4
--- /dev/null
+++ b/source/unitreelab/unitreelab/tasks/locomotion/velocity/config/h1_wbc/wbc_env_cfg.py
@@ -0,0 +1,441 @@
+# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
+# All rights reserved.
+#
+# SPDX-License-Identifier: BSD-3-Clause
+
+import math
+from dataclasses import MISSING  # noqa: F401
+
+import isaaclab.sim as sim_utils
+from isaaclab.assets import ArticulationCfg, AssetBaseCfg
+from isaaclab.envs import ManagerBasedRLEnvCfg
+from isaaclab.managers import CurriculumTermCfg as CurrTerm  # noqa: F401
+from isaaclab.managers import EventTermCfg as EventTerm
+from isaaclab.managers import ObservationGroupCfg as ObsGroup
+from isaaclab.managers import ObservationTermCfg as ObsTerm
+from isaaclab.managers import RewardTermCfg as RewTerm
+from isaaclab.managers import SceneEntityCfg
+from isaaclab.managers import TerminationTermCfg as DoneTerm
+from unitreelab.managers import MotionTermCfg
+from isaaclab.scene import InteractiveSceneCfg
+from isaaclab.sensors import ContactSensorCfg, RayCasterCfg, patterns  # noqa: F401
+from isaaclab.terrains import TerrainImporterCfg
+from isaaclab.utils import configclass
+from isaaclab.utils.assets import ISAAC_NUCLEUS_DIR, ISAACLAB_NUCLEUS_DIR  # noqa: F401
+from isaaclab.utils.noise import AdditiveUniformNoiseCfg as Unoise
+
+
+##
+# Pre-defined configs
+##
+from unitreelab.assets.unitree import H1_CFG
+import unitreelab.tasks.locomotion.velocity.mdp as mdp
+from unitreelab.terrains.terrain_generator_cfg import GRAVEL_TERRAINS_CFG
+
+
+##
+# Scene definition
+##
+
+@configclass
+class MotionCfg(MotionTermCfg):
+    asset_name="robot"
+    motion_file = "data/amass_filtered.pt"
+    key_bodies = ['left_ankle_link', 'right_ankle_link', 'left_arm_end_effector',
+                  'right_arm_end_effector', 'head', 'left_shoulder_roll_link',
+                  'right_shoulder_roll_link', 'left_elbow_link', 'right_elbow_link',
+                  'left_knee_link', 'right_knee_link', 'left_hip_pitch_link', 'right_hip_pitch_link', 'torso_link', 'pelvis']
+
+
+@configclass
+class MySceneCfg(InteractiveSceneCfg):
+    """Configuration for the terrain scene with a legged robot."""
+
+    # ground terrain
+    terrain = TerrainImporterCfg(
+        prim_path="/World/ground",
+        terrain_type="generator",
+        terrain_generator=GRAVEL_TERRAINS_CFG,
+        max_init_terrain_level=0,
+        collision_group=-1,
+        physics_material=sim_utils.RigidBodyMaterialCfg(
+            friction_combine_mode="multiply",
+            restitution_combine_mode="multiply",
+            static_friction=1.0,
+            dynamic_friction=1.0,
+        ),
+        # visual_material=sim_utils.MdlFileCfg(
+        #     mdl_path=f"{ISAACLAB_NUCLEUS_DIR}/Materials/TilesMarbleSpiderWhiteBrickBondHoned/TilesMarbleSpiderWhiteBrickBondHoned.mdl",
+        #     project_uvw=True,
+        #     texture_scale=(0.25, 0.25),
+        # ),
+        debug_vis=False,
+    )
+    # robots
+    robot: ArticulationCfg = H1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
+    # sensors
+    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True)
+    # lights
+    sky_light = AssetBaseCfg(
+        prim_path="/World/skyLight",
+        spawn=sim_utils.DomeLightCfg(
+            intensity=750.0,
+            # texture_file=f"{ISAAC_NUCLEUS_DIR}/Materials/Textures/Skies/PolyHaven/kloofendal_43d_clear_puresky_4k.hdr",
+        ),
+    )
+
+
+##
+# MDP settings
+##
+
+
+@configclass
+class CommandsCfg:
+    """Command specifications for the MDP."""
+
+    base_velocity = mdp.UniformVelocityCommandCfg(
+        asset_name="robot",
+        resampling_time_range=(10.0, 10.0),
+        rel_standing_envs=0.02,
+        rel_heading_envs=1.0,
+        heading_command=True,
+        heading_control_stiffness=1.0,
+        debug_vis=True,
+        ranges=mdp.UniformVelocityCommandCfg.Ranges(
+            lin_vel_x=(-0.6, 1.0), lin_vel_y=(-0.5, 0.5), ang_vel_z=(-1.0, 1.0), heading=(-math.pi, math.pi)
+        ),
+    )
+
+
+@configclass
+class ActionsCfg:
+    """Action specifications for the MDP."""
+
+    joint_pos = mdp.JointPositionActionCfg(asset_name="robot", joint_names=[".*"], scale=0.25, use_default_offset=True)
+
+
+@configclass
+class ObservationsTeacherCfg:
+    """Observation specifications for the MDP."""
+
+    @configclass
+    class PolicyCfg(ObsGroup):
+        """Observations for policy group."""
+
+        # observation terms (order preserved)
+        base_ang_vel = ObsTerm(func=mdp.base_ang_vel)  # [0:3]
+        projected_gravity = ObsTerm(
+            func=mdp.projected_gravity,
+        )  # [3:6]
+        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})  # [6:9]
+        joint_pos = ObsTerm(func=mdp.joint_pos_rel)  # [9:38]
+        joint_vel = ObsTerm(func=mdp.joint_vel_rel)  # [38:67]
+        actions = ObsTerm(func=mdp.last_action)  # [67:96]
+        base_lin_vel = ObsTerm(func=mdp.base_lin_vel)  # [96:99]
+        feet_contact = ObsTerm(func=mdp.body_contact, params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle.*")})  # [99:101]
+
+        def __post_init__(self):
+            self.enable_corruption = True
+            self.concatenate_terms = True
+
+    # observation groups
+    policy: PolicyCfg = PolicyCfg()
+
+
+@configclass
+class ObservationsStudentCfg:
+    """Observation specifications for the MDP."""
+
+    @configclass
+    class PolicyCfg(ObsGroup):
+        """Observations for policy group."""
+
+        # observation terms (order preserved)
+        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))  # [0:3]
+        projected_gravity = ObsTerm(
+            func=mdp.projected_gravity,
+            noise=Unoise(n_min=-0.05, n_max=0.05),
+        )  # [3:6]
+        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})  # [6:9]
+        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))  # [9:38]
+        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))  # [38:67]
+        actions = ObsTerm(func=mdp.last_action)  # [67:96]
+
+        def __post_init__(self):
+            self.enable_corruption = True
+            self.concatenate_terms = True
+
+    # observation groups
+    policy: PolicyCfg = PolicyCfg()
+    critic: ObservationsTeacherCfg.PolicyCfg = ObservationsTeacherCfg.PolicyCfg()
+
+
+@configclass
+class EventCfg:
+    """Configuration for events."""
+
+    # startup
+    physics_material = EventTerm(
+        func=mdp.randomize_rigid_body_material,
+        mode="startup",
+        params={
+            "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
+            "static_friction_range": (0.6, 1.0),
+            "dynamic_friction_range": (0.4, 0.8),
+            "restitution_range": (0.0, 0.005),
+            "num_buckets": 64,
+        },
+    )
+
+    add_base_mass = EventTerm(
+        func=mdp.randomize_rigid_body_mass,
+        mode="startup",
+        params={
+            "asset_cfg": SceneEntityCfg("robot", body_names=[".*torso.*"]),
+            "mass_distribution_params": (-5.0, 10.0),
+            "operation": "add",
+        },
+    )
+
+    # reset
+    base_external_force_torque = EventTerm(
+        func=mdp.apply_external_force_torque,
+        mode="reset",
+        params={
+            "asset_cfg": SceneEntityCfg("robot", body_names=[".*torso.*"]),
+            "force_range": (0.0, 0.0),
+            "torque_range": (-0.0, 0.0),
+        },
+    )
+
+    reset_base = EventTerm(
+        func=mdp.reset_root_state_uniform,
+        mode="reset",
+        params={
+            "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
+            "velocity_range": {
+                "x": (0.0, 0.0),
+                "y": (0.0, 0.0),
+                "z": (0.0, 0.0),
+                "roll": (0.0, 0.0),
+                "pitch": (0.0, 0.0),
+                "yaw": (0.0, 0.0),
+            },
+        },
+    )
+
+    reset_robot_joints = EventTerm(
+        func=mdp.reset_joints_by_scale,
+        mode="reset",
+        params={
+            "position_range": (0.5, 1.5),
+            "velocity_range": (0.0, 0.0),
+        },
+    )
+
+    # interval
+    push_robot = EventTerm(
+        func=mdp.push_by_setting_velocity,
+        mode="interval",
+        interval_range_s=(6.0, 10.0),
+        params={"velocity_range": {"x": (-0.8, 0.8), "y": (-0.8, 0.8)}},
+    )
+
+
+@configclass
+class RewardsCfg:
+    """Reward terms for the MDP."""
+    track_lin_vel_xy_exp = RewTerm(
+        func=mdp.track_lin_vel_xy_yaw_frame_exp,
+        weight=1.0,
+        params={"command_name": "base_velocity", "std": 0.5},
+    )
+    track_ang_vel_z_exp = RewTerm(
+        func=mdp.track_ang_vel_z_world_exp, weight=1.0, params={"command_name": "base_velocity", "std": 0.5}
+    )
+    lin_vel_z_l2 = RewTerm(func=mdp.lin_vel_z_l2, weight=-1.0)
+    ang_vel_xy_l2 = RewTerm(func=mdp.ang_vel_xy_l2, weight=-0.05)
+    energy = RewTerm(func=mdp.energy, weight=-1e-3)
+    dof_acc_l2 = RewTerm(func=mdp.joint_acc_l2, weight=-2.5e-7)
+    action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-0.01)
+    undesired_contacts = RewTerm(
+        func=mdp.undesired_contacts,
+        weight=-1.0,
+        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names="(?!.*ankle.*).*"), "threshold": 1.0},
+    )
+    fly = RewTerm(
+        func=mdp.fly,
+        weight=-1.0,
+        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle.*")},
+    )
+    flat_orientation_l2 = RewTerm(func=mdp.flat_orientation_l2, weight=-1.0)
+    termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
+    feet_air_time = RewTerm(
+        func=mdp.feet_air_time_positive_biped,
+        weight=0.25,
+        params={
+            "command_name": "base_velocity",
+            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle.*"),
+            "threshold": 0.4,
+        },
+    )
+    feet_slide = RewTerm(
+        func=mdp.feet_slide,
+        weight=-0.25,
+        params={
+            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle.*"),
+            "asset_cfg": SceneEntityCfg("robot", body_names=".*ankle.*"),
+        },
+    )
+    feet_force = RewTerm(
+        func=mdp.body_force,
+        weight=-3e-3,
+        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle.*"), "threshold": 500, "max_reward": 400},
+    )
+    feet_stumble = RewTerm(
+        func=mdp.feet_stumble,
+        weight=-2.0,
+        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle.*")},
+    )
+    dof_pos_limits = RewTerm(func=mdp.joint_pos_limits, weight=-2.0)
+    joint_deviation_hip = RewTerm(
+        func=mdp.joint_deviation_l1,
+        weight=-0.1,
+        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_yaw.*", ".*_hip_roll.*"])},
+    )
+    joint_deviation_arms = RewTerm(
+        func=mdp.joint_deviation_l1,
+        weight=-0.2,
+        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*torso.*", ".*_shoulder.*", ".*_elbow.*"])}
+    )
+    joint_deviation_legs = RewTerm(
+        func=mdp.joint_deviation_l1,
+        weight=-0.025,
+        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_pitch.*", ".*_knee.*", ".*ankle.*"])},
+    )
+
+
+@configclass
+class TerminationsCfg:
+    """Termination terms for the MDP."""
+
+    time_out = DoneTerm(func=mdp.time_out, time_out=True)
+    base_contact = DoneTerm(
+        func=mdp.illegal_contact,
+        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=["torso_link"]), "threshold": 1.0},
+    )
+
+
+@configclass
+class CurriculumCfg:
+    """Curriculum terms for the MDP."""
+
+    # terrain_levels = CurrTerm(func=mdp.terrain_levels_vel)
+
+
+##
+# Environment configuration
+##
+
+
+@configclass
+class H1WBCTeacherEnvCfg(ManagerBasedRLEnvCfg):
+    """Configuration for the locomotion velocity-tracking environment."""
+
+    # Scene settings
+    scene: MySceneCfg = MySceneCfg(num_envs=4096, env_spacing=2.5)
+    # Basic settings
+    observations: ObservationsTeacherCfg = ObservationsTeacherCfg()
+    actions: ActionsCfg = ActionsCfg()
+    commands: CommandsCfg = CommandsCfg()
+    # MDP settings
+    rewards: RewardsCfg = RewardsCfg()
+    terminations: TerminationsCfg = TerminationsCfg()
+    events: EventCfg = EventCfg()
+    curriculum: CurriculumCfg = CurriculumCfg()
+    motion: MotionCfg = MotionCfg()
+
+    def __post_init__(self):
+        """Post initialization."""
+        # general settings
+        self.decimation = 4
+        self.episode_length_s = 20.0
+        # simulation settings
+        self.sim.dt = 0.005
+        self.sim.render_interval = self.decimation
+        self.sim.disable_contact_processing = True
+        self.sim.physics_material = self.scene.terrain.physics_material
+        # update sensor update periods
+        # we tick all the sensors based on the smallest update period (physics update period)
+        self.scene.contact_forces.update_period = self.sim.dt
+
+        # check if terrain levels curriculum is enabled - if so, enable curriculum for terrain generator
+        # this generates terrains with increasing difficulty and is useful for training
+        if getattr(self.curriculum, "terrain_levels", None) is not None:
+            if self.scene.terrain.terrain_generator is not None:
+                self.scene.terrain.terrain_generator.curriculum = True
+        else:
+            if self.scene.terrain.terrain_generator is not None:
+                self.scene.terrain.terrain_generator.curriculum = False
+
+
+@configclass
+class H1WBCStudentEnvCfg(H1WBCTeacherEnvCfg):
+    observations: ObservationsStudentCfg = ObservationsStudentCfg()
+
+
+@configclass
+class H1WBCTeacherEnvCfg_PLAY(H1WBCTeacherEnvCfg):
+    def __post_init__(self):
+        # post init of parent
+        super().__post_init__()
+
+        # make a smaller scene for play
+        self.scene.num_envs = 50
+        self.scene.env_spacing = 2.5
+        self.episode_length_s = 40.0
+        # spawn the robot randomly in the grid (instead of their terrain levels)
+        self.scene.terrain.max_init_terrain_level = None
+        # reduce the number of terrains to save memory
+        if self.scene.terrain.terrain_generator is not None:
+            self.scene.terrain.terrain_generator.num_rows = 5
+            self.scene.terrain.terrain_generator.num_cols = 5
+            self.scene.terrain.terrain_generator.curriculum = False
+
+        self.commands.base_velocity.ranges.lin_vel_x = (0.5, 0.5)
+        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
+        self.commands.base_velocity.ranges.heading = (0.0, 0.0)
+        # disable randomization for play
+        self.observations.policy.enable_corruption = False
+        # remove random pushing
+        self.events.base_external_force_torque = None
+        self.events.push_robot = None
+
+
+@configclass
+class H1WBCStudentEnvCfg_PLAY(H1WBCStudentEnvCfg):
+    def __post_init__(self):
+        # post init of parent
+        super().__post_init__()
+
+        # make a smaller scene for play
+        self.scene.num_envs = 50
+        self.scene.env_spacing = 2.5
+        self.episode_length_s = 40.0
+        # spawn the robot randomly in the grid (instead of their terrain levels)
+        self.scene.terrain.max_init_terrain_level = None
+        # reduce the number of terrains to save memory
+        if self.scene.terrain.terrain_generator is not None:
+            self.scene.terrain.terrain_generator.num_rows = 5
+            self.scene.terrain.terrain_generator.num_cols = 5
+            self.scene.terrain.terrain_generator.curriculum = False
+
+        self.commands.base_velocity.ranges.lin_vel_x = (0.5, 0.5)
+        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
+        self.commands.base_velocity.ranges.heading = (0.0, 0.0)
+        # disable randomization for play
+        self.observations.policy.enable_corruption = False
+        # remove random pushing
+        self.events.base_external_force_torque = None
+        self.events.push_robot = None
diff --git a/source/unitreelab/unitreelab/tasks/locomotion/velocity/mdp/observations.py b/source/unitreelab/unitreelab/tasks/locomotion/velocity/mdp/observations.py
index f05a6ff..40883ff 100755
--- a/source/unitreelab/unitreelab/tasks/locomotion/velocity/mdp/observations.py
+++ b/source/unitreelab/unitreelab/tasks/locomotion/velocity/mdp/observations.py
@@ -15,3 +15,15 @@ def body_contact(env: ManagerBasedRLEnv, sensor_cfg: SceneEntityCfg) -> torch.Te
     """The feet contact of the robot."""
     contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
     return contact_sensor.data.current_contact_time[:, sensor_cfg.body_ids] > 0.001
+
+
+def height_scan(env: ManagerBasedEnv, sensor_cfg: SceneEntityCfg, offset: float = 0.5) -> torch.Tensor:
+    """Height scan from the given sensor w.r.t. the sensor's frame.
+
+    The provided offset (Defaults to 0.5) is subtracted from the returned values.
+    """
+    # extract the used quantities (to enable type-hinting)
+    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
+    # height scan: height = sensor_height - hit_point_z - offset
+    return sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - offset
+    # return torch.ones_like(sensor.data.ray_hits_w[..., 2], device=sensor.data.ray_hits_w[..., 2].device) * 0.322
diff --git a/source/unitreelab/unitreelab/terrains/terrain_generator_cfg.py b/source/unitreelab/unitreelab/terrains/terrain_generator_cfg.py
index 1f481f7..c09fb5f 100644
--- a/source/unitreelab/unitreelab/terrains/terrain_generator_cfg.py
+++ b/source/unitreelab/unitreelab/terrains/terrain_generator_cfg.py
@@ -44,36 +44,33 @@ ROUGH_TERRAINS_CFG = TerrainGeneratorCfg(
     slope_threshold=0.75,
     use_cache=False,
     sub_terrains={
-        "flat": terrain_gen.MeshPlaneTerrainCfg(
-            proportion=0.3,
-        ),
-        "hf_pyramid_slope": terrain_gen.HfPyramidSlopedTerrainCfg(
-            proportion=0.1, slope_range=(0.0, 0.4), platform_width=2.0, border_width=0.25
-        ),
-        "hf_pyramid_slope_inv": terrain_gen.HfInvertedPyramidSlopedTerrainCfg(
-            proportion=0.1, slope_range=(0.0, 0.4), platform_width=2.0, border_width=0.25
-        ),
         "pyramid_stairs": terrain_gen.MeshPyramidStairsTerrainCfg(
-            proportion=0.05,
-            step_height_range=(0.0, 0.1),
-            step_width=0.3,
+            proportion=0.2,
+            step_height_range=(0.05, 0.23),
+            step_width=0.33,
             platform_width=3.0,
             border_width=1.0,
             holes=False,
         ),
         "pyramid_stairs_inv": terrain_gen.MeshInvertedPyramidStairsTerrainCfg(
-            proportion=0.05,
-            step_height_range=(0.0, 0.1),
-            step_width=0.3,
+            proportion=0.2,
+            step_height_range=(0.05, 0.23),
+            step_width=0.33,
             platform_width=3.0,
             border_width=1.0,
             holes=False,
         ),
-        "wave_terrain": terrain_gen.HfWaveTerrainCfg(
-            proportion=0.2, amplitude_range=(0.0, 0.2), num_waves=4, border_width=0.25
+        "boxes": terrain_gen.MeshRandomGridTerrainCfg(
+            proportion=0.2, grid_width=0.45, grid_height_range=(0.05, 0.15), platform_width=2.0
         ),
         "random_rough": terrain_gen.HfRandomUniformTerrainCfg(
-            proportion=0.2, noise_range=(0.0, 0.06), noise_step=0.02, border_width=0.25
+            proportion=0.2, noise_range=(0.02, 0.10), noise_step=0.02, border_width=0.25
+        ),
+        "hf_pyramid_slope": terrain_gen.HfPyramidSlopedTerrainCfg(
+            proportion=0.1, slope_range=(0.0, 0.4), platform_width=2.0, border_width=0.25
+        ),
+        "hf_pyramid_slope_inv": terrain_gen.HfInvertedPyramidSlopedTerrainCfg(
+            proportion=0.1, slope_range=(0.0, 0.4), platform_width=2.0, border_width=0.25
         ),
     },
 )